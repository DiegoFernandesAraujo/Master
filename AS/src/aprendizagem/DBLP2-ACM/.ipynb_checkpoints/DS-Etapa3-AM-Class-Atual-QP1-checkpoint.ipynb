{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Classificação a partir de conjuntos de treinamento gerados para QP1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base atual: cd\n",
      "QP atual: qp1\n",
      "**************************\n",
      "Base: cd - qp1 - Abordagem: 2 - AA[pet-chr] - Classificador: rf\n",
      "**************************\n",
      "../csv/conjuntosDS/treinoTeste/cd/qp1/PtChr/\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../csv/estatisticas/cd/qp1/estatisticaDS2-PtChr.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1d0fc8732b10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;31m# estatisticas = pd.read_csv(estat, index_col=['etapa','permutacao'], sep=';')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m             \u001b[0mestatisticas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'algoritmosUtilizados'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'permutacao'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'etapa'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[1;31m#     estatisticas.tail()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    983\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas\\_libs\\parsers.c:4209)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas\\_libs\\parsers.c:8873)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'../csv/estatisticas/cd/qp1/estatisticaDS2-PtChr.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\"\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# janelas = [1, 0.01, 0.03, 0.05] # K = {1, 1%, 3%, 5%}\n",
    "# abordagens = [\"dgArj\", \"ptChr\"] #Quando houver a janela vencedora. Após testar as combinações acima\n",
    "#                  #com todas as bases. Lembrando que só deve ser usada para as demais QP além e QP6\n",
    "\n",
    "abordagens = [\"ptChr\"] #Quando houver a janela vencedora. Após testar as combinações acima\n",
    "                 #com todas as bases. Lembrando que só deve ser usada para as demais QP além e QP6\n",
    "\n",
    "# janelas = [100] #Quando for utilizar Peter Christen. Ou então, criar um conjunto de 3 valores\n",
    "                    # de modo que cada um represente DgArj, PtChr ou Aleatório\n",
    "\n",
    "# bases = [\"cds\", \"cds1\", \"cds2\"]\n",
    "bases = [\"DBLP2-ACM\"]\n",
    "# qps = [\"QP1\", \"QP2\"] #Todos menos QP6\n",
    "# qps = [\"qp1\", \"qp2b\", \"qp2m\", \"qp2r\", \"qp3all\", \"qp3lot\"] #Todos menos QP6 e QP7\n",
    "# qps = [\"qp1\", \"qp2b\", \"qp2r\", \"qp3all\", \"qp3lot\"] #Todos menos QP6 e QP7 - (Considerando 2 grupos divididos pela mediaa)\n",
    "qps = [\"qp1\"] #Todos menos QP6 e QP7 - (Considerando 2 grupos divididos pela mediaa)\n",
    "regiao = \"incert\"\n",
    "janela = 0.01\n",
    "\n",
    "\n",
    "for base in bases:\n",
    "    \n",
    "    print(\"Base atual: {0}\".format(base))\n",
    "    \n",
    "    for qp in qps: #Loop abrangendo os dois classificadores\n",
    "        \n",
    "        print(\"QP atual: {0}\".format(qp))\n",
    "    \n",
    "        for k in abordagens: \n",
    "\n",
    "            abordagemAA = \"\"\n",
    "            \n",
    "            dirOrig = \"\"\n",
    "            dirEstat = \"\"\n",
    "            estat = \"\"\n",
    "            \n",
    "            if(k == \"ptChr\"): #Se Peter Christen\n",
    "                \n",
    "                abordagemAA = '2 - AA[pet-chr]'\n",
    "            \n",
    "                dirOrig = \"../../csv/conjuntosDS/treinoTeste/\"+base+\"/\"+qp+\"/\"+\"PtChr/\"\n",
    "                dirEstat = \"../../csv/estatisticas/\"+base+\"/\"+qp+\"/\"\n",
    "                estat = dirEstat+\"estatisticaDS2-PtChr.csv\"\n",
    "    #   \n",
    "            elif(k == \"dgArj\"): #Se Diego Araújo\n",
    "            \n",
    "                abordagemAA = '2 - AA[dg-arj]'\n",
    "            \n",
    "#                 dirOrig = \"../../csv/conjuntosDS/treinoTeste/\"+base+\"/\"+qp+\"/\"+\"DgArj/\"\n",
    "                dirOrig = \"../../csv/conjuntosDS/treinoTeste/\"+base+\"/\"+qp+\"/\"+\"DgArj-k\"+str(janela)+\"/\"\n",
    "                dirEstat = \"../../csv/estatisticas/\"+base+\"/\"+qp+\"/\"\n",
    "                #                 estat = dirEstat+\"estatisticaDS2-DgArj.csv\"\n",
    "                estat = dirEstat+\"estatisticaDS2-DgArj-k\"+str(janela)+\".csv\"\n",
    "                \n",
    "            elif(k == \"rand\"): #Se Random\n",
    "            \n",
    "                abordagemAA = '2 - AA[random]'\n",
    "            \n",
    "                dirOrig = \"../../csv/conjuntosDS/treinoTeste/\"+base+\"/\"+qp+\"/\"+\"Random/\"\n",
    "                dirEstat = \"../../csv/estatisticas/\"+base+\"/\"+qp+\"/\"\n",
    "                estat = dirEstat+\"estatisticaDS2-Random.csv\"\n",
    "                \n",
    "            print(\"**************************\")\n",
    "            print(\"Base: {0} - {1} - Abordagem: {2} - Classificador: rf\".format(base,qp,abordagemAA))\n",
    "            print(\"**************************\")\n",
    "            \n",
    "            print(dirOrig)\n",
    "\n",
    "            etapa = '3 - clasf'\n",
    "\n",
    "            # estatisticas = pd.read_csv(estat, index_col=['etapa','permutacao'], sep=';')\n",
    "            estatisticas = pd.read_csv(estat, index_col=['algoritmosUtilizados','permutacao','etapa'], sep=';')\n",
    "        #     estatisticas.tail()\n",
    "\n",
    "            arquivos = os.listdir(dirOrig)\n",
    "            # print(arquivos)\n",
    "            #print(len(arquivos))\n",
    "\n",
    "            train = []\n",
    "            test = []\n",
    "\n",
    "            for arq in arquivos:\n",
    "                if arq.startswith(\"train\"):\n",
    "                    train.append(arq)\n",
    "                elif arq.startswith(\"test\"):\n",
    "                    test.append(arq)\n",
    "\n",
    "            # print('tamanho de train: %d' %(len(train)))        \n",
    "\n",
    "            # print('tamanho de test: %d' %(len(test)))\n",
    "\n",
    "            # print (train)    \n",
    "            # print (test)    \n",
    "\n",
    "\n",
    "            lista =[]\n",
    "\n",
    "            #lista = ['10', '15', '20', '25'] #Assumindo que serão esses os conjuntos de algoritmos\n",
    "            #ATENÇÃO, DIEGO!! ISSO MUDA PARA QP2 E QP3\n",
    "            if (qp == \"qp1\"):\n",
    "                lista = ['5', '15', '25'] #Assumindo que serão esses os conjuntos de algoritmos \n",
    "            elif (qp == \"qp2b\"):\n",
    "                lista = ['8'] #Assumindo que serão esses os conjuntos de algoritmos \n",
    "            elif (qp == \"qp2m\"):\n",
    "                lista = ['8'] #Assumindo que serão esses os conjuntos de algoritmos \n",
    "            elif (qp == \"qp2r\"):\n",
    "                lista = ['8'] #Assumindo que serão esses os conjuntos de algoritmos \n",
    "            elif (qp == \"qp3all\"):\n",
    "                lista = ['13'] #Assumindo que serão esses os conjuntos de algoritmos \n",
    "            elif (qp == \"qp3lot\"):\n",
    "                lista = ['5'] #Assumindo que serão esses os conjuntos de algoritmos \n",
    "            \n",
    "            for i in lista: \n",
    "\n",
    "                cont = 0\n",
    "\n",
    "                trainAtual = []\n",
    "                testAtual = []\n",
    "\n",
    "                for arq in train:\n",
    "                    if '('+i+')' in arq:\n",
    "        #             if 'train(10)22' in arq:    \n",
    "                        trainAtual.append(arq)\n",
    "                for arq in test:\n",
    "                    if '('+i+')' in arq:\n",
    "        #             if 'test(10)22' in arq:\n",
    "                        testAtual.append(arq)\n",
    "\n",
    "            #     print('lista desordenada')\n",
    "            #     print(trainAtual)\n",
    "\n",
    "            #     print('tamanho antes: %d' %(len(trainAtual)))\n",
    "\n",
    "                #Ordenando a lista\n",
    "                trainAtual = sorted(trainAtual)\n",
    "                testAtual = sorted(testAtual)\n",
    "\n",
    "            #     for x in trainAtual:\n",
    "            #         print(x)\n",
    "\n",
    "\n",
    "\n",
    "            # #     print('lista ordenada')\n",
    "            # #     print(trainAtual)\n",
    "\n",
    "            #     print('tamanho depois: %d' %(len(trainAtual)))\n",
    "\n",
    "            #     print(type(trainAtual))\n",
    "\n",
    "\n",
    "                print (i)\n",
    "\n",
    "            #     print(trainAtual)\n",
    "            #     print(testAtual)\n",
    "\n",
    "                tam = len(trainAtual) #Mesma coisa para testAtual\n",
    "\n",
    "                for pos in range(tam):\n",
    "            #         print(pos)\n",
    "                    print('')\n",
    "                    print('##########################')\n",
    "                    print('Arquivos atuais: %s e %s' %(trainAtual[pos], testAtual[pos]))\n",
    "                    print('##########################')\n",
    "\n",
    "                    treino = dirOrig+trainAtual[pos]\n",
    "                    teste = dirOrig+testAtual[pos]\n",
    "\n",
    "            #         print('treino: %s' %(treino))\n",
    "\n",
    "            #         tp, fp, tn, fn = 0\n",
    "\n",
    "                    num = trainAtual[pos].replace('train('+i+')','')\n",
    "                    num = num.replace('.csv','')\n",
    "\n",
    "                    cont += 1\n",
    "\n",
    "                    print('Iteração: %d' %(cont))\n",
    "\n",
    "            #         print('num: %s' %(num))\n",
    "\n",
    "                    algUtl = re.sub('train.*\\(', r'', trainAtual[pos]) #Alterar para fazer a substituição de tudo em uma linha só\n",
    "                    algUtl = re.sub('\\).*', r'', algUtl) #Alterar para fazer a substituição de tudo em uma linha só\n",
    "                    algUtl = int(algUtl)\n",
    "\n",
    "\n",
    "                    print('algUtl: %d' %(algUtl))\n",
    "\n",
    "                    permutacao = int(num)\n",
    "            #         linhaAtual = estatisticas.loc[('2 - AA[pet-chr]', permutacao), : ] #Armazena a linha correspondente à permutação\n",
    "            #         linhaAtual = estatisticas.loc[('1 - acm diverg', permutacao), : ] #Armazena a linha correspondente à permutação\n",
    "                    linhaAtual = estatisticas.loc[algUtl, permutacao, abordagemAA, : ] #Armazena a linha correspondente à permutação\n",
    "        #             print (linhaAtual)\n",
    "        #             print(linhaAtual.columns)\n",
    "\n",
    "                #'''\n",
    "            #         all = pd.read_csv(\"train.csv\", index_col=False, sep=';', names=('title', 'artist', 'track01', 'track02', 'track03', 'duplicata'), header = None)\n",
    "            #         toClass = pd.read_csv(\"test.csv\", index_col=False, sep=';', names=('title', 'artist', 'track01', 'track02', 'track03', 'duplicata'), header = None)\n",
    "\n",
    "            #         all = pd.read_csv(treino, index_col=False, sep=';', names=('title', 'artist', 'track01', 'track02', 'track03', 'duplicata'), header = None)\n",
    "            #         toClass = pd.read_csv(teste, index_col=False, sep=';', names=('title', 'artist', 'track01', 'track02', 'track03', 'duplicata'), header = None)\n",
    "\n",
    "                    all = pd.read_csv(treino, index_col=False, sep=';', names=('title', 'authors', 'venue', 'year', 'duplicata'), header = 0) #Era header = None\n",
    "                    toClass = pd.read_csv(teste, index_col=False, sep=';', names=('title', 'authors', 'venue', 'year', 'duplicata'), header = 0) #Era header = None\n",
    "\n",
    "\n",
    "                    # all['artist-title'] = all['artist'] * all['title']\n",
    "                    # cols = list(all.columns.values)\n",
    "                    # cols.pop(cols.index('duplicata'))\n",
    "                    # all = all[cols+['duplicata']]\n",
    "\n",
    "                    #Pesos baseados no Alg17\n",
    "\n",
    "                    #all['artist-title'] = all['artist'] * all['title']\n",
    "                    all['soma-pesos'] = (all['title']*2 + all['authors']*0.5 + all['venue']*0.5 + all['year']*1)/4.0\n",
    "                    cols = list(all.columns.values)\n",
    "                    cols.pop(cols.index('duplicata'))\n",
    "                    all = all[cols+['duplicata']]\n",
    "\n",
    "                    # toClass['artist-title'] = toClass['artist'] * toClass['title']\n",
    "                    # cols = list(toClass.columns.values)\n",
    "                    # cols.pop(cols.index('duplicata'))\n",
    "                    # toClass = toClass[cols+['duplicata']]\n",
    "\n",
    "                    #Pesos baseados no Alg17\n",
    "\n",
    "#                     toClass['artist-title'] = toClass['artist'] * toClass['title']\n",
    "                    toClass['soma-pesos'] = (toClass['title']*2 + toClass['authors']*0.5 + toClass['venue']*0.5 + toClass['year']*1)/4.0\n",
    "                    cols = list(toClass.columns.values)\n",
    "                    cols.pop(cols.index('duplicata'))\n",
    "                    toClass = toClass[cols+['duplicata']]\n",
    "\n",
    "                    #Separação do conjunto X do conjunto y\n",
    "                    # X = all.loc[:,'title':'artist-title']\n",
    "                    X = all.loc[:,'title':'soma-pesos']\n",
    "                    y = all.duplicata\n",
    "\n",
    "                    # XtoClass = toClass.loc[:,'title':'artist-title']\n",
    "                    XtoClass = toClass.loc[:,'title':'soma-pesos']\n",
    "                    ytoClass = toClass.duplicata\n",
    "\n",
    "                    from sklearn import model_selection\n",
    "\n",
    "                    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.20, random_state=2)\n",
    "\n",
    "                    from sklearn.ensemble import RandomForestClassifier\n",
    "                    from sklearn.model_selection import StratifiedKFold\n",
    "                    from sklearn.model_selection import KFold\n",
    "                    from sklearn import model_selection\n",
    "\n",
    "                    seed = 500\n",
    "\n",
    "                    '''\n",
    "\n",
    "                    from sklearn.linear_model import LogisticRegression\n",
    "                    from sklearn.tree import DecisionTreeClassifier\n",
    "                    from sklearn.neighbors import SVC\n",
    "                    from sklearn.svm import SVC\n",
    "\n",
    "                    modelos = []\n",
    "                    modelos.append(('LR', LogisticRegression()))\n",
    "                    modelos.append(('KNN', SVC()))\n",
    "                    modelos.append(('RFC', DecisionTreeClassifier()))\n",
    "                    modelos.append(('SVM', SVC()))\n",
    "                    modelos.append(('RF', RandomForestClassifier()))\n",
    "\n",
    "\n",
    "\n",
    "                    # Avaliação de cada modelo por vez\n",
    "                    resultados = []\n",
    "                    nomes = []\n",
    "                    for nome, modelo in modelos:\n",
    "                        kfold = StratifiedKFold(n_splits=10, random_state=seed) #Mudar para n-fold\n",
    "                        cv_results = model_selection.cross_val_score(modelo, X_train, y_train, cv=kfold, scoring='f1')\n",
    "                        msg = \"%s: %f (%f)\" % (nome, cv_results.mean(), cv_results.std()) #Iprimir em um arquivo\n",
    "                        print(msg)\n",
    "                    '''\n",
    "\n",
    "                    rfc = RandomForestClassifier(random_state=12)\n",
    "\n",
    "                    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "                    param_grid = [{\"criterion\": ['gini', 'entropy'],\"n_estimators\": [10,25,30,35,40,45,50,100],\n",
    "                                   #\"max_depth\": [2,3,4,5,6,7],\n",
    "                                  \"class_weight\": [\"balanced\",None],\n",
    "                                  \"min_samples_split\": [2, 5, 10]}]\n",
    "\n",
    "                    score = 'f1'\n",
    "\n",
    "                    print(\"# Tunando hiperparâmetros para %s\" % score)\n",
    "                    print()\n",
    "                    \n",
    "                    rfc = RandomForestClassifier(random_state=500)\n",
    "                    \n",
    "                    try:\n",
    "\n",
    "                        grid = GridSearchCV(rfc, param_grid, cv=2, scoring='%s_macro' % score)\n",
    "\n",
    "                        try:\n",
    "\n",
    "                            grid = GridSearchCV(rfc, param_grid, cv=5, scoring='%s_macro' % score)\n",
    "                            grid.fit(X_train, y_train)\n",
    "\n",
    "                        except ValueError:\n",
    "\n",
    "                            try:\n",
    "\n",
    "                                grid = GridSearchCV(rfc, param_grid, cv=3, scoring='%s_macro' % score)\n",
    "                                grid.fit(X_train, y_train)\n",
    "\n",
    "                            except ValueError:\n",
    "\n",
    "                                grid = GridSearchCV(rfc, param_grid, cv=2, scoring='%s_macro' % score)\n",
    "                                grid.fit(X_train, y_train)\n",
    "\n",
    "                        print(\"Melhor conjunto de hiperparâmetros encontrado:\")\n",
    "                        print()\n",
    "                        print(grid.best_params_)\n",
    "                        # print()\n",
    "                        # print(\"Grade de valores encontrados:\")\n",
    "                        # print()\n",
    "                        medias = grid.cv_results_['mean_test_score']\n",
    "                        desvios = grid.cv_results_['std_test_score']\n",
    "                        # for media, desvio, params in zip(medias, desvios, grid.cv_results_['params']):\n",
    "                        #     print(\"%0.3f (+/-%0.03f) for %r\" % (media, desvio * 2, params))\n",
    "                        #     print()\n",
    "\n",
    "                        rfc = RandomForestClassifier(**grid.best_params_, random_state=500)\n",
    "\n",
    "                    except ValueError:\n",
    "\n",
    "                        rfc = RandomForestClassifier(random_state=500)\n",
    "\n",
    "            #         rfc = RandomForestClassifier(parameters = grid.best_params_, random_state=12)\n",
    "\n",
    "                    kfold = KFold(n_splits=2, random_state=seed)\n",
    "\n",
    "                    try:\n",
    "\n",
    "                        kfold = StratifiedKFold(n_splits=10, random_state=seed)\n",
    "                        kfoldUtilizado = \"skf-10\"\n",
    "\n",
    "                    except ValueError:\n",
    "\n",
    "                        try:\n",
    "\n",
    "                            print(\"Primeiro erro!\")\n",
    "                            kfold = KFold(n_splits=10, random_state=seed)\n",
    "                            kfoldUtilizado = \"kf-10\"\n",
    "\n",
    "                        except ValueError:\n",
    "\n",
    "                            try:\n",
    "\n",
    "                                print(\"Segundo erro!\")\n",
    "                                kfold = StratifiedKFold(n_splits=5, random_state=seed)\n",
    "                                kfoldUtilizado = \"skf-5\"\n",
    "\n",
    "                            except ValueError:\n",
    "\n",
    "                                try:\n",
    "\n",
    "                                    print(\"Terceiro erro!\")\n",
    "                                    kfold = KFold(n_splits=5, random_state=seed)\n",
    "                                    kfoldUtilizado = \"kf-5\"\n",
    "                                \n",
    "                                except ValueError:\n",
    "\n",
    "                                    try:\n",
    "\n",
    "                                        print(\"Quarto erro!\")\n",
    "                                        kfold = KFold(n_splits=2, random_state=seed)\n",
    "                                        kfoldUtilizado = \"kf-2\"\n",
    "\n",
    "                                    except:\n",
    "                                        print(\"ERRO NA VALIDAÇÃO CRUZADA!\")\n",
    "                                        print(\"kfold utilizado: {0}\".format(kfoldUtilizado))\n",
    "                                        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "                                        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "                                        print(exc_type, fname, exc_tb.tb_lineno)\n",
    "                                        print(sys.exc_info())\n",
    "\n",
    "                    #Isso só serve para validar o uso do conjunto de treino \n",
    "                    #Não influencia no resultado final\n",
    "        #             try:\n",
    "        #                 cv_results2 = model_selection.cross_val_score(rfc, X_train, y_train, cv=kfold, scoring='f1')\n",
    "        #                 msg = \"%s com hiperparâmetros tunados: F1 = %f com desvio padrão = %f\" % ('RFC', cv_results2.mean(), cv_results2.std())\n",
    "        #                 print(msg)\n",
    "\n",
    "        #             except:\n",
    "        #                 print(\"ERRO NA VALIDAÇÃO CRUZADA!\")\n",
    "        #                 exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        #                 fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        #                 print(exc_type, fname, exc_tb.tb_lineno)\n",
    "        #                 print(sys.exc_info())\n",
    "\n",
    "\n",
    "                    rfc.fit(X_train, y_train) #Treinando o modelo\n",
    "                    predicoes = rfc.predict(X_test) #Realizando a predição\n",
    "\n",
    "            #         from sklearn.metrics import confusion_matrix\n",
    "            #         matriz = confusion_matrix(y_test, predicoes)\n",
    "\n",
    "            #         print(matriz)\n",
    "\n",
    "                    from sklearn.metrics import classification_report\n",
    "                    from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "                    # print(classification_report(y_test, predicoes))\n",
    "            #         prec, rec, fbeta, supp = precision_recall_fscore_support(y_test, predicoes, average=None)\n",
    "            #         print ('precision: %.5f' %(prec[1]))\n",
    "            #         print ('recall: %.5f' %(rec[1]))\n",
    "            #         print ('f1: %.5f' %(fbeta[1]))\n",
    "            #         print ('non-matches: %d - matches: %d' %(supp[0], supp[1]))\n",
    "            #         print('')\n",
    "\n",
    "                    # from sklearn.externals.six import StringIO  \n",
    "                    # from IPython.display import Image  \n",
    "                    # from sklearn.tree import export_graphviz\n",
    "                    # import pydotplus\n",
    "                    # dot_data = StringIO()\n",
    "                    # export_graphviz(rfc, out_file=dot_data,  \n",
    "                    #                 filled=True, rounded=True,\n",
    "                    #                 special_characters=True, feature_names=X.columns)\n",
    "                    # graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "                    # Image(graph.create_png())\n",
    "\n",
    "                    predicoes = rfc.predict(XtoClass)\n",
    "\n",
    "                    from sklearn.metrics import confusion_matrix\n",
    "                    matriz = confusion_matrix(ytoClass, predicoes)\n",
    "                    print(matriz)\n",
    "\n",
    "                    this_tn, this_fp, this_fn, this_tp = 0, 0, 0, 0\n",
    "\n",
    "                    this_tn, this_fp, this_fn, this_tp = confusion_matrix(ytoClass, predicoes, labels=[0,1]).ravel()\n",
    "                    print('tn, fp, fn, tp')\n",
    "                    print(this_tn, this_fp, this_fn, this_tp)\n",
    "\n",
    "\n",
    "\n",
    "                    from sklearn.metrics import classification_report\n",
    "                    from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "                    # print(classification_report(y_test, predicoes))\n",
    "            #         prec, rec, fbeta, supp = precision_recall_fscore_support(ytoClass, predicoes, average=None)\n",
    "            #         print ('precision: %.5f' %(prec[1]))\n",
    "            #         print ('recall: %.5f' %(rec[1]))\n",
    "            #         print ('f1: %.5f' %(fbeta[1]))\n",
    "            #         print ('non-matches: %d - matches: %d' %(supp[0], supp[1]))\n",
    "            #         print('')\n",
    "\n",
    "\n",
    "                    ########################\n",
    "\n",
    "                    #Atualizar as estatísticas\n",
    "                    abordagem = 'DS'\n",
    "                    #print 'abordagem é %s' %(abordagem)\n",
    "\n",
    "                    #algUtl = linhaAtual['algoritmosUtilizados'].item()\n",
    "                    iteracao = 2\n",
    "                    inspecoesManuais = linhaAtual['inspecoesManuais'].item()\n",
    "                    da = linhaAtual['da'].item() + (this_tp + this_fp) #Adiciona todos os casos positivos identificados\n",
    "                    dm = linhaAtual['dm'].item()             #na predição\n",
    "                    ndm = linhaAtual['ndm'].item()\n",
    "\n",
    "\n",
    "                    fn = float(linhaAtual['fn'].item() - this_tp ) #Não tenho certeza se é isso\n",
    "                    tp = this_tp + float(linhaAtual['tp'].item()) #+ dm) #Recuperar de toClass\n",
    "#                     print('tp é igual a: %d' %(tp))\n",
    "\n",
    "                    fp = this_fp + float(linhaAtual['fp'].item())\n",
    "                    #tn = tn + float(linhaAtual['tn'].item() - tp) #+ ndm) #Recuperar de toClass\n",
    "                    tn = (2616 * 2294) -(tp+fp+fn)\n",
    "\n",
    "\n",
    "\n",
    "                    precision = tp/(tp+fp)\n",
    "                    recall = tp/(tp+fn)\n",
    "                    fmeasure = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "                    #Adicionando valor à última linha\n",
    "            #         estatisticas.loc[(etapa, permutacao), ['abordagem', 'algoritmosUtilizados', 'iteracao', 'inspecoesManuais',\n",
    "            #            'precision', 'recall', 'f-measure', 'da', 'dm', 'ndm', 'tp',\n",
    "            #            'fp', 'tn', 'fn'] ] = ([abordagem, algUtl, iteracao, inspecoesManuais,\n",
    "            #            precision, recall, fmeasure, da, dm, ndm, tp, fp, tn, fn])\n",
    "                    estatisticas.loc[(algUtl, permutacao, etapa), ['abordagem', 'iteracao', 'inspecoesManuais',\n",
    "                        'precision', 'recall', 'f-measure', 'da', 'dm', 'ndm', 'tp',\n",
    "                        'fp', 'tn', 'fn'] ] = ([abordagem, iteracao, inspecoesManuais,\n",
    "                        precision, recall, fmeasure, da, dm, ndm, tp, fp, tn, fn])\n",
    "\n",
    "                    #'''\n",
    "\n",
    "            # estatisticas = estatisticas.reset_index(level=['etapa', 'permutacao'])\n",
    "            estatisticas = estatisticas.reset_index(level=['algoritmosUtilizados', 'permutacao', 'etapa'])\n",
    "            # estatisticas.head()                   \n",
    "            #                    \n",
    "\n",
    "            estatisticas = estatisticas[['abordagem', 'etapa', 'algoritmosUtilizados', 'permutacao', 'iteracao', 'inspecoesManuais', 'precision', 'recall', 'f-measure', 'da', 'dm', 'ndm', 'tp', 'fp', 'tn', 'fn']]\n",
    "\n",
    "            estatisticas[['algoritmosUtilizados', 'iteracao', 'inspecoesManuais', 'da', 'dm', 'ndm', 'tp', 'fp', 'tn', 'fn']] = \\\n",
    "            estatisticas[['algoritmosUtilizados', 'iteracao', 'inspecoesManuais', 'da', 'dm', 'ndm', 'tp', 'fp', 'tn', 'fn']].astype(int)\n",
    "\n",
    "#             dirEst = \"../../csv/\"\n",
    "            # dirEst = \"c:/Users/Diego/Documents/NetBeansProjects/Master-SKYAM/AS/src/csv/\"\n",
    "            # dirEst = \"../arqResult/csv/\"\n",
    "        \n",
    "            if(k == \"ptChr\"): #Se Peter Christen\n",
    "                estatisticas.to_csv(dirEstat+'estatisticaDS3-PtChr.csv', sep=';', index=False)        \n",
    "            elif(k == \"dgArj\"): #Se Diego Araújo\n",
    "                estatisticas.to_csv(dirEstat+'estatisticaDS3-DgArj.csv', sep=';', index=False) \n",
    "            else: #Se Random\n",
    "                estatisticas.to_csv(dirEstat+'estatisticaDS3-Random.csv', sep=';', index=False)        \n",
    "\n",
    "                \n",
    "                \n",
    "            ####################### \n",
    "            #ATENÇÃO, DIEGO!#\n",
    "            #Adicionar outro classificador aqui\n",
    "            \n",
    "#             print('lista aqui:')\n",
    "#             print(lista)\n",
    "            \n",
    "#         for k in abordagens:\n",
    "            \n",
    "#             abordagemAA = \"\"\n",
    "            \n",
    "#             dirOrig = \"\"\n",
    "#             dirEstat = \"\"\n",
    "#             estat = \"\"\n",
    "            \n",
    "#             if(k == \"ptChr\"): #Se Peter Christen\n",
    "                \n",
    "#                 abordagemAA = '2 - AA[pet-chr]'\n",
    "            \n",
    "#                 dirOrig = \"../../csv/conjuntosDS/treinoTeste/\"+base+\"/\"+qp+\"/\"+\"PtChr/\"\n",
    "#                 dirEstat = \"../../csv/estatisticas/\"+base+\"/\"+qp+\"/\"\n",
    "#                 estat = dirEstat+\"estatisticaDS2-PtChr.csv\"\n",
    "#     #   \n",
    "#             elif(k == \"dgArj\"): #Se Diego Araújo\n",
    "            \n",
    "#                 abordagemAA = '2 - AA[dg-arj]'\n",
    "            \n",
    "#                 dirOrig = \"../../csv/conjuntosDS/treinoTeste/\"+base+\"/\"+qp+\"/\"+\"DgArj/\"\n",
    "#                 dirEstat = \"../../csv/estatisticas/\"+base+\"/\"+qp+\"/\"\n",
    "#                 estat = dirEstat+\"estatisticaDS2-DgArj.csv\"\n",
    "                \n",
    "#             elif(k == \"rand\"): #Se Random\n",
    "            \n",
    "#                 abordagemAA = '2 - AA[random]'\n",
    "            \n",
    "#                 dirOrig = \"../../csv/conjuntosDS/treinoTeste/\"+base+\"/\"+qp+\"/\"+\"Random/\"\n",
    "#                 dirEstat = \"../../csv/estatisticas/\"+base+\"/\"+qp+\"/\"\n",
    "#                 estat = dirEstat+\"estatisticaDS2-Random.csv\"\n",
    "                \n",
    "            \n",
    "            print(\"**************************\")\n",
    "            print(\"Base: {0} - {1} - Abordagem: {2} - - Classificador: svm\".format(base,qp, abordagemAA))\n",
    "            print(\"**************************\")\n",
    "            \n",
    "            print(dirOrig)\n",
    "\n",
    "#             etapa = '3 - clasf'\n",
    "\n",
    "            # estatisticas = pd.read_csv(estat, index_col=['etapa','permutacao'], sep=';')\n",
    "            estatisticas = pd.read_csv(estat, index_col=['algoritmosUtilizados','permutacao','etapa'], sep=';')\n",
    "        #     estatisticas.tail()\n",
    "\n",
    "            arquivos = os.listdir(dirOrig)\n",
    "            # print(arquivos)\n",
    "            #print(len(arquivos))\n",
    "\n",
    "            train = []\n",
    "            test = []\n",
    "\n",
    "            for arq in arquivos:\n",
    "                if arq.startswith(\"train\"):\n",
    "                    train.append(arq)\n",
    "                elif arq.startswith(\"test\"):\n",
    "                    test.append(arq)\n",
    "\n",
    "            # print('tamanho de train: %d' %(len(train)))        \n",
    "\n",
    "            # print('tamanho de test: %d' %(len(test)))\n",
    "\n",
    "            # print (train)    \n",
    "            # print (test)    \n",
    "\n",
    "\n",
    "            #Desnecessário - Já setado acima\n",
    "            #lista = ['10', '15', '20', '25'] #Assumindo que serão esses os conjuntos de algoritmos\n",
    "            #ATENÇÃO, DIEGO!! ISSO MUDA PARA QP2 E QP3\n",
    "#             if (qps == \"qp1\"):\n",
    "#                 lista = ['5', '15', '25'] #Assumindo que serão esses os conjuntos de algoritmos \n",
    "#             elif (qps \"qp2b\"):\n",
    "#                 lista == ['5'] #Assumindo que serão esses os conjuntos de algoritmos \n",
    "#             elif (qps \"qp2m\"):\n",
    "#                 lista == ['8'] #Assumindo que serão esses os conjuntos de algoritmos \n",
    "#             elif (qps \"qp2r\"):\n",
    "#                 lista == ['4'] #Assumindo que serão esses os conjuntos de algoritmos \n",
    "#             elif (qps \"qp3all\"):\n",
    "#                 lista == ['8'] #Assumindo que serão esses os conjuntos de algoritmos \n",
    "#             elif (qps \"qp3lot\"):\n",
    "#                 lista == ['5'] #Assumindo que serão esses os conjuntos de algoritmos \n",
    "\n",
    "            for i in lista: \n",
    "\n",
    "                cont = 0\n",
    "\n",
    "                trainAtual = []\n",
    "                testAtual = []\n",
    "\n",
    "                for arq in train:\n",
    "                    if '('+i+')' in arq:\n",
    "        #             if 'train(10)22' in arq:    \n",
    "                        trainAtual.append(arq)\n",
    "                for arq in test:\n",
    "                    if '('+i+')' in arq:\n",
    "        #             if 'test(10)22' in arq:\n",
    "                        testAtual.append(arq)\n",
    "\n",
    "            #     print('lista desordenada')\n",
    "            #     print(trainAtual)\n",
    "\n",
    "            #     print('tamanho antes: %d' %(len(trainAtual)))\n",
    "\n",
    "                #Ordenando a lista\n",
    "                trainAtual = sorted(trainAtual)\n",
    "                testAtual = sorted(testAtual)\n",
    "\n",
    "            #     for x in trainAtual:\n",
    "            #         print(x)\n",
    "\n",
    "\n",
    "\n",
    "            # #     print('lista ordenada')\n",
    "            # #     print(trainAtual)\n",
    "\n",
    "            #     print('tamanho depois: %d' %(len(trainAtual)))\n",
    "\n",
    "            #     print(type(trainAtual))\n",
    "\n",
    "\n",
    "                print (i)\n",
    "\n",
    "            #     print(trainAtual)\n",
    "            #     print(testAtual)\n",
    "\n",
    "                tam = len(trainAtual) #Mesma coisa para testAtual\n",
    "\n",
    "                for pos in range(tam):\n",
    "            #         print(pos)\n",
    "                    print('')\n",
    "                    print('##########################')\n",
    "                    print('Arquivos atuais: %s e %s' %(trainAtual[pos], testAtual[pos]))\n",
    "                    print('##########################')\n",
    "\n",
    "                    treino = dirOrig+trainAtual[pos]\n",
    "                    teste = dirOrig+testAtual[pos]\n",
    "\n",
    "            #         print('treino: %s' %(treino))\n",
    "\n",
    "            #         tp, fp, tn, fn = 0\n",
    "\n",
    "                    num = trainAtual[pos].replace('train('+i+')','')\n",
    "                    num = num.replace('.csv','')\n",
    "\n",
    "                    cont += 1\n",
    "\n",
    "                    print('Iteração: %d' %(cont))\n",
    "\n",
    "            #         print('num: %s' %(num))\n",
    "\n",
    "                    algUtl = re.sub('train.*\\(', r'', trainAtual[pos]) #Alterar para fazer a substituição de tudo em uma linha só\n",
    "                    algUtl = re.sub('\\).*', r'', algUtl) #Alterar para fazer a substituição de tudo em uma linha só\n",
    "                    algUtl = int(algUtl)\n",
    "\n",
    "\n",
    "                    print('algUtl: %d' %(algUtl))\n",
    "\n",
    "                    permutacao = int(num)\n",
    "            #         linhaAtual = estatisticas.loc[('2 - AA[pet-chr]', permutacao), : ] #Armazena a linha correspondente à permutação\n",
    "            #         linhaAtual = estatisticas.loc[('1 - acm diverg', permutacao), : ] #Armazena a linha correspondente à permutação\n",
    "                    linhaAtual = estatisticas.loc[algUtl, permutacao, abordagemAA, : ] #Armazena a linha correspondente à permutação\n",
    "        #             print (linhaAtual)\n",
    "        #             print(linhaAtual.columns)\n",
    "\n",
    "                #'''\n",
    "            #         all = pd.read_csv(\"train.csv\", index_col=False, sep=';', names=('title', 'artist', 'track01', 'track02', 'track03', 'duplicata'), header = None)\n",
    "            #         toClass = pd.read_csv(\"test.csv\", index_col=False, sep=';', names=('title', 'artist', 'track01', 'track02', 'track03', 'duplicata'), header = None)\n",
    "\n",
    "            #         all = pd.read_csv(treino, index_col=False, sep=';', names=('title', 'artist', 'track01', 'track02', 'track03', 'duplicata'), header = None)\n",
    "            #         toClass = pd.read_csv(teste, index_col=False, sep=';', names=('title', 'artist', 'track01', 'track02', 'track03', 'duplicata'), header = None)\n",
    "\n",
    "                    all = pd.read_csv(treino, index_col=False, sep=';', names=('title', 'authors', 'venue', 'year', 'duplicata'), header = 0) #Era header = None\n",
    "                    toClass = pd.read_csv(teste, index_col=False, sep=';', names=('title', 'authors', 'venue', 'year', 'duplicata'), header = 0) #Era header = None\n",
    "\n",
    "\n",
    "                    # all['artist-title'] = all['artist'] * all['title']\n",
    "                    # cols = list(all.columns.values)\n",
    "                    # cols.pop(cols.index('duplicata'))\n",
    "                    # all = all[cols+['duplicata']]\n",
    "\n",
    "                    #Pesos baseados no Alg17\n",
    "\n",
    "#                     all['artist-title'] = all['artist'] * all['title']\n",
    "                    all['soma-pesos'] = (all['title']*2 + all['authors']*0.5 + all['venue']*0.5 + all['year']*1)/4.0\n",
    "                    cols = list(all.columns.values)\n",
    "                    cols.pop(cols.index('duplicata'))\n",
    "                    all = all[cols+['duplicata']]\n",
    "\n",
    "                    # toClass['artist-title'] = toClass['artist'] * toClass['title']\n",
    "                    # cols = list(toClass.columns.values)\n",
    "                    # cols.pop(cols.index('duplicata'))\n",
    "                    # toClass = toClass[cols+['duplicata']]\n",
    "\n",
    "                    #Pesos baseados no Alg17\n",
    "\n",
    "#                     toClass['artist-title'] = toClass['artist'] * toClass['title']\n",
    "                    toClass['soma-pesos'] = (toClass['title']*2 + toClass['authors']*0.5 + toClass['venue']*0.5 + toClass['year']*1)/4.0\n",
    "                    cols = list(toClass.columns.values)\n",
    "                    cols.pop(cols.index('duplicata'))\n",
    "                    toClass = toClass[cols+['duplicata']]\n",
    "\n",
    "                    #Separação do conjunto X do conjunto y\n",
    "                    # X = all.loc[:,'title':'artist-title']\n",
    "                    X = all.loc[:,'title':'soma-pesos']\n",
    "                    y = all.duplicata\n",
    "\n",
    "                    # XtoClass = toClass.loc[:,'title':'artist-title']\n",
    "                    XtoClass = toClass.loc[:,'title':'soma-pesos']\n",
    "                    ytoClass = toClass.duplicata\n",
    "\n",
    "                    from sklearn import model_selection\n",
    "\n",
    "                    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.20, random_state=2)\n",
    "\n",
    "                    from sklearn.svm import SVC\n",
    "                    from sklearn.model_selection import StratifiedKFold\n",
    "                    from sklearn.model_selection import KFold\n",
    "                    from sklearn import model_selection\n",
    "\n",
    "                    seed = 500\n",
    "\n",
    "                    '''\n",
    "\n",
    "                    from sklearn.linear_model import LogisticRegression\n",
    "                    from sklearn.tree import DecisionTreeClassifier\n",
    "                    from sklearn.neighbors import SVC\n",
    "                    from sklearn.svm import SVC\n",
    "\n",
    "                    modelos = []\n",
    "                    modelos.append(('LR', LogisticRegression()))\n",
    "                    modelos.append(('KNN', SVC()))\n",
    "                    modelos.append(('RFC', DecisionTreeClassifier()))\n",
    "                    modelos.append(('SVM', SVC()))\n",
    "                    modelos.append(('RF', RandomForestClassifier()))\n",
    "\n",
    "\n",
    "\n",
    "                    # Avaliação de cada modelo por vez\n",
    "                    resultados = []\n",
    "                    nomes = []\n",
    "                    for nome, modelo in modelos:\n",
    "                        kfold = StratifiedKFold(n_splits=10, random_state=seed) #Mudar para n-fold\n",
    "                        cv_results = model_selection.cross_val_score(modelo, X_train, y_train, cv=kfold, scoring='f1')\n",
    "                        msg = \"%s: %f (%f)\" % (nome, cv_results.mean(), cv_results.std()) #Iprimir em um arquivo\n",
    "                        print(msg)\n",
    "                    '''\n",
    "\n",
    "                    svm = SVC()\n",
    "\n",
    "                    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "                    param_grid = [{\"C\": [0.001, 0.1, 10, 100, 1000], \"kernel\": ['rbf', 'sigmoid' , 'linear'],\n",
    "                                  \"class_weight\": [\"balanced\",None],\n",
    "                                  \"gamma\": [1e-2, 1e-3, 1e-4, 1e-5]}]\n",
    "                    score = 'f1'\n",
    "\n",
    "                    print(\"# Tunando hiperparâmetros para %s\" % score)\n",
    "                    print()\n",
    "\n",
    "                    svm = SVC()\n",
    "\n",
    "                    try:\n",
    "\n",
    "                        grid = GridSearchCV(svm, param_grid, cv=2, scoring='%s_macro' % score)\n",
    "\n",
    "                        try:\n",
    "\n",
    "                            grid = GridSearchCV(svm, param_grid, cv=5, scoring='%s_macro' % score)\n",
    "                            grid.fit(X_train, y_train)\n",
    "\n",
    "                        except ValueError:\n",
    "\n",
    "                            try:\n",
    "\n",
    "                                grid = GridSearchCV(svm, param_grid, cv=3, scoring='%s_macro' % score)\n",
    "                                grid.fit(X_train, y_train)\n",
    "\n",
    "                            except ValueError:\n",
    "\n",
    "                                grid = GridSearchCV(svm, param_grid, cv=2, scoring='%s_macro' % score)\n",
    "                                grid.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "        #                 print(\"Melhor conjunto de hiperparâmetros encontrado:\")\n",
    "        #                 print()\n",
    "        #                 print(grid.best_params_)\n",
    "                        # print()\n",
    "                        # print(\"Grade de valores encontrados:\")\n",
    "                        # print()\n",
    "        #                 medias = grid.cv_results_['mean_test_score']\n",
    "        #                 desvios = grid.cv_results_['std_test_score']\n",
    "                        # for media, desvio, params in zip(medias, desvios, grid.cv_results_['params']):\n",
    "                        #     print(\"%0.3f (+/-%0.03f) for %r\" % (media, desvio * 2, params))\n",
    "                        #     print()\n",
    "\n",
    "                        svm = SVC(**grid.best_params_)\n",
    "\n",
    "                    except ValueError:\n",
    "\n",
    "                        svm = SVC()\n",
    "\n",
    "            #         rfc = RandomForestClassifier(parameters = grid.best_params_, random_state=12)\n",
    "\n",
    "                    kfold = KFold(n_splits=2, random_state=seed)\n",
    "\n",
    "                    try:\n",
    "\n",
    "                        kfold = StratifiedKFold(n_splits=10, random_state=seed)\n",
    "                        kfoldUtilizado = \"skf-10\"\n",
    "\n",
    "                    except ValueError:\n",
    "\n",
    "                        try:\n",
    "\n",
    "                            print(\"Primeiro erro!\")\n",
    "                            kfold = KFold(n_splits=10, random_state=seed)\n",
    "                            kfoldUtilizado = \"kf-10\"\n",
    "\n",
    "                        except ValueError:\n",
    "\n",
    "                            try:\n",
    "\n",
    "                                print(\"Segundo erro!\")\n",
    "                                kfold = StratifiedKFold(n_splits=5, random_state=seed)\n",
    "                                kfoldUtilizado = \"skf-5\"\n",
    "\n",
    "                            except ValueError:\n",
    "\n",
    "                                try:\n",
    "\n",
    "                                    print(\"Terceiro erro!\")\n",
    "                                    kfold = KFold(n_splits=5, random_state=seed)\n",
    "                                    kfoldUtilizado = \"kf-5\"\n",
    "                                \n",
    "                                except ValueError:\n",
    "\n",
    "                                    try:\n",
    "\n",
    "                                        print(\"Quarto erro!\")\n",
    "                                        kfold = KFold(n_splits=2, random_state=seed)\n",
    "                                        kfoldUtilizado = \"kf-2\"\n",
    "\n",
    "                                    except:\n",
    "                                        print(\"ERRO NA VALIDAÇÃO CRUZADA!\")\n",
    "                                        print(\"kfold utilizado: {0}\".format(kfoldUtilizado))\n",
    "                                        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "                                        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "                                        print(exc_type, fname, exc_tb.tb_lineno)\n",
    "                                        print(sys.exc_info())\n",
    "\n",
    "                    #Isso só serve para validar o uso do conjunto de treino \n",
    "                    #Não influencia no resultado final\n",
    "        #             try:\n",
    "        #                 cv_results2 = model_selection.cross_val_score(rfc, X_train, y_train, cv=kfold, scoring='f1')\n",
    "        #                 msg = \"%s com hiperparâmetros tunados: F1 = %f com desvio padrão = %f\" % ('RFC', cv_results2.mean(), cv_results2.std())\n",
    "        #                 print(msg)\n",
    "\n",
    "        #             except:\n",
    "        #                 print(\"ERRO NA VALIDAÇÃO CRUZADA!\")\n",
    "        #                 exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        #                 fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        #                 print(exc_type, fname, exc_tb.tb_lineno)\n",
    "        #                 print(sys.exc_info())\n",
    "\n",
    "\n",
    "                    try:\n",
    "                        svm.fit(X_train, y_train) #Treinando o modelo\n",
    "                    except ValueError:\n",
    "                        estatisticas.loc[(algUtl, permutacao, etapa), ['abordagem', 'iteracao', 'inspecoesManuais',\n",
    "                        'precision', 'recall', 'f-measure', 'da', 'dm', 'ndm', 'tp',\n",
    "                        'fp', 'tn', 'fn'] ] = (['ERRO-SVM', 2, inspecoesManuais,\n",
    "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "                        continue\n",
    "                    predicoes = svm.predict(X_test) #Realizando a predição\n",
    "\n",
    "            #         from sklearn.metrics import confusion_matrix\n",
    "            #         matriz = confusion_matrix(y_test, predicoes)\n",
    "\n",
    "            #         print(matriz)\n",
    "\n",
    "                    from sklearn.metrics import classification_report\n",
    "                    from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "                    # print(classification_report(y_test, predicoes))\n",
    "            #         prec, rec, fbeta, supp = precision_recall_fscore_support(y_test, predicoes, average=None)\n",
    "            #         print ('precision: %.5f' %(prec[1]))\n",
    "            #         print ('recall: %.5f' %(rec[1]))\n",
    "            #         print ('f1: %.5f' %(fbeta[1]))\n",
    "            #         print ('non-matches: %d - matches: %d' %(supp[0], supp[1]))\n",
    "            #         print('')\n",
    "\n",
    "                    # from sklearn.externals.six import StringIO  \n",
    "                    # from IPython.display import Image  \n",
    "                    # from sklearn.tree import export_graphviz\n",
    "                    # import pydotplus\n",
    "                    # dot_data = StringIO()\n",
    "                    # export_graphviz(rfc, out_file=dot_data,  \n",
    "                    #                 filled=True, rounded=True,\n",
    "                    #                 special_characters=True, feature_names=X.columns)\n",
    "                    # graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "                    # Image(graph.create_png())\n",
    "\n",
    "                    predicoes = svm.predict(XtoClass)\n",
    "\n",
    "                    from sklearn.metrics import confusion_matrix\n",
    "                    matriz = confusion_matrix(ytoClass, predicoes)\n",
    "                    print(matriz)\n",
    "\n",
    "                    this_tn, this_fp, this_fn, this_tp = 0, 0, 0, 0\n",
    "\n",
    "                    this_tn, this_fp, this_fn, this_tp = confusion_matrix(ytoClass, predicoes, labels=[0,1]).ravel()\n",
    "                    print('tn, fp, fn, tp')\n",
    "                    print(this_tn, this_fp, this_fn, this_tp)\n",
    "\n",
    "\n",
    "\n",
    "                    from sklearn.metrics import classification_report\n",
    "                    from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "                    # print(classification_report(y_test, predicoes))\n",
    "            #         prec, rec, fbeta, supp = precision_recall_fscore_support(ytoClass, predicoes, average=None)\n",
    "            #         print ('precision: %.5f' %(prec[1]))\n",
    "            #         print ('recall: %.5f' %(rec[1]))\n",
    "            #         print ('f1: %.5f' %(fbeta[1]))\n",
    "            #         print ('non-matches: %d - matches: %d' %(supp[0], supp[1]))\n",
    "            #         print('')\n",
    "\n",
    "\n",
    "                    ########################\n",
    "\n",
    "                    #Atualizar as estatísticas\n",
    "                    abordagem = 'DS'\n",
    "                    #print 'abordagem é %s' %(abordagem)\n",
    "\n",
    "                    #algUtl = linhaAtual['algoritmosUtilizados'].item()\n",
    "                    iteracao = 2\n",
    "                    inspecoesManuais = linhaAtual['inspecoesManuais'].item()\n",
    "                    da = linhaAtual['da'].item() + (this_tp + this_fp) #Adiciona todos os casos positivos identificados\n",
    "                    dm = linhaAtual['dm'].item()             #na predição\n",
    "                    ndm = linhaAtual['ndm'].item()\n",
    "\n",
    "\n",
    "                    fn = float(linhaAtual['fn'].item() - this_tp ) #Não tenho certeza se é isso\n",
    "                    tp = this_tp + float(linhaAtual['tp'].item()) #+ dm) #Recuperar de toClass\n",
    "                    print('tp é igual a: %d' %(tp))\n",
    "\n",
    "                    fp = this_fp + float(linhaAtual['fp'].item())\n",
    "                    #tn = tn + float(linhaAtual['tn'].item() - tp) #+ ndm) #Recuperar de toClass\n",
    "                    tn = (2616 * 2294) -(tp+fp+fn)\n",
    "\n",
    "\n",
    "\n",
    "                    precision = tp/(tp+fp)\n",
    "                    recall = tp/(tp+fn)\n",
    "                    fmeasure = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "                    #Adicionando valor à última linha\n",
    "            #         estatisticas.loc[(etapa, permutacao), ['abordagem', 'algoritmosUtilizados', 'iteracao', 'inspecoesManuais',\n",
    "            #            'precision', 'recall', 'f-measure', 'da', 'dm', 'ndm', 'tp',\n",
    "            #            'fp', 'tn', 'fn'] ] = ([abordagem, algUtl, iteracao, inspecoesManuais,\n",
    "            #            precision, recall, fmeasure, da, dm, ndm, tp, fp, tn, fn])\n",
    "                    estatisticas.loc[(algUtl, permutacao, etapa), ['abordagem', 'iteracao', 'inspecoesManuais',\n",
    "                        'precision', 'recall', 'f-measure', 'da', 'dm', 'ndm', 'tp',\n",
    "                        'fp', 'tn', 'fn'] ] = ([abordagem, iteracao, inspecoesManuais,\n",
    "                        precision, recall, fmeasure, da, dm, ndm, tp, fp, tn, fn])\n",
    "\n",
    "                    #'''\n",
    "\n",
    "            # estatisticas = estatisticas.reset_index(level=['etapa', 'permutacao'])\n",
    "            estatisticas = estatisticas.reset_index(level=['algoritmosUtilizados', 'permutacao', 'etapa'])\n",
    "            # estatisticas.head()                   \n",
    "            #                    \n",
    "\n",
    "            estatisticas = estatisticas[['abordagem', 'etapa', 'algoritmosUtilizados', 'permutacao', 'iteracao', 'inspecoesManuais', 'precision', 'recall', 'f-measure', 'da', 'dm', 'ndm', 'tp', 'fp', 'tn', 'fn']]\n",
    "\n",
    "            estatisticas[['algoritmosUtilizados', 'iteracao', 'inspecoesManuais', 'da', 'dm', 'ndm', 'tp', 'fp', 'tn', 'fn']] = \\\n",
    "            estatisticas[['algoritmosUtilizados', 'iteracao', 'inspecoesManuais', 'da', 'dm', 'ndm', 'tp', 'fp', 'tn', 'fn']].astype(int)\n",
    "\n",
    "        #     dirEst = \"../../csv/\"\n",
    "            # dirEst = \"c:/Users/Diego/Documents/NetBeansProjects/Master-SKYAM/AS/src/csv/\"\n",
    "            # dirEst = \"../arqResult/csv/\"\n",
    "            \n",
    "            if(k == \"ptChr\"): #Se Peter Christen\n",
    "                estatisticas.to_csv(dirEstat+'estatisticaDS3-PtChr-svm.csv', sep=';', index=False)        \n",
    "            elif(k == \"dgArj\"): #Se Diego Araújo\n",
    "                estatisticas.to_csv(dirEstat+'estatisticaDS3-DgArj-svm.csv', sep=';', index=False) \n",
    "            else: #Se Random\n",
    "                estatisticas.to_csv(dirEstat+'estatisticaDS3-Random-svm.csv', sep=';', index=False)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
