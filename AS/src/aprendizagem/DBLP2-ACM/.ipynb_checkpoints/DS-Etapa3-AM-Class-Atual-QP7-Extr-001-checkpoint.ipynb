{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Classificação a partir de conjuntos de treinamento gerados em QP6 - Janela 0.01 - Extremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\"\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# janelas = [1, 0.01, 0.03, 0.05] # K = {1, 1%, 3%, 5%}\n",
    "# janelas = [0.01] #Quando houver a janela vencedora. Após testar as combinações acima\n",
    "                 #com todas as bases. Lembrando que só deve ser usada para as demais QP além e QP6\n",
    "\n",
    "# regioes = [\"incert\", \"extrm\"]\n",
    "regioes = [\"extrm\"] #Incerteza eu já tenho de QP6\n",
    "# bases = [\"cds\", \"cds1\", \"cds2\"]\n",
    "bases = [\"DBLP2-ACM\"]\n",
    "qps = [\"qp7\"] #Apenas QP7.\n",
    "k = 0.01\n",
    "\n",
    "\n",
    "for base in bases:\n",
    "    \n",
    "    print(\"Base atual: {0}\".format(base))\n",
    "    \n",
    "    for qp in qps:\n",
    "        \n",
    "        print(\"QP atual: {0}\".format(qp))\n",
    "    \n",
    "        for regiao in regioes: #Essa repetição vai ser necessária apenas para QP6\n",
    "\n",
    "            abordagemAA = '2 - AA[dg-arj]' #'2 - AA[pet-chr]' ou '2 - AA[dg-arj]'\n",
    "#             dirOrig = \"../../csv/conjuntosDS/treinoTeste/\"+base+\"/\"+qp+\"/\"+\"DgArj-\"+regiao+\"/\"\n",
    "            dirOrig = \"../../csv/conjuntosDS/treinoTeste/\"+base+\"/\"+qp+\"/\"+\"/DgArj-\"+regiao+\"-k\"+str(k)+\"/\"\n",
    "        #     dirOrig = \"../../csv/conjuntosDS/treinoTeste-k\"+str(k)+\"/\"\n",
    "            # dirOrig = \"c:/Users/Diego/Documents/NetBeansProjects/Master-SKYAM/AS/src/csv/conjuntosDS/treinoTeste/\"\n",
    "            # dirOrig = \"../arqResult/csv/conjuntosDS/conjuntosDiverg/treinoTeste/\"\n",
    "            # estat = \"../../csv/estatisticaDS2.csv\"\n",
    "            dirEstat = \"../../csv/estatisticas/\"+base+\"/\"+qp+\"/\"\n",
    "#             estat = dirEstat+\"estatisticaDS2-DgArj-\"+regiao+\".csv\" #Antes de variar as janelas\n",
    "            estat = dirEstat+\"estatisticaDS2-DgArj-\"+regiao+\"-k\"+str(k)+\".csv\"\n",
    "    #         estat = \"../../csv/estatisticaDS2-DgArj-k\"+str(k)+\".csv\"\n",
    "            # estat = \"c:/Users/Diego/Documents/NetBeansProjects/Master-SKYAM/AS/src/csv/estatisticaDS2.csv\"\n",
    "            # estat = \"../arqResult/csv/estatisticaDS2.csv\"\n",
    "\n",
    "            print(dirOrig)\n",
    "\n",
    "            etapa = '3 - clasf'\n",
    "\n",
    "            # estatisticas = pd.read_csv(estat, index_col=['etapa','permutacao'], sep=';')\n",
    "            estatisticas = pd.read_csv(estat, index_col=['algoritmosUtilizados','permutacao','etapa'], sep=';')\n",
    "        #     estatisticas.tail()\n",
    "\n",
    "            arquivos = os.listdir(dirOrig)\n",
    "            # print(arquivos)\n",
    "            #print(len(arquivos))\n",
    "\n",
    "            train = []\n",
    "            test = []\n",
    "\n",
    "            for arq in arquivos:\n",
    "                if arq.startswith(\"train\"):\n",
    "                    train.append(arq)\n",
    "                elif arq.startswith(\"test\"):\n",
    "                    test.append(arq)\n",
    "\n",
    "            # print('tamanho de train: %d' %(len(train)))        \n",
    "\n",
    "            # print('tamanho de test: %d' %(len(test)))\n",
    "\n",
    "            # print (train)    \n",
    "            # print (test)    \n",
    "\n",
    "\n",
    "\n",
    "            #lista = ['10', '15', '20', '25'] #Assumindo que serão esses os conjuntos de algoritmos\n",
    "            #ATENÇÃO, DIEGO!! ISSO MUDA PARA QP2 E QP3\n",
    "            lista = ['5', '15', '25'] #Assumindo que serão esses os conjuntos de algoritmos \n",
    "        #     lista = ['10'] #Assumindo que serão esses os conjuntos de algoritmos\n",
    "            # lista = ['3'] #Assumindo que serão esses os conjuntos de algoritmos\n",
    "            \n",
    "            ####CLASSIFICAÇÃO COM RANDOM FOREST\n",
    "\n",
    "            for i in lista: \n",
    "\n",
    "                cont = 0\n",
    "\n",
    "                trainAtual = []\n",
    "                testAtual = []\n",
    "\n",
    "                for arq in train:\n",
    "                    if '('+i+')' in arq:\n",
    "        #             if 'train(10)22' in arq:    \n",
    "                        trainAtual.append(arq)\n",
    "                for arq in test:\n",
    "                    if '('+i+')' in arq:\n",
    "        #             if 'test(10)22' in arq:\n",
    "                        testAtual.append(arq)\n",
    "\n",
    "            #     print('lista desordenada')\n",
    "            #     print(trainAtual)\n",
    "\n",
    "            #     print('tamanho antes: %d' %(len(trainAtual)))\n",
    "\n",
    "                #Ordenando a lista\n",
    "                trainAtual = sorted(trainAtual)\n",
    "                testAtual = sorted(testAtual)\n",
    "\n",
    "            #     for x in trainAtual:\n",
    "            #         print(x)\n",
    "\n",
    "\n",
    "\n",
    "            # #     print('lista ordenada')\n",
    "            # #     print(trainAtual)\n",
    "\n",
    "            #     print('tamanho depois: %d' %(len(trainAtual)))\n",
    "\n",
    "            #     print(type(trainAtual))\n",
    "\n",
    "\n",
    "                print (i)\n",
    "\n",
    "            #     print(trainAtual)\n",
    "            #     print(testAtual)\n",
    "\n",
    "                tam = len(trainAtual) #Mesma coisa para testAtual\n",
    "\n",
    "                for pos in range(tam):\n",
    "            #         print(pos)\n",
    "                    print('')\n",
    "                    print('##########################')\n",
    "                    print('Arquivos atuais: %s e %s' %(trainAtual[pos], testAtual[pos]))\n",
    "                    print('##########################')\n",
    "\n",
    "                    treino = dirOrig+trainAtual[pos]\n",
    "                    teste = dirOrig+testAtual[pos]\n",
    "\n",
    "            #         print('treino: %s' %(treino))\n",
    "\n",
    "            #         tp, fp, tn, fn = 0\n",
    "\n",
    "                    num = trainAtual[pos].replace('train('+i+')','')\n",
    "                    num = num.replace('.csv','')\n",
    "\n",
    "                    cont += 1\n",
    "\n",
    "                    print('Iteração: %d' %(cont))\n",
    "\n",
    "            #         print('num: %s' %(num))\n",
    "\n",
    "                    algUtl = re.sub('train.*\\(', r'', trainAtual[pos]) #Alterar para fazer a substituição de tudo em uma linha só\n",
    "                    algUtl = re.sub('\\).*', r'', algUtl) #Alterar para fazer a substituição de tudo em uma linha só\n",
    "                    algUtl = int(algUtl)\n",
    "\n",
    "\n",
    "                    print('algUtl: %d' %(algUtl))\n",
    "\n",
    "                    permutacao = int(num)\n",
    "            #         linhaAtual = estatisticas.loc[('2 - AA[pet-chr]', permutacao), : ] #Armazena a linha correspondente à permutação\n",
    "            #         linhaAtual = estatisticas.loc[('1 - acm diverg', permutacao), : ] #Armazena a linha correspondente à permutação\n",
    "                    linhaAtual = estatisticas.loc[algUtl, permutacao, abordagemAA, : ] #Armazena a linha correspondente à permutação\n",
    "        #             print (linhaAtual)\n",
    "        #             print(linhaAtual.columns)\n",
    "\n",
    "                #'''\n",
    "            #         all = pd.read_csv(\"train.csv\", index_col=False, sep=';', names=('title', 'artist', 'track01', 'track02', 'track03', 'duplicata'), header = None)\n",
    "            #         toClass = pd.read_csv(\"test.csv\", index_col=False, sep=';', names=('title', 'artist', 'track01', 'track02', 'track03', 'duplicata'), header = None)\n",
    "\n",
    "            #         all = pd.read_csv(treino, index_col=False, sep=';', names=('title', 'artist', 'track01', 'track02', 'track03', 'duplicata'), header = None)\n",
    "            #         toClass = pd.read_csv(teste, index_col=False, sep=';', names=('title', 'artist', 'track01', 'track02', 'track03', 'duplicata'), header = None)\n",
    "\n",
    "                    all = pd.read_csv(treino, index_col=False, sep=';', names=('title', 'authors', 'venue', 'year', 'duplicata'), header = 0) #Era header = None\n",
    "                    toClass = pd.read_csv(teste, index_col=False, sep=';', names=('title', 'authors', 'venue', 'year', 'duplicata'), header = 0) #Era header = None\n",
    "\n",
    "\n",
    "                    # all['artist-title'] = all['artist'] * all['title']\n",
    "                    # cols = list(all.columns.values)\n",
    "                    # cols.pop(cols.index('duplicata'))\n",
    "                    # all = all[cols+['duplicata']]\n",
    "\n",
    "                    #Pesos baseados no Alg17\n",
    "\n",
    "                    #all['artist-title'] = all['artist'] * all['title']\n",
    "                    all['soma-pesos'] = (all['title']*2 + all['authors']*0.5 + all['venue']*0.5 + all['year']*1)/4.0\n",
    "                    cols = list(all.columns.values)\n",
    "                    cols.pop(cols.index('duplicata'))\n",
    "                    all = all[cols+['duplicata']]\n",
    "\n",
    "                    # toClass['artist-title'] = toClass['artist'] * toClass['title']\n",
    "                    # cols = list(toClass.columns.values)\n",
    "                    # cols.pop(cols.index('duplicata'))\n",
    "                    # toClass = toClass[cols+['duplicata']]\n",
    "\n",
    "                    #Pesos baseados no Alg17\n",
    "\n",
    "#                     toClass['artist-title'] = toClass['artist'] * toClass['title']\n",
    "                    toClass['soma-pesos'] = (toClass['title']*2 + toClass['authors']*0.5 + toClass['venue']*0.5 + toClass['year']*1)/4.0\n",
    "                    cols = list(toClass.columns.values)\n",
    "                    cols.pop(cols.index('duplicata'))\n",
    "                    toClass = toClass[cols+['duplicata']]\n",
    "\n",
    "                    #Separação do conjunto X do conjunto y\n",
    "                    # X = all.loc[:,'title':'artist-title']\n",
    "                    X = all.loc[:,'title':'soma-pesos']\n",
    "                    y = all.duplicata\n",
    "\n",
    "                    # XtoClass = toClass.loc[:,'title':'artist-title']\n",
    "                    XtoClass = toClass.loc[:,'title':'soma-pesos']\n",
    "                    ytoClass = toClass.duplicata\n",
    "\n",
    "                    from sklearn import model_selection\n",
    "\n",
    "                    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.20, random_state=2)\n",
    "\n",
    "                    from sklearn.ensemble import RandomForestClassifier\n",
    "                    from sklearn.model_selection import StratifiedKFold\n",
    "                    from sklearn.model_selection import KFold\n",
    "                    from sklearn import model_selection\n",
    "\n",
    "                    seed = 500\n",
    "\n",
    "                    '''\n",
    "\n",
    "                    from sklearn.linear_model import LogisticRegression\n",
    "                    from sklearn.tree import DecisionTreeClassifier\n",
    "                    from sklearn.neighbors import SVC\n",
    "                    from sklearn.svm import SVC\n",
    "\n",
    "                    modelos = []\n",
    "                    modelos.append(('LR', LogisticRegression()))\n",
    "                    modelos.append(('KNN', SVC()))\n",
    "                    modelos.append(('RFC', DecisionTreeClassifier()))\n",
    "                    modelos.append(('SVM', SVC()))\n",
    "                    modelos.append(('RF', RandomForestClassifier()))\n",
    "\n",
    "\n",
    "\n",
    "                    # Avaliação de cada modelo por vez\n",
    "                    resultados = []\n",
    "                    nomes = []\n",
    "                    for nome, modelo in modelos:\n",
    "                        kfold = StratifiedKFold(n_splits=10, random_state=seed) #Mudar para n-fold\n",
    "                        cv_results = model_selection.cross_val_score(modelo, X_train, y_train, cv=kfold, scoring='f1')\n",
    "                        msg = \"%s: %f (%f)\" % (nome, cv_results.mean(), cv_results.std()) #Iprimir em um arquivo\n",
    "                        print(msg)\n",
    "                    '''\n",
    "\n",
    "                    rfc = RandomForestClassifier(random_state=12)\n",
    "\n",
    "                    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "                    param_grid = [{\"criterion\": ['gini', 'entropy'],\"n_estimators\": [10,25,30,35,40,45,50,100],\n",
    "                                   #\"max_depth\": [2,3,4,5,6,7],\n",
    "                                  \"class_weight\": [\"balanced\",None],\n",
    "                                  \"min_samples_split\": [2, 5, 10]}]\n",
    "\n",
    "                    score = 'f1'\n",
    "\n",
    "                    print(\"# Tunando hiperparâmetros para %s\" % score)\n",
    "                    print()\n",
    "\n",
    "                    grid = GridSearchCV(rfc, param_grid, cv=2, scoring='%s_macro' % score)\n",
    "                    \n",
    "                    try:\n",
    "\n",
    "                        grid = GridSearchCV(rfc, param_grid, cv=5, scoring='%s_macro' % score)\n",
    "                        grid.fit(X_train, y_train)\n",
    "                        \n",
    "                    except ValueError:\n",
    "                        \n",
    "                        try:\n",
    "\n",
    "                            grid = GridSearchCV(rfc, param_grid, cv=3, scoring='%s_macro' % score)\n",
    "                            grid.fit(X_train, y_train)\n",
    "\n",
    "                        except ValueError:\n",
    "\n",
    "                            grid = GridSearchCV(rfc, param_grid, cv=2, scoring='%s_macro' % score)\n",
    "                            grid.fit(X_train, y_train)\n",
    "\n",
    "                    print(\"Melhor conjunto de hiperparâmetros encontrado:\")\n",
    "                    print()\n",
    "                    print(grid.best_params_)\n",
    "                    # print()\n",
    "                    # print(\"Grade de valores encontrados:\")\n",
    "                    # print()\n",
    "                    medias = grid.cv_results_['mean_test_score']\n",
    "                    desvios = grid.cv_results_['std_test_score']\n",
    "                    # for media, desvio, params in zip(medias, desvios, grid.cv_results_['params']):\n",
    "                    #     print(\"%0.3f (+/-%0.03f) for %r\" % (media, desvio * 2, params))\n",
    "                    #     print()\n",
    "\n",
    "                    rfc = RandomForestClassifier(**grid.best_params_, random_state=500)\n",
    "\n",
    "            #         rfc = RandomForestClassifier(parameters = grid.best_params_, random_state=12)\n",
    "\n",
    "                    kfold = KFold(n_splits=2, random_state=seed)\n",
    "\n",
    "                    try:\n",
    "\n",
    "                        kfold = StratifiedKFold(n_splits=10, random_state=seed)\n",
    "                        kfoldUtilizado = \"skf-10\"\n",
    "\n",
    "                    except ValueError:\n",
    "\n",
    "                        try:\n",
    "\n",
    "                            print(\"Primeiro erro!\")\n",
    "                            kfold = KFold(n_splits=10, random_state=seed)\n",
    "                            kfoldUtilizado = \"kf-10\"\n",
    "\n",
    "                        except ValueError:\n",
    "\n",
    "                            try:\n",
    "\n",
    "                                print(\"Segundo erro!\")\n",
    "                                kfold = StratifiedKFold(n_splits=5, random_state=seed)\n",
    "                                kfoldUtilizado = \"skf-5\"\n",
    "\n",
    "                            except ValueError:\n",
    "\n",
    "                                try:\n",
    "\n",
    "                                    print(\"Terceiro erro!\")\n",
    "                                    kfold = KFold(n_splits=5, random_state=seed)\n",
    "                                    kfoldUtilizado = \"kf-5\"\n",
    "                                \n",
    "                                except ValueError:\n",
    "\n",
    "                                    try:\n",
    "\n",
    "                                        print(\"Quarto erro!\")\n",
    "                                        kfold = KFold(n_splits=2, random_state=seed)\n",
    "                                        kfoldUtilizado = \"kf-2\"\n",
    "\n",
    "                                    except:\n",
    "                                        print(\"ERRO NA VALIDAÇÃO CRUZADA!\")\n",
    "                                        print(\"kfold utilizado: {0}\".format(kfoldUtilizado))\n",
    "                                        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "                                        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "                                        print(exc_type, fname, exc_tb.tb_lineno)\n",
    "                                        print(sys.exc_info())\n",
    "\n",
    "                    #Isso só serve para validar o uso do conjunto de treino \n",
    "                    #Não influencia no resultado final\n",
    "        #             try:\n",
    "        #                 cv_results2 = model_selection.cross_val_score(rfc, X_train, y_train, cv=kfold, scoring='f1')\n",
    "        #                 msg = \"%s com hiperparâmetros tunados: F1 = %f com desvio padrão = %f\" % ('RFC', cv_results2.mean(), cv_results2.std())\n",
    "        #                 print(msg)\n",
    "\n",
    "        #             except:\n",
    "        #                 print(\"ERRO NA VALIDAÇÃO CRUZADA!\")\n",
    "        #                 exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        #                 fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        #                 print(exc_type, fname, exc_tb.tb_lineno)\n",
    "        #                 print(sys.exc_info())\n",
    "\n",
    "\n",
    "                    rfc.fit(X_train, y_train) #Treinando o modelo\n",
    "                    predicoes = rfc.predict(X_test) #Realizando a predição\n",
    "\n",
    "            #         from sklearn.metrics import confusion_matrix\n",
    "            #         matriz = confusion_matrix(y_test, predicoes)\n",
    "\n",
    "            #         print(matriz)\n",
    "\n",
    "                    from sklearn.metrics import classification_report\n",
    "                    from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "                    # print(classification_report(y_test, predicoes))\n",
    "            #         prec, rec, fbeta, supp = precision_recall_fscore_support(y_test, predicoes, average=None)\n",
    "            #         print ('precision: %.5f' %(prec[1]))\n",
    "            #         print ('recall: %.5f' %(rec[1]))\n",
    "            #         print ('f1: %.5f' %(fbeta[1]))\n",
    "            #         print ('non-matches: %d - matches: %d' %(supp[0], supp[1]))\n",
    "            #         print('')\n",
    "\n",
    "                    # from sklearn.externals.six import StringIO  \n",
    "                    # from IPython.display import Image  \n",
    "                    # from sklearn.tree import export_graphviz\n",
    "                    # import pydotplus\n",
    "                    # dot_data = StringIO()\n",
    "                    # export_graphviz(rfc, out_file=dot_data,  \n",
    "                    #                 filled=True, rounded=True,\n",
    "                    #                 special_characters=True, feature_names=X.columns)\n",
    "                    # graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "                    # Image(graph.create_png())\n",
    "\n",
    "                    predicoes = rfc.predict(XtoClass)\n",
    "\n",
    "                    from sklearn.metrics import confusion_matrix\n",
    "                    matriz = confusion_matrix(ytoClass, predicoes)\n",
    "                    print(matriz)\n",
    "\n",
    "                    this_tn, this_fp, this_fn, this_tp = 0, 0, 0, 0\n",
    "\n",
    "                    this_tn, this_fp, this_fn, this_tp = confusion_matrix(ytoClass, predicoes, labels=[0,1]).ravel()\n",
    "                    print('tn, fp, fn, tp')\n",
    "                    print(this_tn, this_fp, this_fn, this_tp)\n",
    "\n",
    "\n",
    "\n",
    "                    from sklearn.metrics import classification_report\n",
    "                    from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "                    # print(classification_report(y_test, predicoes))\n",
    "            #         prec, rec, fbeta, supp = precision_recall_fscore_support(ytoClass, predicoes, average=None)\n",
    "            #         print ('precision: %.5f' %(prec[1]))\n",
    "            #         print ('recall: %.5f' %(rec[1]))\n",
    "            #         print ('f1: %.5f' %(fbeta[1]))\n",
    "            #         print ('non-matches: %d - matches: %d' %(supp[0], supp[1]))\n",
    "            #         print('')\n",
    "\n",
    "\n",
    "                    ########################\n",
    "\n",
    "                    #Atualizar as estatísticas\n",
    "                    abordagem = 'DS'\n",
    "                    #print 'abordagem é %s' %(abordagem)\n",
    "\n",
    "                    #algUtl = linhaAtual['algoritmosUtilizados'].item()\n",
    "                    iteracao = 2\n",
    "                    inspecoesManuais = linhaAtual['inspecoesManuais'].item()\n",
    "                    da = linhaAtual['da'].item() + (this_tp + this_fp) #Adiciona todos os casos positivos identificados\n",
    "                    dm = linhaAtual['dm'].item()             #na predição\n",
    "                    ndm = linhaAtual['ndm'].item()\n",
    "\n",
    "\n",
    "                    fn = float(linhaAtual['fn'].item() - this_tp ) #Não tenho certeza se é isso\n",
    "                    tp = this_tp + float(linhaAtual['tp'].item()) #+ dm) #Recuperar de toClass\n",
    "#                     print('tp é igual a: %d' %(tp))\n",
    "\n",
    "                    fp = this_fp + float(linhaAtual['fp'].item())\n",
    "                    #tn = tn + float(linhaAtual['tn'].item() - tp) #+ ndm) #Recuperar de toClass\n",
    "                    tn = (2616 * 2294) -(tp+fp+fn)\n",
    "\n",
    "\n",
    "\n",
    "                    precision = tp/(tp+fp)\n",
    "                    recall = tp/(tp+fn)\n",
    "                    fmeasure = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "                    #Adicionando valor à última linha\n",
    "            #         estatisticas.loc[(etapa, permutacao), ['abordagem', 'algoritmosUtilizados', 'iteracao', 'inspecoesManuais',\n",
    "            #            'precision', 'recall', 'f-measure', 'da', 'dm', 'ndm', 'tp',\n",
    "            #            'fp', 'tn', 'fn'] ] = ([abordagem, algUtl, iteracao, inspecoesManuais,\n",
    "            #            precision, recall, fmeasure, da, dm, ndm, tp, fp, tn, fn])\n",
    "                    estatisticas.loc[(algUtl, permutacao, etapa), ['abordagem', 'iteracao', 'inspecoesManuais',\n",
    "                        'precision', 'recall', 'f-measure', 'da', 'dm', 'ndm', 'tp',\n",
    "                        'fp', 'tn', 'fn'] ] = ([abordagem, iteracao, inspecoesManuais,\n",
    "                        precision, recall, fmeasure, da, dm, ndm, tp, fp, tn, fn])\n",
    "\n",
    "                    #'''\n",
    "\n",
    "            # estatisticas = estatisticas.reset_index(level=['etapa', 'permutacao'])\n",
    "            estatisticas = estatisticas.reset_index(level=['algoritmosUtilizados', 'permutacao', 'etapa'])\n",
    "            # estatisticas.head()                   \n",
    "            #                    \n",
    "\n",
    "            estatisticas = estatisticas[['abordagem', 'etapa', 'algoritmosUtilizados', 'permutacao', 'iteracao', 'inspecoesManuais', 'precision', 'recall', 'f-measure', 'da', 'dm', 'ndm', 'tp', 'fp', 'tn', 'fn']]\n",
    "\n",
    "            estatisticas[['algoritmosUtilizados', 'iteracao', 'inspecoesManuais', 'da', 'dm', 'ndm', 'tp', 'fp', 'tn', 'fn']] = \\\n",
    "            estatisticas[['algoritmosUtilizados', 'iteracao', 'inspecoesManuais', 'da', 'dm', 'ndm', 'tp', 'fp', 'tn', 'fn']].astype(int)\n",
    "\n",
    "#             dirEst = \"../../csv/\"\n",
    "            # dirEst = \"c:/Users/Diego/Documents/NetBeansProjects/Master-SKYAM/AS/src/csv/\"\n",
    "            # dirEst = \"../arqResult/csv/\"\n",
    "\n",
    "\n",
    "#             estatisticas.to_csv(dirEstat+'estatisticaDS3-DgArj-'+regiao+'.csv', sep=';', index=False)        \n",
    "            estatisticas.to_csv(dirEstat+'estatisticaDS3-DgArj-'+regiao+'-k'+str(k)+'.csv', sep=';', index=False)        \n",
    "\n",
    "            ####################### \n",
    "            #ATENÇÃO, DIEGO!#\n",
    "            #Adicionar outro classificador aqui\n",
    "            \n",
    "#             print('lista aqui:')\n",
    "#             print(lista)\n",
    "\n",
    "        #Para o 2º classificador\n",
    "        for regiao in regioes:\n",
    "\n",
    "\n",
    "            estatisticas = pd.read_csv(estat, index_col=['algoritmosUtilizados','permutacao','etapa'], sep=';')\n",
    "        #     estatisticas.tail()\n",
    "\n",
    "            arquivos = os.listdir(dirOrig)\n",
    "            # print(arquivos)\n",
    "            #print(len(arquivos))\n",
    "\n",
    "            train = []\n",
    "            test = []\n",
    "\n",
    "            for arq in arquivos:\n",
    "                if arq.startswith(\"train\"):\n",
    "                    train.append(arq)\n",
    "                elif arq.startswith(\"test\"):\n",
    "                    test.append(arq)\n",
    "\n",
    "            # print('tamanho de train: %d' %(len(train)))        \n",
    "\n",
    "            # print('tamanho de test: %d' %(len(test)))\n",
    "\n",
    "            # print (train)    \n",
    "            # print (test)    \n",
    "\n",
    "\n",
    "\n",
    "            ####CLASSIFICAÇÃO COM KNN\n",
    "\n",
    "            for i in lista: \n",
    "\n",
    "                cont = 0\n",
    "\n",
    "                trainAtual = []\n",
    "                testAtual = []\n",
    "\n",
    "                for arq in train:\n",
    "                    if '('+i+')' in arq:\n",
    "        #             if 'train(10)22' in arq:    \n",
    "                        trainAtual.append(arq)\n",
    "                for arq in test:\n",
    "                    if '('+i+')' in arq:\n",
    "        #             if 'test(10)22' in arq:\n",
    "                        testAtual.append(arq)\n",
    "\n",
    "            #     print('lista desordenada')\n",
    "            #     print(trainAtual)\n",
    "\n",
    "            #     print('tamanho antes: %d' %(len(trainAtual)))\n",
    "\n",
    "                #Ordenando a lista\n",
    "                trainAtual = sorted(trainAtual)\n",
    "                testAtual = sorted(testAtual)\n",
    "\n",
    "            #     for x in trainAtual:\n",
    "            #         print(x)\n",
    "\n",
    "\n",
    "\n",
    "            # #     print('lista ordenada')\n",
    "            # #     print(trainAtual)\n",
    "\n",
    "            #     print('tamanho depois: %d' %(len(trainAtual)))\n",
    "\n",
    "            #     print(type(trainAtual))\n",
    "\n",
    "\n",
    "                print (i)\n",
    "\n",
    "            #     print(trainAtual)\n",
    "            #     print(testAtual)\n",
    "\n",
    "                tam = len(trainAtual) #Mesma coisa para testAtual\n",
    "\n",
    "                for pos in range(tam):\n",
    "            #         print(pos)\n",
    "                    print('')\n",
    "                    print('##########################')\n",
    "                    print('Arquivos atuais: %s e %s' %(trainAtual[pos], testAtual[pos]))\n",
    "                    print('##########################')\n",
    "\n",
    "                    treino = dirOrig+trainAtual[pos]\n",
    "                    teste = dirOrig+testAtual[pos]\n",
    "\n",
    "            #         print('treino: %s' %(treino))\n",
    "\n",
    "            #         tp, fp, tn, fn = 0\n",
    "\n",
    "                    num = trainAtual[pos].replace('train('+i+')','')\n",
    "                    num = num.replace('.csv','')\n",
    "\n",
    "                    cont += 1\n",
    "\n",
    "                    print('Iteração: %d' %(cont))\n",
    "\n",
    "            #         print('num: %s' %(num))\n",
    "\n",
    "                    algUtl = re.sub('train.*\\(', r'', trainAtual[pos]) #Alterar para fazer a substituição de tudo em uma linha só\n",
    "                    algUtl = re.sub('\\).*', r'', algUtl) #Alterar para fazer a substituição de tudo em uma linha só\n",
    "                    algUtl = int(algUtl)\n",
    "\n",
    "\n",
    "                    print('algUtl: %d' %(algUtl))\n",
    "\n",
    "                    permutacao = int(num)\n",
    "            #         linhaAtual = estatisticas.loc[('2 - AA[pet-chr]', permutacao), : ] #Armazena a linha correspondente à permutação\n",
    "            #         linhaAtual = estatisticas.loc[('1 - acm diverg', permutacao), : ] #Armazena a linha correspondente à permutação\n",
    "                    linhaAtual = estatisticas.loc[algUtl, permutacao, abordagemAA, : ] #Armazena a linha correspondente à permutação\n",
    "        #             print (linhaAtual)\n",
    "        #             print(linhaAtual.columns)\n",
    "\n",
    "                #'''\n",
    "            #         all = pd.read_csv(\"train.csv\", index_col=False, sep=';', names=('title', 'artist', 'track01', 'track02', 'track03', 'duplicata'), header = None)\n",
    "            #         toClass = pd.read_csv(\"test.csv\", index_col=False, sep=';', names=('title', 'artist', 'track01', 'track02', 'track03', 'duplicata'), header = None)\n",
    "\n",
    "            #         all = pd.read_csv(treino, index_col=False, sep=';', names=('title', 'artist', 'track01', 'track02', 'track03', 'duplicata'), header = None)\n",
    "            #         toClass = pd.read_csv(teste, index_col=False, sep=';', names=('title', 'artist', 'track01', 'track02', 'track03', 'duplicata'), header = None)\n",
    "\n",
    "                    all = pd.read_csv(treino, index_col=False, sep=';', names=('title', 'authors', 'venue', 'year', 'duplicata'), header = 0) #Era header = None\n",
    "                    toClass = pd.read_csv(teste, index_col=False, sep=';', names=('title', 'authors', 'venue', 'year', 'duplicata'), header = 0) #Era header = None\n",
    "\n",
    "\n",
    "                    # all['artist-title'] = all['artist'] * all['title']\n",
    "                    # cols = list(all.columns.values)\n",
    "                    # cols.pop(cols.index('duplicata'))\n",
    "                    # all = all[cols+['duplicata']]\n",
    "\n",
    "                    #Pesos baseados no Alg17\n",
    "\n",
    "#                     all['artist-title'] = all['artist'] * all['title']\n",
    "                    all['soma-pesos'] = (all['title']*2 + all['authors']*0.5 + all['venue']*0.5 + all['year']*1)/4.0\n",
    "                    cols = list(all.columns.values)\n",
    "                    cols.pop(cols.index('duplicata'))\n",
    "                    all = all[cols+['duplicata']]\n",
    "\n",
    "                    # toClass['artist-title'] = toClass['artist'] * toClass['title']\n",
    "                    # cols = list(toClass.columns.values)\n",
    "                    # cols.pop(cols.index('duplicata'))\n",
    "                    # toClass = toClass[cols+['duplicata']]\n",
    "\n",
    "                    #Pesos baseados no Alg17\n",
    "\n",
    "#                     toClass['artist-title'] = toClass['artist'] * toClass['title']\n",
    "                    toClass['soma-pesos'] = (toClass['title']*2 + toClass['authors']*0.5 + toClass['venue']*0.5 + toClass['year']*1)/4.0\n",
    "                    cols = list(toClass.columns.values)\n",
    "                    cols.pop(cols.index('duplicata'))\n",
    "                    toClass = toClass[cols+['duplicata']]\n",
    "\n",
    "                    #Separação do conjunto X do conjunto y\n",
    "                    # X = all.loc[:,'title':'artist-title']\n",
    "                    X = all.loc[:,'title':'soma-pesos']\n",
    "                    y = all.duplicata\n",
    "\n",
    "                    # XtoClass = toClass.loc[:,'title':'artist-title']\n",
    "                    XtoClass = toClass.loc[:,'title':'soma-pesos']\n",
    "                    ytoClass = toClass.duplicata\n",
    "\n",
    "                    from sklearn import model_selection\n",
    "\n",
    "                    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.20, random_state=2)\n",
    "\n",
    "                    from sklearn.svm import SVC\n",
    "                    from sklearn.model_selection import StratifiedKFold\n",
    "                    from sklearn.model_selection import KFold\n",
    "                    from sklearn import model_selection\n",
    "\n",
    "                    seed = 500\n",
    "\n",
    "                    '''\n",
    "\n",
    "                    from sklearn.linear_model import LogisticRegression\n",
    "                    from sklearn.tree import DecisionTreeClassifier\n",
    "                    from sklearn.neighbors import SVC\n",
    "                    from sklearn.svm import SVC\n",
    "\n",
    "                    modelos = []\n",
    "                    modelos.append(('LR', LogisticRegression()))\n",
    "                    modelos.append(('KNN', SVC()))\n",
    "                    modelos.append(('RFC', DecisionTreeClassifier()))\n",
    "                    modelos.append(('SVM', SVC()))\n",
    "                    modelos.append(('RF', RandomForestClassifier()))\n",
    "\n",
    "\n",
    "\n",
    "                    # Avaliação de cada modelo por vez\n",
    "                    resultados = []\n",
    "                    nomes = []\n",
    "                    for nome, modelo in modelos:\n",
    "                        kfold = StratifiedKFold(n_splits=10, random_state=seed) #Mudar para n-fold\n",
    "                        cv_results = model_selection.cross_val_score(modelo, X_train, y_train, cv=kfold, scoring='f1')\n",
    "                        msg = \"%s: %f (%f)\" % (nome, cv_results.mean(), cv_results.std()) #Iprimir em um arquivo\n",
    "                        print(msg)\n",
    "                    '''\n",
    "\n",
    "                    svm = SVC()\n",
    "\n",
    "                    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "                    param_grid = [{\"C\": [0.001, 0.1, 10, 100, 1000], \"kernel\": ['rbf', 'sigmoid' , 'linear'],\n",
    "                                  \"class_weight\": [\"balanced\",None],\n",
    "                                  \"gamma\": [1e-2, 1e-3, 1e-4, 1e-5]}]\n",
    "\n",
    "                    score = 'f1'\n",
    "\n",
    "                    print(\"# Tunando hiperparâmetros para %s\" % score)\n",
    "                    print()\n",
    "\n",
    "                    svm = SVC()\n",
    "\n",
    "                    try:\n",
    "\n",
    "                        grid = GridSearchCV(svm, param_grid, cv=2, scoring='%s_macro' % score)\n",
    "\n",
    "                        try:\n",
    "\n",
    "                            grid = GridSearchCV(svm, param_grid, cv=5, scoring='%s_macro' % score)\n",
    "                            grid.fit(X_train, y_train)\n",
    "\n",
    "                        except ValueError:\n",
    "\n",
    "                            try:\n",
    "\n",
    "                                grid = GridSearchCV(svm, param_grid, cv=3, scoring='%s_macro' % score)\n",
    "                                grid.fit(X_train, y_train)\n",
    "\n",
    "                            except ValueError:\n",
    "\n",
    "                                grid = GridSearchCV(svm, param_grid, cv=2, scoring='%s_macro' % score)\n",
    "                                grid.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "        #                 print(\"Melhor conjunto de hiperparâmetros encontrado:\")\n",
    "        #                 print()\n",
    "        #                 print(grid.best_params_)\n",
    "                        # print()\n",
    "                        # print(\"Grade de valores encontrados:\")\n",
    "                        # print()\n",
    "        #                 medias = grid.cv_results_['mean_test_score']\n",
    "        #                 desvios = grid.cv_results_['std_test_score']\n",
    "                        # for media, desvio, params in zip(medias, desvios, grid.cv_results_['params']):\n",
    "                        #     print(\"%0.3f (+/-%0.03f) for %r\" % (media, desvio * 2, params))\n",
    "                        #     print()\n",
    "\n",
    "                        svm = SVC(**grid.best_params_)\n",
    "\n",
    "                    except ValueError:\n",
    "\n",
    "                        svm = SVC()\n",
    "\n",
    "            #         rfc = RandomForestClassifier(parameters = grid.best_params_, random_state=12)\n",
    "\n",
    "                    kfold = KFold(n_splits=2, random_state=seed)\n",
    "\n",
    "                    try:\n",
    "\n",
    "                        kfold = StratifiedKFold(n_splits=10, random_state=seed)\n",
    "                        kfoldUtilizado = \"skf-10\"\n",
    "\n",
    "                    except ValueError:\n",
    "\n",
    "                        try:\n",
    "\n",
    "                            print(\"Primeiro erro!\")\n",
    "                            kfold = KFold(n_splits=10, random_state=seed)\n",
    "                            kfoldUtilizado = \"kf-10\"\n",
    "\n",
    "                        except ValueError:\n",
    "\n",
    "                            try:\n",
    "\n",
    "                                print(\"Segundo erro!\")\n",
    "                                kfold = StratifiedKFold(n_splits=5, random_state=seed)\n",
    "                                kfoldUtilizado = \"skf-5\"\n",
    "\n",
    "                            except ValueError:\n",
    "\n",
    "                                try:\n",
    "\n",
    "                                    print(\"Terceiro erro!\")\n",
    "                                    kfold = KFold(n_splits=5, random_state=seed)\n",
    "                                    kfoldUtilizado = \"kf-5\"\n",
    "                                \n",
    "                                except ValueError:\n",
    "\n",
    "                                    try:\n",
    "\n",
    "                                        print(\"Quarto erro!\")\n",
    "                                        kfold = KFold(n_splits=2, random_state=seed)\n",
    "                                        kfoldUtilizado = \"kf-2\"\n",
    "\n",
    "                                    except:\n",
    "                                        print(\"ERRO NA VALIDAÇÃO CRUZADA!\")\n",
    "                                        print(\"kfold utilizado: {0}\".format(kfoldUtilizado))\n",
    "                                        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "                                        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "                                        print(exc_type, fname, exc_tb.tb_lineno)\n",
    "                                        print(sys.exc_info())\n",
    "\n",
    "                    #Isso só serve para validar o uso do conjunto de treino \n",
    "                    #Não influencia no resultado final\n",
    "        #             try:\n",
    "        #                 cv_results2 = model_selection.cross_val_score(rfc, X_train, y_train, cv=kfold, scoring='f1')\n",
    "        #                 msg = \"%s com hiperparâmetros tunados: F1 = %f com desvio padrão = %f\" % ('RFC', cv_results2.mean(), cv_results2.std())\n",
    "        #                 print(msg)\n",
    "\n",
    "        #             except:\n",
    "        #                 print(\"ERRO NA VALIDAÇÃO CRUZADA!\")\n",
    "        #                 exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        #                 fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        #                 print(exc_type, fname, exc_tb.tb_lineno)\n",
    "        #                 print(sys.exc_info())\n",
    "\n",
    "\n",
    "                    try:\n",
    "                        svm.fit(X_train, y_train) #Treinando o modelo\n",
    "                    except ValueError:\n",
    "                        estatisticas.loc[(algUtl, permutacao, etapa), ['abordagem', 'iteracao', 'inspecoesManuais',\n",
    "                        'precision', 'recall', 'f-measure', 'da', 'dm', 'ndm', 'tp',\n",
    "                        'fp', 'tn', 'fn'] ] = (['ERRO-SVM', 2, inspecoesManuais,\n",
    "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "                        continue\n",
    "                    predicoes = svm.predict(X_test) #Realizando a predição\n",
    "\n",
    "            #         from sklearn.metrics import confusion_matrix\n",
    "            #         matriz = confusion_matrix(y_test, predicoes)\n",
    "\n",
    "            #         print(matriz)\n",
    "\n",
    "                    from sklearn.metrics import classification_report\n",
    "                    from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "                    # print(classification_report(y_test, predicoes))\n",
    "            #         prec, rec, fbeta, supp = precision_recall_fscore_support(y_test, predicoes, average=None)\n",
    "            #         print ('precision: %.5f' %(prec[1]))\n",
    "            #         print ('recall: %.5f' %(rec[1]))\n",
    "            #         print ('f1: %.5f' %(fbeta[1]))\n",
    "            #         print ('non-matches: %d - matches: %d' %(supp[0], supp[1]))\n",
    "            #         print('')\n",
    "\n",
    "                    # from sklearn.externals.six import StringIO  \n",
    "                    # from IPython.display import Image  \n",
    "                    # from sklearn.tree import export_graphviz\n",
    "                    # import pydotplus\n",
    "                    # dot_data = StringIO()\n",
    "                    # export_graphviz(rfc, out_file=dot_data,  \n",
    "                    #                 filled=True, rounded=True,\n",
    "                    #                 special_characters=True, feature_names=X.columns)\n",
    "                    # graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "                    # Image(graph.create_png())\n",
    "\n",
    "                    predicoes = svm.predict(XtoClass)\n",
    "\n",
    "                    from sklearn.metrics import confusion_matrix\n",
    "                    matriz = confusion_matrix(ytoClass, predicoes)\n",
    "                    print(matriz)\n",
    "\n",
    "                    this_tn, this_fp, this_fn, this_tp = 0, 0, 0, 0\n",
    "\n",
    "                    this_tn, this_fp, this_fn, this_tp = confusion_matrix(ytoClass, predicoes, labels=[0,1]).ravel()\n",
    "                    print('tn, fp, fn, tp')\n",
    "                    print(this_tn, this_fp, this_fn, this_tp)\n",
    "\n",
    "\n",
    "\n",
    "                    from sklearn.metrics import classification_report\n",
    "                    from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "                    # print(classification_report(y_test, predicoes))\n",
    "            #         prec, rec, fbeta, supp = precision_recall_fscore_support(ytoClass, predicoes, average=None)\n",
    "            #         print ('precision: %.5f' %(prec[1]))\n",
    "            #         print ('recall: %.5f' %(rec[1]))\n",
    "            #         print ('f1: %.5f' %(fbeta[1]))\n",
    "            #         print ('non-matches: %d - matches: %d' %(supp[0], supp[1]))\n",
    "            #         print('')\n",
    "\n",
    "\n",
    "                    ########################\n",
    "\n",
    "                    #Atualizar as estatísticas\n",
    "                    abordagem = 'DS'\n",
    "                    #print 'abordagem é %s' %(abordagem)\n",
    "\n",
    "                    #algUtl = linhaAtual['algoritmosUtilizados'].item()\n",
    "                    iteracao = 2\n",
    "                    inspecoesManuais = linhaAtual['inspecoesManuais'].item()\n",
    "                    da = linhaAtual['da'].item() + (this_tp + this_fp) #Adiciona todos os casos positivos identificados\n",
    "                    dm = linhaAtual['dm'].item()             #na predição\n",
    "                    ndm = linhaAtual['ndm'].item()\n",
    "\n",
    "\n",
    "                    fn = float(linhaAtual['fn'].item() - this_tp ) #Não tenho certeza se é isso\n",
    "                    tp = this_tp + float(linhaAtual['tp'].item()) #+ dm) #Recuperar de toClass\n",
    "                    print('tp é igual a: %d' %(tp))\n",
    "\n",
    "                    fp = this_fp + float(linhaAtual['fp'].item())\n",
    "                    #tn = tn + float(linhaAtual['tn'].item() - tp) #+ ndm) #Recuperar de toClass\n",
    "                    tn = (2616 * 2294) -(tp+fp+fn)\n",
    "\n",
    "\n",
    "\n",
    "                    precision = tp/(tp+fp)\n",
    "                    recall = tp/(tp+fn)\n",
    "                    fmeasure = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "                    #Adicionando valor à última linha\n",
    "            #         estatisticas.loc[(etapa, permutacao), ['abordagem', 'algoritmosUtilizados', 'iteracao', 'inspecoesManuais',\n",
    "            #            'precision', 'recall', 'f-measure', 'da', 'dm', 'ndm', 'tp',\n",
    "            #            'fp', 'tn', 'fn'] ] = ([abordagem, algUtl, iteracao, inspecoesManuais,\n",
    "            #            precision, recall, fmeasure, da, dm, ndm, tp, fp, tn, fn])\n",
    "                    estatisticas.loc[(algUtl, permutacao, etapa), ['abordagem', 'iteracao', 'inspecoesManuais',\n",
    "                        'precision', 'recall', 'f-measure', 'da', 'dm', 'ndm', 'tp',\n",
    "                        'fp', 'tn', 'fn'] ] = ([abordagem, iteracao, inspecoesManuais,\n",
    "                        precision, recall, fmeasure, da, dm, ndm, tp, fp, tn, fn])\n",
    "\n",
    "                    #'''\n",
    "\n",
    "            # estatisticas = estatisticas.reset_index(level=['etapa', 'permutacao'])\n",
    "            estatisticas = estatisticas.reset_index(level=['algoritmosUtilizados', 'permutacao', 'etapa'])\n",
    "            # estatisticas.head()                   \n",
    "            #                    \n",
    "\n",
    "            estatisticas = estatisticas[['abordagem', 'etapa', 'algoritmosUtilizados', 'permutacao', 'iteracao', 'inspecoesManuais', 'precision', 'recall', 'f-measure', 'da', 'dm', 'ndm', 'tp', 'fp', 'tn', 'fn']]\n",
    "\n",
    "            estatisticas[['algoritmosUtilizados', 'iteracao', 'inspecoesManuais', 'da', 'dm', 'ndm', 'tp', 'fp', 'tn', 'fn']] = \\\n",
    "            estatisticas[['algoritmosUtilizados', 'iteracao', 'inspecoesManuais', 'da', 'dm', 'ndm', 'tp', 'fp', 'tn', 'fn']].astype(int)\n",
    "\n",
    "        #     dirEst = \"../../csv/\"\n",
    "            # dirEst = \"c:/Users/Diego/Documents/NetBeansProjects/Master-SKYAM/AS/src/csv/\"\n",
    "            # dirEst = \"../arqResult/csv/\"\n",
    "            \n",
    "#             estatisticas.to_csv(dirEstat+'estatisticaDS3-DgArj-svm-'+regiao+'.csv', sep=';', index=False)        \n",
    "            estatisticas.to_csv(dirEstat+'estatisticaDS3-DgArj-svm-'+regiao+'-k'+str(k)+'.csv', sep=';', index=False)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
