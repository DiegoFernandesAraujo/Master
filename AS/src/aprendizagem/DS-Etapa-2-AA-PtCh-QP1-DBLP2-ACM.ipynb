{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base atual: DBLP2-ACM\n",
      "QP atual: qp1\n",
      "Index([u'abordagem', u'iteracao', u'inspecoesManuais', u'precision', u'recall',\n",
      "       u'f-measure', u'da', u'dm', u'ndm', u'tp', u'fp', u'tn', u'fn'],\n",
      "      dtype='object')\n",
      "estatisticas.shape\n",
      "(3, 13)\n",
      "Analisando o arquivo: diverg(10)1_NEW.csv\n",
      "\n",
      "Load weight vector file: ../csv/conjuntosDS/conjuntosDivergAA/DBLP2-ACM/qp1/diverg(10)1_NEW.csv\n",
      "  Weights to use: ['title', 'authors', 'venue', 'year']\n",
      "  Number of weight vectors: 4309\n",
      "    Number of entity ID pairs that occurred more than once: 0\n",
      "\n",
      "Analyse set of 4309 weight vectors\n",
      "  Containing 1878 true matches and 2431 true non-matches\n",
      "    (43.58% true matches)\n",
      "  Identified 3499 unique weight vectors\n",
      "  Frequency distribution of occurences of weight vectors:\n",
      "    Occurence : Number of weight vectors that occur that often\n",
      "          1 :  3202  (91.51%)\n",
      "          2 :   201  (5.74%)\n",
      "          3 :    42  (1.20%)\n",
      "          4 :    21  (0.60%)\n",
      "          5 :     7  (0.20%)\n",
      "          6 :     5  (0.14%)\n",
      "          7 :     3  (0.09%)\n",
      "          8 :     4  (0.11%)\n",
      "          9 :     1  (0.03%)\n",
      "         10 :     4  (0.11%)\n",
      "         13 :     1  (0.03%)\n",
      "         14 :     1  (0.03%)\n",
      "         15 :     1  (0.03%)\n",
      "         16 :     1  (0.03%)\n",
      "         17 :     1  (0.03%)\n",
      "         18 :     2  (0.06%)\n",
      "         23 :     1  (0.03%)\n",
      "        194 :     1  (0.03%)\n",
      "\n",
      "Identified 7 non-pure unique weight vectors (from 3499 unique weight vectors)\n",
      "Pureness (as percentage of matches) for a certain unique weight vector:\n",
      "  Pureness : Count\n",
      "     1.000 : 1525\n",
      "     0.500 :  5   (all weight vectors with this pureness to be removed)\n",
      "     0.333 :  1   (all weight vectors with this pureness to be removed)\n",
      "     0.100 :  1   (all weight vectors with this pureness to be removed)\n",
      "     0.000 : 1967\n",
      "\n",
      "Removed 31 non-pure weight vectors\n",
      "\n",
      "Final number of weight vectors to use: 4278\n",
      "  Number of unique weight vectors: 3493\n",
      "\n",
      "Time to load and analyse the weight vector file: 0.34 sec\n",
      "\n",
      "Initial estimated match proportion: 0.500\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Loop 1: Queue length: 1\n",
      "  Number of manual oracle classifications performed: 0\n",
      "  Size, purity, entropy, and estimated match proportion of clusters in queue:\n",
      "    (3493, 0.5, 1.0, 0.5)\n",
      "\n",
      "Current size of match and non-match training data sets: 0 / 0\n",
      "\n",
      "  Balanced cluster selection, select cluster closest to [1,..,1]\n",
      "\n",
      "Selected cluster with (queue ordering: balance):\n",
      "- Purity 0.50 and entropy 1.00\n",
      "- Size 3493 weight vectors\n",
      "- Estimated match proportion 0.500\n",
      "\n",
      "Sample size for this cluster: 93\n",
      "\n",
      "Perform initial selection using \"far\" method\n",
      "\n",
      "Farthest first selection of 93 weight vectors from 3493 vectors\n",
      "  The selected farthest weight vectors are:\n",
      "    [0.708, 0.933, 0.613, 1.000] (True)\n",
      "    [0.500, 0.502, 0.553, 0.875] (False)\n",
      "    [0.787, 0.617, 0.452, 1.000] (False)\n",
      "    [0.577, 0.716, 0.613, 0.875] (False)\n",
      "    [0.957, 0.850, 0.723, 0.875] (False)\n",
      "    [0.802, 0.742, 0.458, 0.875] (False)\n",
      "    [1.000, 0.503, 0.723, 0.500] (False)\n",
      "    [0.524, 0.500, 0.368, 1.000] (False)\n",
      "    [0.500, 0.503, 0.723, 0.875] (False)\n",
      "    [0.500, 1.000, 0.651, 0.875] (False)\n",
      "    [0.512, 0.689, 0.565, 1.000] (False)\n",
      "    [0.558, 0.868, 0.452, 1.000] (False)\n",
      "    [0.500, 1.000, 0.520, 0.875] (False)\n",
      "    [0.994, 0.500, 0.458, 1.000] (False)\n",
      "    [1.000, 0.828, 0.723, 1.000] (True)\n",
      "    [0.500, 1.000, 0.553, 1.000] (False)\n",
      "    [0.951, 0.760, 0.553, 0.875] (False)\n",
      "    [0.556, 1.000, 0.368, 1.000] (False)\n",
      "    [0.514, 0.786, 0.723, 1.000] (True)\n",
      "    [0.700, 0.794, 0.553, 0.875] (False)\n",
      "    [0.561, 0.552, 0.565, 1.000] (False)\n",
      "    [0.779, 1.000, 0.520, 1.000] (False)\n",
      "    [0.563, 0.513, 0.368, 0.875] (False)\n",
      "    [1.000, 0.769, 0.458, 1.000] (True)\n",
      "    [0.662, 0.564, 0.613, 1.000] (True)\n",
      "    [1.000, 0.532, 0.723, 0.875] (False)\n",
      "    [1.000, 0.633, 0.723, 1.000] (True)\n",
      "    [0.790, 0.736, 0.452, 1.000] (False)\n",
      "    [0.661, 0.757, 0.458, 0.875] (False)\n",
      "    [0.632, 0.559, 0.723, 0.875] (False)\n",
      "    [0.758, 1.000, 0.678, 0.875] (False)\n",
      "    [0.690, 0.907, 0.452, 0.875] (False)\n",
      "    [0.578, 0.915, 0.613, 1.000] (False)\n",
      "    [0.560, 0.645, 0.678, 1.000] (False)\n",
      "    [0.971, 1.000, 0.565, 0.875] (False)\n",
      "    [1.000, 0.900, 0.613, 1.000] (True)\n",
      "    [1.000, 0.577, 0.452, 0.875] (False)\n",
      "    [0.575, 0.612, 0.452, 1.000] (False)\n",
      "    [0.952, 1.000, 0.723, 1.000] (True)\n",
      "    [0.881, 0.917, 0.678, 0.875] (False)\n",
      "    [0.500, 1.000, 0.380, 0.875] (False)\n",
      "    [0.515, 0.938, 0.723, 1.000] (True)\n",
      "    [0.805, 1.000, 0.452, 0.875] (False)\n",
      "    [0.950, 0.683, 0.678, 0.875] (False)\n",
      "    [0.767, 0.511, 0.368, 1.000] (False)\n",
      "    [0.905, 0.756, 0.723, 1.000] (True)\n",
      "    [0.585, 1.000, 0.723, 0.875] (False)\n",
      "    [0.573, 0.885, 0.500, 0.875] (False)\n",
      "    [1.000, 0.723, 0.613, 1.000] (True)\n",
      "    [0.500, 0.505, 0.651, 1.000] (False)\n",
      "    [0.545, 0.621, 0.452, 0.875] (False)\n",
      "    [0.858, 0.533, 0.565, 1.000] (False)\n",
      "    [0.765, 1.000, 0.723, 1.000] (True)\n",
      "    [0.649, 1.000, 0.520, 1.000] (False)\n",
      "    [0.881, 0.922, 0.452, 0.875] (False)\n",
      "    [0.955, 0.619, 0.613, 1.000] (True)\n",
      "    [0.604, 0.503, 0.723, 1.000] (False)\n",
      "    [0.750, 0.744, 0.651, 0.875] (False)\n",
      "    [0.705, 0.514, 0.520, 1.000] (False)\n",
      "    [1.000, 0.717, 0.452, 0.875] (False)\n",
      "    [0.714, 0.586, 0.368, 0.875] (False)\n",
      "    [0.633, 0.764, 0.613, 1.000] (False)\n",
      "    [0.557, 0.750, 0.452, 1.000] (False)\n",
      "    [0.607, 1.000, 0.565, 0.875] (False)\n",
      "    [1.000, 0.632, 0.553, 0.875] (False)\n",
      "    [0.857, 0.545, 0.723, 1.000] (False)\n",
      "    [0.681, 0.681, 0.452, 1.000] (False)\n",
      "    [0.713, 0.640, 0.565, 0.875] (False)\n",
      "    [0.701, 0.500, 0.452, 0.875] (False)\n",
      "    [0.611, 1.000, 0.723, 1.000] (False)\n",
      "    [0.523, 0.783, 0.458, 0.875] (False)\n",
      "    [0.755, 0.657, 0.553, 1.000] (False)\n",
      "    [0.863, 0.905, 0.723, 1.000] (True)\n",
      "    [1.000, 0.887, 0.458, 1.000] (True)\n",
      "    [0.863, 0.650, 0.671, 1.000] (True)\n",
      "    [0.529, 0.668, 0.723, 0.875] (False)\n",
      "    [0.703, 0.647, 0.723, 1.000] (True)\n",
      "    [0.642, 0.905, 0.723, 0.875] (False)\n",
      "    [0.799, 0.833, 0.613, 1.000] (True)\n",
      "    [1.000, 1.000, 0.458, 1.000] (True)\n",
      "    [0.811, 0.543, 0.723, 0.875] (False)\n",
      "    [0.766, 0.903, 0.565, 0.875] (False)\n",
      "    [0.887, 1.000, 0.613, 1.000] (True)\n",
      "    [0.616, 1.000, 0.452, 0.875] (False)\n",
      "    [0.727, 0.500, 0.565, 0.875] (False)\n",
      "    [0.974, 1.000, 0.723, 0.875] (False)\n",
      "    [0.818, 0.514, 0.458, 0.875] (False)\n",
      "    [0.767, 0.518, 0.646, 1.000] (False)\n",
      "    [0.908, 0.570, 0.565, 0.875] (False)\n",
      "    [1.000, 0.634, 0.458, 1.000] (True)\n",
      "    [0.797, 0.622, 0.452, 0.875] (False)\n",
      "    [0.826, 0.693, 0.565, 0.875] (False)\n",
      "    [0.915, 1.000, 0.368, 1.000] (False)\n",
      "\n",
      "Perform oracle with 100.00 accuracy on 93 weight vectors\n",
      "  The oracle will correctly classify 93 weight vectors and wrongly classify 0\n",
      "  Classified 21 matches and 72 non-matches\n",
      "    Purity of oracle classification:  0.774\n",
      "    Entropy of oracle classification: 0.771\n",
      "    Number of true matches:      21\n",
      "    Number of false matches:     0\n",
      "    Number of true non-matches:  72\n",
      "    Number of false non-matches: 0\n",
      "\n",
      "Deleted 93 weight vectors (classified by oracle) from cluster\n",
      "\n",
      "Cluster not pure enough or too large, and can be split further\n",
      "\n",
      "SVM classification of 3400 weight vectors\n",
      "  Based on 21 matches and 72 non-matches\n",
      "  Classified 1341 matches and 2059 non-matches\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Loop 2: Queue length: 2\n",
      "  Number of manual oracle classifications performed: 93\n",
      "  Size, purity, entropy, and estimated match proportion of clusters in queue:\n",
      "    (1341, 0.7741935483870968, 0.7706290693639405, 0.22580645161290322)\n",
      "    (2059, 0.7741935483870968, 0.7706290693639405, 0.22580645161290322)\n",
      "\n",
      "Current size of match and non-match training data sets: 21 / 72\n",
      "\n",
      "  Balanced cluster selection, select cluster closest to [1,..,1]\n",
      "\n",
      "Selected cluster with (queue ordering: balance):\n",
      "- Purity 0.77 and entropy 0.77\n",
      "- Size 1341 weight vectors\n",
      "- Estimated match proportion 0.226\n",
      "\n",
      "Sample size for this cluster: 64\n",
      "\n",
      "Farthest first selection of 64 weight vectors from 1341 vectors\n",
      "  The selected farthest weight vectors are:\n",
      "    [0.965, 0.955, 0.613, 1.000] (True)\n",
      "    [0.671, 1.000, 0.553, 1.000] (False)\n",
      "    [0.800, 0.769, 0.613, 1.000] (True)\n",
      "    [0.856, 0.743, 0.671, 1.000] (False)\n",
      "    [0.651, 1.000, 0.500, 0.875] (False)\n",
      "    [0.877, 1.000, 0.565, 1.000] (False)\n",
      "    [0.950, 0.514, 0.452, 0.875] (False)\n",
      "    [0.946, 0.581, 0.452, 0.875] (False)\n",
      "    [1.000, 0.607, 0.723, 0.875] (False)\n",
      "    [1.000, 0.508, 0.723, 1.000] (True)\n",
      "    [0.957, 0.906, 0.723, 1.000] (True)\n",
      "    [0.956, 0.609, 0.553, 0.875] (False)\n",
      "    [1.000, 0.790, 0.458, 1.000] (True)\n",
      "    [1.000, 0.541, 0.671, 0.875] (False)\n",
      "    [0.818, 0.708, 0.458, 0.875] (False)\n",
      "    [0.932, 0.958, 0.671, 1.000] (True)\n",
      "    [1.000, 0.500, 0.458, 1.000] (True)\n",
      "    [1.000, 0.917, 0.458, 1.000] (True)\n",
      "    [0.730, 1.000, 0.452, 0.875] (False)\n",
      "    [0.974, 1.000, 0.723, 1.000] (True)\n",
      "    [1.000, 0.625, 0.646, 0.875] (False)\n",
      "    [0.897, 0.676, 0.723, 1.000] (True)\n",
      "    [0.995, 0.625, 0.452, 0.875] (False)\n",
      "    [1.000, 0.584, 0.458, 1.000] (True)\n",
      "    [0.875, 0.842, 0.613, 1.000] (True)\n",
      "    [0.905, 0.919, 0.613, 1.000] (True)\n",
      "    [0.971, 0.850, 0.723, 0.875] (False)\n",
      "    [0.861, 0.668, 0.613, 1.000] (True)\n",
      "    [0.945, 0.624, 0.613, 1.000] (True)\n",
      "    [0.946, 0.761, 0.678, 0.875] (False)\n",
      "    [0.786, 1.000, 0.660, 0.875] (False)\n",
      "    [0.946, 0.664, 0.678, 0.875] (False)\n",
      "    [0.778, 0.917, 0.613, 1.000] (True)\n",
      "    [1.000, 0.712, 0.561, 0.875] (False)\n",
      "    [0.956, 0.534, 0.520, 0.875] (False)\n",
      "    [0.950, 0.700, 0.452, 0.875] (False)\n",
      "    [0.921, 0.690, 0.613, 1.000] (True)\n",
      "    [0.951, 0.529, 0.613, 1.000] (True)\n",
      "    [0.649, 1.000, 0.452, 1.000] (False)\n",
      "    [0.962, 0.681, 0.671, 1.000] (True)\n",
      "    [0.914, 0.833, 0.723, 1.000] (True)\n",
      "    [0.940, 1.000, 0.553, 0.875] (False)\n",
      "    [1.000, 0.933, 0.671, 1.000] (True)\n",
      "    [0.710, 1.000, 0.678, 0.875] (False)\n",
      "    [1.000, 0.869, 0.671, 1.000] (True)\n",
      "    [0.940, 1.000, 0.452, 1.000] (False)\n",
      "    [0.843, 0.881, 0.723, 1.000] (True)\n",
      "    [0.762, 1.000, 0.723, 0.875] (False)\n",
      "    [0.788, 1.000, 0.613, 1.000] (True)\n",
      "    [1.000, 0.580, 0.660, 1.000] (True)\n",
      "    [0.949, 0.786, 0.613, 1.000] (True)\n",
      "    [1.000, 0.800, 0.723, 1.000] (True)\n",
      "    [1.000, 0.716, 0.723, 1.000] (True)\n",
      "    [0.922, 0.743, 0.671, 1.000] (True)\n",
      "    [1.000, 1.000, 0.723, 0.875] (False)\n",
      "    [1.000, 0.669, 0.458, 1.000] (True)\n",
      "    [1.000, 0.704, 0.613, 1.000] (True)\n",
      "    [0.707, 0.991, 0.565, 0.875] (False)\n",
      "    [0.700, 1.000, 0.723, 1.000] (True)\n",
      "    [0.700, 1.000, 0.613, 1.000] (True)\n",
      "    [0.952, 0.914, 0.651, 0.875] (False)\n",
      "    [0.808, 1.000, 0.723, 1.000] (True)\n",
      "    [1.000, 0.844, 0.613, 1.000] (True)\n",
      "    [0.881, 1.000, 0.723, 1.000] (True)\n",
      "\n",
      "Perform oracle with 100.00 accuracy on 64 weight vectors\n",
      "  The oracle will correctly classify 64 weight vectors and wrongly classify 0\n",
      "  Classified 36 matches and 28 non-matches\n",
      "    Purity of oracle classification:  0.562\n",
      "    Entropy of oracle classification: 0.989\n",
      "    Number of true matches:      36\n",
      "    Number of false matches:     0\n",
      "    Number of true non-matches:  28\n",
      "    Number of false non-matches: 0\n",
      "\n",
      "Deleted 64 weight vectors (classified by oracle) from cluster\n",
      "\n",
      "Cluster not pure enough or too large, and can be split further\n",
      "\n",
      "SVM classification of 1277 weight vectors\n",
      "  Based on 36 matches and 28 non-matches\n",
      "  Classified 1277 matches and 0 non-matches\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Loop 3: Queue length: 1\n",
      "  Number of manual oracle classifications performed: 157\n",
      "  Size, purity, entropy, and estimated match proportion of clusters in queue:\n",
      "    (2059, 0.7741935483870968, 0.7706290693639405, 0.22580645161290322)\n",
      "\n",
      "Current size of match and non-match training data sets: 57 / 100\n",
      "\n",
      "  Balanced cluster selection, select cluster closest to [1,..,1]\n",
      "\n",
      "Selected cluster with (queue ordering: balance):\n",
      "- Purity 0.77 and entropy 0.77\n",
      "- Size 2059 weight vectors\n",
      "- Estimated match proportion 0.226\n",
      "\n",
      "Sample size for this cluster: 65\n",
      "\n",
      "Farthest first selection of 65 weight vectors from 2059 vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The selected farthest weight vectors are:\n",
      "    [0.750, 0.500, 0.368, 1.000] (False)\n",
      "    [0.787, 0.702, 0.613, 0.875] (False)\n",
      "    [0.523, 0.601, 0.452, 1.000] (False)\n",
      "    [0.702, 0.534, 0.458, 0.875] (False)\n",
      "    [0.672, 0.711, 0.723, 1.000] (True)\n",
      "    [0.519, 0.667, 0.452, 0.875] (False)\n",
      "    [0.518, 0.563, 0.723, 1.000] (False)\n",
      "    [0.716, 0.544, 0.458, 1.000] (False)\n",
      "    [0.500, 1.000, 0.452, 0.875] (False)\n",
      "    [0.817, 0.500, 0.458, 1.000] (False)\n",
      "    [0.613, 0.607, 0.613, 1.000] (False)\n",
      "    [0.643, 0.900, 0.458, 0.875] (False)\n",
      "    [0.661, 0.684, 0.613, 0.875] (False)\n",
      "    [0.680, 0.697, 0.553, 1.000] (False)\n",
      "    [0.746, 0.719, 0.380, 0.875] (False)\n",
      "    [0.747, 0.562, 0.723, 0.875] (False)\n",
      "    [0.684, 1.000, 0.723, 1.000] (True)\n",
      "    [0.963, 0.523, 0.553, 0.875] (False)\n",
      "    [0.629, 0.510, 0.723, 1.000] (False)\n",
      "    [0.789, 0.625, 0.723, 1.000] (True)\n",
      "    [0.575, 0.635, 0.553, 0.875] (False)\n",
      "    [0.518, 0.500, 0.553, 0.875] (False)\n",
      "    [0.667, 0.657, 0.452, 0.875] (False)\n",
      "    [0.742, 0.841, 0.613, 0.875] (False)\n",
      "    [0.839, 0.738, 0.723, 1.000] (True)\n",
      "    [0.962, 0.500, 0.613, 1.000] (True)\n",
      "    [0.963, 0.500, 0.723, 0.875] (False)\n",
      "    [0.526, 1.000, 0.651, 0.875] (False)\n",
      "    [0.808, 0.658, 0.458, 0.875] (False)\n",
      "    [0.523, 0.705, 0.458, 1.000] (False)\n",
      "    [0.796, 0.702, 0.613, 1.000] (True)\n",
      "    [0.514, 0.803, 0.553, 0.875] (False)\n",
      "    [0.891, 0.562, 0.660, 0.875] (False)\n",
      "    [1.000, 0.505, 0.723, 0.500] (False)\n",
      "    [0.684, 0.773, 0.452, 0.875] (False)\n",
      "    [0.875, 0.561, 0.613, 1.000] (True)\n",
      "    [0.523, 0.682, 0.678, 1.000] (False)\n",
      "    [0.813, 0.514, 0.565, 0.875] (False)\n",
      "    [0.646, 1.000, 0.565, 0.875] (False)\n",
      "    [0.500, 0.513, 0.452, 0.875] (False)\n",
      "    [0.598, 1.000, 0.368, 0.875] (False)\n",
      "    [0.750, 0.613, 0.565, 1.000] (False)\n",
      "    [0.520, 0.852, 0.452, 0.875] (False)\n",
      "    [0.907, 0.560, 0.458, 0.875] (False)\n",
      "    [0.527, 0.659, 0.565, 1.000] (False)\n",
      "    [0.696, 1.000, 0.723, 0.875] (False)\n",
      "    [0.560, 0.925, 0.565, 0.875] (False)\n",
      "    [0.542, 0.535, 0.723, 0.875] (False)\n",
      "    [0.559, 0.900, 0.723, 1.000] (False)\n",
      "    [0.500, 1.000, 0.723, 1.000] (False)\n",
      "    [0.920, 0.607, 0.723, 1.000] (True)\n",
      "    [0.500, 0.543, 0.368, 1.000] (False)\n",
      "    [0.773, 0.633, 0.452, 1.000] (False)\n",
      "    [0.561, 0.502, 0.452, 1.000] (False)\n",
      "    [0.629, 1.000, 0.613, 1.000] (True)\n",
      "    [1.000, 0.504, 0.723, 1.000] (False)\n",
      "    [0.722, 0.615, 0.553, 0.875] (False)\n",
      "    [0.780, 0.538, 0.380, 0.875] (False)\n",
      "    [0.534, 1.000, 0.380, 1.000] (False)\n",
      "    [0.957, 0.629, 0.723, 0.875] (False)\n",
      "    [0.796, 0.500, 0.723, 1.000] (True)\n",
      "    [0.521, 1.000, 0.520, 1.000] (False)\n",
      "    [0.701, 0.500, 0.613, 0.875] (False)\n",
      "    [0.711, 0.500, 0.565, 1.000] (False)\n",
      "    [0.548, 0.580, 0.368, 0.875] (False)\n",
      "\n",
      "Perform oracle with 100.00 accuracy on 65 weight vectors\n",
      "  The oracle will correctly classify 65 weight vectors and wrongly classify 0\n",
      "  Classified 10 matches and 55 non-matches\n",
      "    Purity of oracle classification:  0.846\n",
      "    Entropy of oracle classification: 0.619\n",
      "    Number of true matches:      10\n",
      "    Number of false matches:     0\n",
      "    Number of true non-matches:  55\n",
      "    Number of false non-matches: 0\n",
      "\n",
      "Deleted 65 weight vectors (classified by oracle) from cluster\n",
      "\n",
      "Cluster not pure enough or too large, and can be split further\n",
      "\n",
      "Reached end of manual classification budget\n",
      "\n",
      "330\n",
      "Analisando o arquivo: diverg(15)1_NEW.csv\n",
      "\n",
      "Load weight vector file: ../csv/conjuntosDS/conjuntosDivergAA/DBLP2-ACM/qp1/diverg(15)1_NEW.csv\n",
      "  Weights to use: ['title', 'authors', 'venue', 'year']\n",
      "  Number of weight vectors: 9483\n",
      "    Number of entity ID pairs that occurred more than once: 0\n",
      "\n",
      "Analyse set of 9483 weight vectors\n",
      "  Containing 2058 true matches and 7425 true non-matches\n",
      "    (21.70% true matches)\n",
      "  Identified 8273 unique weight vectors\n",
      "  Frequency distribution of occurences of weight vectors:\n",
      "    Occurence : Number of weight vectors that occur that often\n",
      "          1 :  7731  (93.45%)\n",
      "          2 :   355  (4.29%)\n",
      "          3 :    96  (1.16%)\n",
      "          4 :    40  (0.48%)\n",
      "          5 :    15  (0.18%)\n",
      "          6 :    11  (0.13%)\n",
      "          7 :     5  (0.06%)\n",
      "          8 :     6  (0.07%)\n",
      "          9 :     1  (0.01%)\n",
      "         10 :     4  (0.05%)\n",
      "         12 :     2  (0.02%)\n",
      "         13 :     1  (0.01%)\n",
      "         17 :     1  (0.01%)\n",
      "         18 :     3  (0.04%)\n",
      "         19 :     1  (0.01%)\n",
      "        194 :     1  (0.01%)\n",
      "\n",
      "Identified 8 non-pure unique weight vectors (from 8273 unique weight vectors)\n",
      "Pureness (as percentage of matches) for a certain unique weight vector:\n",
      "  Pureness : Count\n",
      "     1.000 : 1644\n",
      "     0.667 :  1   (all weight vectors with this pureness to be removed)\n",
      "     0.500 :  5   (all weight vectors with this pureness to be removed)\n",
      "     0.368 :  1   (all weight vectors with this pureness to be removed)\n",
      "     0.100 :  1   (all weight vectors with this pureness to be removed)\n",
      "     0.000 : 6621\n",
      "\n",
      "Removed 35 non-pure weight vectors\n",
      "\n",
      "Final number of weight vectors to use: 9448\n",
      "  Number of unique weight vectors: 8266\n",
      "\n",
      "Time to load and analyse the weight vector file: 0.80 sec\n",
      "\n",
      "Initial estimated match proportion: 0.500\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Loop 1: Queue length: 1\n",
      "  Number of manual oracle classifications performed: 0\n",
      "  Size, purity, entropy, and estimated match proportion of clusters in queue:\n",
      "    (8266, 0.5, 1.0, 0.5)\n",
      "\n",
      "Current size of match and non-match training data sets: 0 / 0\n",
      "\n",
      "  Balanced cluster selection, select cluster closest to [1,..,1]\n",
      "\n",
      "Selected cluster with (queue ordering: balance):\n",
      "- Purity 0.50 and entropy 1.00\n",
      "- Size 8266 weight vectors\n",
      "- Estimated match proportion 0.500\n",
      "\n",
      "Sample size for this cluster: 94\n",
      "\n",
      "Perform initial selection using \"far\" method\n",
      "\n",
      "Farthest first selection of 94 weight vectors from 8266 vectors\n",
      "  The selected farthest weight vectors are:\n",
      "    [0.516, 0.504, 0.458, 0.875] (False)\n",
      "    [0.629, 0.510, 0.723, 1.000] (False)\n",
      "    [0.986, 0.629, 0.723, 0.875] (False)\n",
      "    [0.506, 0.618, 0.723, 1.000] (False)\n",
      "    [0.658, 0.500, 0.368, 1.000] (False)\n",
      "    [0.755, 0.542, 0.565, 0.875] (False)\n",
      "    [1.000, 0.712, 0.561, 0.875] (False)\n",
      "    [0.662, 0.584, 0.458, 1.000] (False)\n",
      "    [0.528, 0.505, 0.723, 0.500] (False)\n",
      "    [0.500, 1.000, 0.723, 0.875] (False)\n",
      "    [0.558, 0.868, 0.452, 1.000] (False)\n",
      "    [0.623, 0.583, 0.561, 0.875] (False)\n",
      "    [0.980, 0.516, 0.452, 0.500] (False)\n",
      "    [0.773, 0.620, 0.565, 1.000] (False)\n",
      "    [0.766, 1.000, 0.651, 0.500] (False)\n",
      "    [1.000, 0.828, 0.723, 1.000] (True)\n",
      "    [0.654, 0.518, 0.368, 0.875] (False)\n",
      "    [0.649, 1.000, 0.452, 1.000] (False)\n",
      "    [0.514, 0.786, 0.723, 1.000] (True)\n",
      "    [0.613, 0.607, 0.613, 1.000] (False)\n",
      "    [0.727, 1.000, 0.452, 0.875] (False)\n",
      "    [0.629, 0.525, 0.678, 0.875] (False)\n",
      "    [0.790, 0.736, 0.452, 1.000] (False)\n",
      "    [0.553, 0.826, 0.452, 0.500] (False)\n",
      "    [0.512, 0.947, 0.553, 0.875] (False)\n",
      "    [0.578, 0.915, 0.613, 1.000] (False)\n",
      "    [0.946, 0.761, 0.678, 0.875] (False)\n",
      "    [0.523, 0.705, 0.458, 1.000] (False)\n",
      "    [0.708, 0.933, 0.613, 1.000] (True)\n",
      "    [0.968, 0.933, 0.613, 1.000] (True)\n",
      "    [0.521, 1.000, 0.520, 1.000] (False)\n",
      "    [0.881, 0.917, 0.678, 0.875] (False)\n",
      "    [0.603, 1.000, 0.565, 0.500] (False)\n",
      "    [0.793, 0.731, 0.678, 0.875] (False)\n",
      "    [1.000, 0.975, 0.458, 1.000] (True)\n",
      "    [0.515, 0.938, 0.723, 1.000] (True)\n",
      "    [0.940, 1.000, 0.553, 0.875] (False)\n",
      "    [0.941, 1.000, 0.678, 0.500] (False)\n",
      "    [0.559, 0.659, 0.452, 0.500] (False)\n",
      "    [1.000, 0.500, 0.458, 1.000] (True)\n",
      "    [0.967, 0.500, 0.671, 0.875] (False)\n",
      "    [0.963, 0.696, 0.565, 0.500] (False)\n",
      "    [0.881, 0.922, 0.452, 0.875] (False)\n",
      "    [0.878, 0.500, 0.613, 1.000] (True)\n",
      "    [0.684, 1.000, 0.723, 1.000] (True)\n",
      "    [0.929, 0.737, 0.723, 0.500] (False)\n",
      "    [0.523, 0.708, 0.565, 0.500] (False)\n",
      "    [0.790, 0.599, 0.458, 0.875] (False)\n",
      "    [0.704, 0.658, 0.723, 1.000] (True)\n",
      "    [0.745, 0.578, 0.723, 0.875] (False)\n",
      "    [0.506, 0.500, 0.368, 0.500] (False)\n",
      "    [0.610, 0.750, 0.565, 1.000] (False)\n",
      "    [0.708, 0.648, 0.678, 0.500] (False)\n",
      "    [0.770, 0.544, 0.678, 1.000] (False)\n",
      "    [0.799, 0.833, 0.613, 1.000] (True)\n",
      "    [0.547, 0.643, 0.458, 0.875] (False)\n",
      "    [0.548, 1.000, 0.368, 0.500] (False)\n",
      "    [0.615, 1.000, 0.613, 0.875] (False)\n",
      "    [0.963, 0.500, 0.723, 0.500] (False)\n",
      "    [0.694, 0.689, 0.565, 0.875] (False)\n",
      "    [1.000, 1.000, 0.452, 0.500] (False)\n",
      "    [1.000, 0.611, 0.613, 1.000] (True)\n",
      "    [0.583, 0.750, 0.678, 0.875] (False)\n",
      "    [0.779, 1.000, 0.520, 1.000] (False)\n",
      "    [0.523, 0.783, 0.458, 0.875] (False)\n",
      "    [0.993, 0.688, 0.723, 1.000] (True)\n",
      "    [1.000, 0.843, 0.458, 1.000] (True)\n",
      "    [0.534, 1.000, 0.380, 1.000] (False)\n",
      "    [0.598, 1.000, 0.368, 0.875] (False)\n",
      "    [0.514, 0.503, 0.553, 0.500] (False)\n",
      "    [0.863, 0.650, 0.671, 1.000] (True)\n",
      "    [0.843, 0.881, 0.723, 1.000] (True)\n",
      "    [0.627, 0.743, 0.368, 0.875] (False)\n",
      "    [0.720, 0.509, 0.565, 1.000] (False)\n",
      "    [0.642, 0.905, 0.723, 0.875] (False)\n",
      "    [0.525, 0.500, 0.368, 1.000] (False)\n",
      "    [0.934, 0.762, 0.613, 1.000] (True)\n",
      "    [0.500, 0.500, 0.723, 0.875] (False)\n",
      "    [0.766, 0.903, 0.565, 0.875] (False)\n",
      "    [0.512, 0.500, 0.651, 1.000] (False)\n",
      "    [0.881, 1.000, 0.723, 1.000] (True)\n",
      "    [0.789, 1.000, 0.452, 0.500] (False)\n",
      "    [1.000, 0.504, 0.723, 1.000] (False)\n",
      "    [0.786, 1.000, 0.660, 0.875] (False)\n",
      "    [0.500, 1.000, 0.723, 0.500] (False)\n",
      "    [0.713, 0.814, 0.452, 0.875] (False)\n",
      "    [1.000, 1.000, 0.723, 0.875] (False)\n",
      "    [0.908, 0.570, 0.565, 0.875] (False)\n",
      "    [1.000, 0.634, 0.458, 1.000] (True)\n",
      "    [0.533, 0.627, 0.651, 0.875] (False)\n",
      "    [0.950, 0.514, 0.452, 0.875] (False)\n",
      "    [0.662, 1.000, 0.723, 0.500] (False)\n",
      "    [0.915, 1.000, 0.368, 1.000] (False)\n",
      "    [0.763, 0.667, 0.452, 0.500] (False)\n",
      "\n",
      "Perform oracle with 100.00 accuracy on 94 weight vectors\n",
      "  The oracle will correctly classify 94 weight vectors and wrongly classify 0\n",
      "  Classified 19 matches and 75 non-matches\n",
      "    Purity of oracle classification:  0.798\n",
      "    Entropy of oracle classification: 0.726\n",
      "    Number of true matches:      19\n",
      "    Number of false matches:     0\n",
      "    Number of true non-matches:  75\n",
      "    Number of false non-matches: 0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 94 weight vectors (classified by oracle) from cluster\n",
      "\n",
      "Cluster not pure enough or too large, and can be split further\n",
      "\n",
      "SVM classification of 8172 weight vectors\n",
      "  Based on 19 matches and 75 non-matches\n",
      "  Classified 0 matches and 8172 non-matches\n",
      "\n",
      "148.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Diego\\Anaconda3\\envs\\python2\\lib\\site-packages\\ipykernel\\kernelbase.py:399: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  user_expressions, allow_stdin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analisando o arquivo: diverg(20)1_NEW.csv\n",
      "\n",
      "Load weight vector file: ../csv/conjuntosDS/conjuntosDivergAA/DBLP2-ACM/qp1/diverg(20)1_NEW.csv\n",
      "  Weights to use: ['title', 'authors', 'venue', 'year']\n",
      "  Number of weight vectors: 9760\n",
      "    Number of entity ID pairs that occurred more than once: 0\n",
      "\n",
      "Analyse set of 9760 weight vectors\n",
      "  Containing 2119 true matches and 7641 true non-matches\n",
      "    (21.71% true matches)\n",
      "  Identified 8446 unique weight vectors\n",
      "  Frequency distribution of occurences of weight vectors:\n",
      "    Occurence : Number of weight vectors that occur that often\n",
      "          1 :  7865  (93.12%)\n",
      "          2 :   385  (4.56%)\n",
      "          3 :    96  (1.14%)\n",
      "          4 :    41  (0.49%)\n",
      "          5 :    17  (0.20%)\n",
      "          6 :    12  (0.14%)\n",
      "          7 :     6  (0.07%)\n",
      "          8 :     7  (0.08%)\n",
      "          9 :     1  (0.01%)\n",
      "         10 :     5  (0.06%)\n",
      "         12 :     2  (0.02%)\n",
      "         13 :     1  (0.01%)\n",
      "         14 :     1  (0.01%)\n",
      "         17 :     1  (0.01%)\n",
      "         18 :     2  (0.02%)\n",
      "         19 :     2  (0.02%)\n",
      "         23 :     1  (0.01%)\n",
      "        194 :     1  (0.01%)\n",
      "\n",
      "Identified 8 non-pure unique weight vectors (from 8446 unique weight vectors)\n",
      "Pureness (as percentage of matches) for a certain unique weight vector:\n",
      "  Pureness : Count\n",
      "     1.000 : 1672\n",
      "     0.667 :  1   (all weight vectors with this pureness to be removed)\n",
      "     0.500 :  5   (all weight vectors with this pureness to be removed)\n",
      "     0.368 :  1   (all weight vectors with this pureness to be removed)\n",
      "     0.100 :  1   (all weight vectors with this pureness to be removed)\n",
      "     0.000 : 6766\n",
      "\n",
      "Removed 35 non-pure weight vectors\n",
      "\n",
      "Final number of weight vectors to use: 9725\n",
      "  Number of unique weight vectors: 8439\n",
      "\n",
      "Time to load and analyse the weight vector file: 0.39 sec\n",
      "\n",
      "Initial estimated match proportion: 0.500\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Loop 1: Queue length: 1\n",
      "  Number of manual oracle classifications performed: 0\n",
      "  Size, purity, entropy, and estimated match proportion of clusters in queue:\n",
      "    (8439, 0.5, 1.0, 0.5)\n",
      "\n",
      "Current size of match and non-match training data sets: 0 / 0\n",
      "\n",
      "  Balanced cluster selection, select cluster closest to [1,..,1]\n",
      "\n",
      "Selected cluster with (queue ordering: balance):\n",
      "- Purity 0.50 and entropy 1.00\n",
      "- Size 8439 weight vectors\n",
      "- Estimated match proportion 0.500\n",
      "\n",
      "Sample size for this cluster: 95\n",
      "\n",
      "Perform initial selection using \"far\" method\n",
      "\n",
      "Farthest first selection of 95 weight vectors from 8439 vectors\n",
      "  The selected farthest weight vectors are:\n",
      "    [0.881, 0.922, 0.452, 0.875] (False)\n",
      "    [1.000, 0.591, 0.613, 1.000] (True)\n",
      "    [0.525, 0.500, 0.368, 1.000] (False)\n",
      "    [0.629, 0.510, 0.723, 1.000] (False)\n",
      "    [0.547, 0.697, 0.613, 0.875] (False)\n",
      "    [0.506, 0.618, 0.723, 1.000] (False)\n",
      "    [0.658, 0.500, 0.368, 1.000] (False)\n",
      "    [0.737, 0.756, 0.678, 0.500] (False)\n",
      "    [0.809, 0.647, 0.723, 1.000] (True)\n",
      "    [0.904, 0.529, 0.678, 0.875] (False)\n",
      "    [0.505, 0.500, 0.452, 0.875] (False)\n",
      "    [0.800, 0.769, 0.613, 1.000] (True)\n",
      "    [0.669, 0.623, 0.613, 0.875] (False)\n",
      "    [0.670, 0.677, 0.565, 1.000] (False)\n",
      "    [0.500, 1.000, 0.723, 0.500] (False)\n",
      "    [0.727, 1.000, 0.452, 0.875] (False)\n",
      "    [0.843, 0.881, 0.723, 1.000] (True)\n",
      "    [0.510, 0.833, 0.678, 0.875] (False)\n",
      "    [0.980, 0.516, 0.452, 0.500] (False)\n",
      "    [0.766, 1.000, 0.651, 0.500] (False)\n",
      "    [1.000, 0.504, 0.723, 1.000] (False)\n",
      "    [0.654, 0.518, 0.368, 0.875] (False)\n",
      "    [0.580, 1.000, 0.565, 1.000] (False)\n",
      "    [0.514, 0.786, 0.723, 1.000] (True)\n",
      "    [0.951, 0.760, 0.553, 0.875] (False)\n",
      "    [0.881, 0.917, 0.678, 0.875] (False)\n",
      "    [0.553, 0.826, 0.452, 0.500] (False)\n",
      "    [0.538, 1.000, 0.565, 0.500] (False)\n",
      "    [0.642, 0.905, 0.723, 0.875] (False)\n",
      "    [0.790, 0.736, 0.452, 1.000] (False)\n",
      "    [0.971, 1.000, 0.565, 0.875] (False)\n",
      "    [0.672, 0.711, 0.723, 1.000] (True)\n",
      "    [0.649, 1.000, 0.452, 1.000] (False)\n",
      "    [0.750, 1.000, 0.452, 0.500] (False)\n",
      "    [0.793, 0.731, 0.678, 0.875] (False)\n",
      "    [0.515, 0.938, 0.723, 1.000] (True)\n",
      "    [0.623, 1.000, 0.613, 0.875] (False)\n",
      "    [0.950, 0.683, 0.678, 0.875] (False)\n",
      "    [0.941, 1.000, 0.678, 0.500] (False)\n",
      "    [0.559, 0.659, 0.452, 0.500] (False)\n",
      "    [0.905, 0.756, 0.723, 1.000] (True)\n",
      "    [1.000, 0.725, 0.613, 1.000] (True)\n",
      "    [0.963, 0.696, 0.565, 0.500] (False)\n",
      "    [0.601, 1.000, 0.452, 0.500] (False)\n",
      "    [0.976, 0.737, 0.723, 0.500] (False)\n",
      "    [0.775, 1.000, 0.613, 1.000] (True)\n",
      "    [0.639, 1.000, 0.723, 1.000] (False)\n",
      "    [0.523, 0.708, 0.565, 0.500] (False)\n",
      "    [0.512, 0.500, 0.651, 1.000] (False)\n",
      "    [0.662, 0.584, 0.458, 1.000] (False)\n",
      "    [1.000, 0.755, 0.458, 1.000] (True)\n",
      "    [0.750, 0.613, 0.561, 0.500] (False)\n",
      "    [0.699, 0.547, 0.723, 0.500] (False)\n",
      "    [0.745, 0.578, 0.723, 0.875] (False)\n",
      "    [0.600, 0.500, 0.553, 0.875] (False)\n",
      "    [0.878, 0.565, 0.613, 1.000] (True)\n",
      "    [0.684, 0.773, 0.452, 0.875] (False)\n",
      "    [0.557, 0.750, 0.452, 1.000] (False)\n",
      "    [0.813, 0.514, 0.565, 0.875] (False)\n",
      "    [0.728, 1.000, 0.723, 0.875] (False)\n",
      "    [0.963, 0.500, 0.723, 0.500] (False)\n",
      "    [0.826, 0.693, 0.565, 0.875] (False)\n",
      "    [0.525, 0.712, 0.613, 1.000] (False)\n",
      "    [1.000, 1.000, 0.452, 0.500] (False)\n",
      "    [1.000, 0.850, 0.723, 1.000] (True)\n",
      "    [0.540, 0.644, 0.452, 0.875] (False)\n",
      "    [0.523, 0.783, 0.458, 0.875] (False)\n",
      "    [0.529, 0.502, 0.613, 0.500] (False)\n",
      "    [0.985, 1.000, 0.613, 1.000] (True)\n",
      "    [0.508, 0.885, 0.561, 0.875] (False)\n",
      "    [1.000, 0.910, 0.458, 1.000] (True)\n",
      "    [0.534, 1.000, 0.380, 1.000] (False)\n",
      "    [0.505, 0.500, 0.368, 0.500] (False)\n",
      "    [0.950, 0.514, 0.452, 0.875] (False)\n",
      "    [0.720, 0.509, 0.565, 1.000] (False)\n",
      "    [0.966, 0.642, 0.723, 1.000] (True)\n",
      "    [0.956, 0.609, 0.553, 0.875] (False)\n",
      "    [0.500, 0.500, 0.723, 0.875] (False)\n",
      "    [0.910, 0.900, 0.613, 1.000] (True)\n",
      "    [0.766, 0.903, 0.565, 0.875] (False)\n",
      "    [0.548, 1.000, 0.723, 0.875] (False)\n",
      "    [0.817, 0.500, 0.458, 1.000] (False)\n",
      "    [0.915, 1.000, 0.368, 1.000] (False)\n",
      "    [0.500, 1.000, 0.368, 0.500] (False)\n",
      "    [0.574, 1.000, 0.458, 0.875] (False)\n",
      "    [0.552, 0.844, 0.613, 0.500] (False)\n",
      "    [0.826, 0.500, 0.723, 1.000] (False)\n",
      "    [1.000, 1.000, 0.723, 0.875] (False)\n",
      "    [1.000, 0.596, 0.458, 1.000] (True)\n",
      "    [0.797, 0.622, 0.452, 0.875] (False)\n",
      "    [0.685, 0.819, 0.613, 0.875] (False)\n",
      "    [0.662, 1.000, 0.723, 0.500] (False)\n",
      "    [0.618, 0.517, 0.678, 0.875] (False)\n",
      "    [0.911, 1.000, 0.723, 1.000] (True)\n",
      "    [0.763, 0.667, 0.452, 0.500] (False)\n",
      "\n",
      "Perform oracle with 100.00 accuracy on 95 weight vectors\n",
      "  The oracle will correctly classify 95 weight vectors and wrongly classify 0\n",
      "  Classified 19 matches and 76 non-matches\n",
      "    Purity of oracle classification:  0.800\n",
      "    Entropy of oracle classification: 0.722\n",
      "    Number of true matches:      19\n",
      "    Number of false matches:     0\n",
      "    Number of true non-matches:  76\n",
      "    Number of false non-matches: 0\n",
      "\n",
      "Deleted 95 weight vectors (classified by oracle) from cluster\n",
      "\n",
      "Cluster not pure enough or too large, and can be split further\n",
      "\n",
      "SVM classification of 8344 weight vectors\n",
      "  Based on 19 matches and 76 non-matches\n",
      "  Classified 0 matches and 8344 non-matches\n",
      "\n",
      "90.0\n"
     ]
    }
   ],
   "source": [
    "# Recursive training example selection for entity resolution\n",
    "#\n",
    "# Copyright 2015 Peter Christen, Dinusha Vatsalan and Qing Wang\n",
    "# Email: peter.christen@anu.edu.au\n",
    "# Web:   http://users.cecs.anu.edu.au/~christen/\n",
    "#\n",
    "#   This program is free software: you can redistribute it and/or modify\n",
    "#   it under the terms of the GNU General Public License as published by\n",
    "#   the Free Software Foundation, either version 3 of the License, or\n",
    "#   (at your option) any later version.\n",
    "#\n",
    "#   This program is distributed in the hope that it will be useful,\n",
    "#   but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "#   GNU General Public License for more details.\n",
    "#\n",
    "#   You should have received a copy of the GNU General Public License\n",
    "#   along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "# TODO: - allow several weight vectors per corner in select_corners function\n",
    "#       - what to do with clusters which are pure but too large\n",
    "#         (> max_cluster_size), how to split? (they need to be split)\n",
    "#\n",
    "# Usage:\n",
    "#\n",
    "# python recursive-train-selection.py [weight_vector_file] [min_data_set_size]\n",
    "#                                     [weights_to_use] [init_method]\n",
    "#                                     [select_method] [sample_error]\n",
    "#                                     [oracle_acc] [split_classifier]\n",
    "#                                     [min_cluster_size] [max_cluster_size]\n",
    "#                                     [min_purity] [budget_num_class]\n",
    "#                                     [res_file_name]\n",
    "# where:\n",
    "#\n",
    "# weight_vector_file  The name of the weight vector file, with an assumed\n",
    "#                     format of: [rec_id1,rec_id2,true_match_status,w1,w2..]\n",
    "#                     where: rec_id1/rec_id2 are the record identifiers,\n",
    "#                            true_match is 1.0 if the weight vector was\n",
    "#                              generated by a true matching record pair,\n",
    "#                              0.0 otherwise,\n",
    "#                            w1, w2, ..., wd the d weights (normalised into\n",
    "#                              0..1)\n",
    "# min_data_set_size   The number of records in the smaller of the two data\n",
    "#                     sets, used to calculate the initial proportion of\n",
    "#                     matches.\n",
    "#\n",
    "# weights_to_use      A list of numbers of the weights to use. Must be of\n",
    "#                     the format [x,y,z] with x/y/z>=0 and x/y/z<=(d-1) and no\n",
    "#                     spaces between numbers.\n",
    "#\n",
    "# init_method         How to select the initial weight vectors, can either be\n",
    "#                     'far' (select weight vectors farthest away from each\n",
    "#                     other), '01' (select weight vectors closest to [0,..,0]\n",
    "#                     and [1,..,1], 'corner' (select weight vectors closest to\n",
    "#                     all corners of the multi-dimensional space), and 'ran'\n",
    "#                     for a random selection.\n",
    "#\n",
    "# select_method       Which approach to use when selecting representative\n",
    "#                     weight vectors in a cluster, possible are:\n",
    "#                       'far' (only use farthest away weight vectors), or\n",
    "#                       'far_med' (also include medoid weight vector)\n",
    "#                       'dense' (density based)\n",
    "#                       'aggl'  (agglomerative clustering based)\n",
    "#                       'ran' (random selection)\n",
    "#\n",
    "# sample_error        When calculating number of samples to use for a certain\n",
    "#                     cluster, the margin of error required. Must be a value\n",
    "#                     between 0.0 and below 1.0 (typical 0.05 to 0.2)\n",
    "#\n",
    "# oracle_acc          The assumed accuracy of the oracle a value between 0.5\n",
    "#                     and 1.\n",
    "# \n",
    "# split_classifier    The approach used to classify and split matches from\n",
    "#                     non-matches, possible are:\n",
    "#                       'knn'   (for k nearest neighbour classifier)\n",
    "#                       'svm'   (for support vector machine classifier)\n",
    "#                       'dtree' (for decision tree classifier)\n",
    "#\n",
    "# min_cluster_size    The minimum size of a cluster allowed, clusters are not\n",
    "#                     split further below that size. Must be a positive number.\n",
    "#\n",
    "# max_cluster_size    The maximum size of a cluster allowed before it is added\n",
    "#                     to the training set. Must be a positive number larger\n",
    "#                     than min_cluster_size.\n",
    "#\n",
    "# min_purity          The minimum 'purity' (i.e. accuracy) of a cluster, once\n",
    "#                     this is reached a cluster will not be split further. Must\n",
    "#                     be a value between 0.5 and 1.\n",
    "#\n",
    "# budget_num_class    The number of manual classifications we can do by the\n",
    "#                     oracle.\n",
    "#\n",
    "# queue_order         The method used to order the queue of clusters and\n",
    "#                     decide which cluster to process next. Possible are:\n",
    "#                     - 'fifo'      First in first out, our initial approach\n",
    "#                                   (PAKDD'15)\n",
    "#                     - 'random'    Randomly select a cluster from the queue\n",
    "#                     - 'max_puri'  Cluster with highest purity first (based\n",
    "#                                   on purity of parent cluster, as child\n",
    "#                                   cluster purity can only be larger)\n",
    "#                     - 'min_puri'  Clusters with lowest purity first (again\n",
    "#                                   based on parent cluster purity)\n",
    "#                     - 'min_entr'  Clusters with lowest entropy first (based \n",
    "#                                   on entropy of parent cluster, as child\n",
    "#                                   cluster entropy can only be smaller)\n",
    "#                     - 'max_entr'  Clusters with highest entropy first (again\n",
    "#                                   based on parent cluster entropy)\n",
    "#                     - 'max_size'  Largest clusters first\n",
    "#                     - 'min_size'  Smallest clusters first\n",
    "#                     - 'close_01'  Closest to 0 or 1 corner\n",
    "#                     - 'close_mid' Closest to middle (half between 0 and 1\n",
    "#                                   corners)\n",
    "#                     - 'balance'   Select so that training set sizes are\n",
    "#                                   balanced\n",
    "#                     - 'sample'    Select cluster which has largest ratio of\n",
    "#                                   cluster size divided by number of number\n",
    "#                                   of samples required\n",
    "#\n",
    "# res_file_name       Name of the file where results are to be appended.\n",
    "\n",
    "# TODO: Maybe add: ****************************\n",
    "# fuzzy_reg_ratio     A value between 0 and 1 giving the maximum ratio of the\n",
    "#                     fuzzy region from where not to select weight vectors in\n",
    "#                     the splitting phase (i.e. if distances between matches\n",
    "#                     and non-matches are nearly the same (idea adapted from:\n",
    "#                     http://link.springer.com/chapter/10.1007/11677437_12,\n",
    "#                     Section 4.2)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "import gzip\n",
    "import itertools\n",
    "import random\n",
    "import math\n",
    "import os, errno\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "import sklearn.svm\n",
    "import sklearn.tree\n",
    "import sklearn.cluster\n",
    "\n",
    "import csv\n",
    "import copy\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Define some constant values\n",
    " \n",
    "# Which column in weight vector file contains match status\n",
    "#\n",
    "# TRUE_MATCH_COL = 2\n",
    "TRUE_MATCH_COL = 6 #Após a próxima coluna é que ficam os vetores de similaridade\n",
    "\n",
    "# Maximum allowed pureness before a weight vector is completely removed from\n",
    "# input weight vector set (if larger then only the minority 'class' weight\n",
    "# vectors are removed)\n",
    "#\n",
    "MIN_REMOVE_PURENESS = 0.1\n",
    "\n",
    "# k-nn classifier k value\n",
    "#\n",
    "KNN_K = 7\n",
    "\n",
    "# Number of weight vectors to sample (per cluster) if a set is too large in the\n",
    "# agglomerative selection approach\n",
    "#\n",
    "NUM_SAMPLE = 100\n",
    "\n",
    "RUN_SVM = False  # If the final SVM classifers are to be run or not\n",
    "\n",
    "# Which distance calculation to use for cluster minimums\n",
    "#\n",
    "CLUSTER_MIN_DIST = 'average'  # One of: 'single', 'average', 'complete'\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Process command line arguments\n",
    "#\n",
    "#weight_vec_file_name = sys.argv[1]\n",
    "weight_vec_file_name = 'vetorSimilaridades-14-03.csv'\n",
    "# weight_vec_file_name = 'vetorSimilaridades-DEMO.csv'\n",
    "\n",
    "#min_data_set_size = int(sys.argv[2])\n",
    "min_data_set_size = 1295\n",
    "assert min_data_set_size > 1\n",
    "\n",
    "#weights_to_use = sys.argv[3]  # List of weights to use (format: [i,j,.. ,k])\n",
    "# weights_to_use = '[0,1,2,3,4]'  # List of weights to use (format: [i,j,.. ,k])\n",
    "# weights_to_use = '[0,1,2,3,4,5,6]'  # List of weights to use (format: [i,j,.. ,k])\n",
    "weights_to_use = '[0,1,2,3]'  # List of weights to use (format: [i,j,.. ,k])\n",
    "weights_to_use_list = eval(weights_to_use)\n",
    "num_weights = len(weights_to_use_list)\n",
    "\n",
    "for i in range(num_weights):  # Convert into indices in weight vector lists\n",
    "  assert weights_to_use_list[i] >= 0\n",
    "#   weights_to_use_list[i] += 3 #A partir da 4ª coluna (coluna 3)\n",
    "  weights_to_use_list[i] += 7 #A partir da 8ª coluna (coluna 7) de vetor_similaridade\n",
    "\n",
    "#init_method = sys.argv[4]\n",
    "init_method = 'far'\n",
    "assert init_method in ['far', '01', 'corner', 'ran']\n",
    "\n",
    "#select_method = sys.argv[5]\n",
    "select_method = 'far'\n",
    "assert select_method in ['far', 'far_med', 'ran', 'dense', 'aggl']\n",
    "\n",
    "#sample_error = float(sys.argv[6])\n",
    "sample_error = 0.1\n",
    "assert sample_error > 0.0 and sample_error < 1.0\n",
    "\n",
    "#oracle_acc = float(sys.argv[7])\n",
    "oracle_acc = 1.0\n",
    "assert oracle_acc > 0.5 and oracle_acc <= 1.0\n",
    "\n",
    "#split_classifier = sys.argv[8]\n",
    "split_classifier = 'svm'\n",
    "assert split_classifier in ['knn', 'svm', 'dtree']\n",
    "\n",
    "#min_cluster_size = int(sys.argv[9])\n",
    "min_cluster_size = 20\n",
    "assert min_cluster_size >= 1\n",
    "\n",
    "#max_cluster_size = int(sys.argv[10])\n",
    "max_cluster_size = 50\n",
    "assert max_cluster_size > min_cluster_size\n",
    "\n",
    "#min_purity = float(sys.argv[11])\n",
    "min_purity = 0.95\n",
    "assert min_purity > 0.5 and min_purity < 1.0\n",
    "\n",
    "#budget_num_class = int(sys.argv[12])\n",
    "# budget_num_class = 1178 # Tamanho máximo de PC\n",
    "budget_num_class = 200 # Vou deixar assim por enquanto\n",
    "assert budget_num_class >= 1\n",
    "\n",
    "#queue_order = sys.argv[13]\n",
    "# queue_order = 'random'\n",
    "queue_order = 'balance'\n",
    "assert queue_order in ['fifo', 'random', 'max_puri', 'min_puri', 'max_size',\n",
    "                       'min_size', 'min_entr', 'max_entr', 'close_01',\n",
    "                       'close_mid', 'balance', 'sample']\n",
    "\n",
    "#res_file_name = sys.argv[14]\n",
    "res_file_name = 'resultado.csv'\n",
    "\n",
    "weight_vector_dict_orig = {}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Function to load weight vector file\n",
    "#\n",
    "def load_weight_vec_file(in_file_name, weights_to_use_list):\n",
    "  \"\"\"Load weight vector file, return a dictionary of weight vectors (as lists)\n",
    "     of the form:\n",
    "                 [rec_id1,rec_id2,true_match_status,w1,w2,...,wd]\n",
    "     where only the selected weights in the 'weights_to_use_list' are being\n",
    "     kept.\n",
    "\n",
    "     Returns a list of the names of these weights (from header line) as well\n",
    "     as the list of weight vectors, where each weight vector is a tuple of\n",
    "     weights.\n",
    "  \"\"\"\n",
    "\n",
    "  print\n",
    "  print 'Load weight vector file:', in_file_name\n",
    "\n",
    "  # Load weight vector file, extract weights from selected attributes\n",
    "  #\n",
    "  weight_vector_dict = {}  # Keys will be record identifiers\n",
    "\n",
    "  num_dup_rec_ids = 0  # Number of record IDs (antity ID pairs) that occur\n",
    "                       # several times\n",
    "\n",
    "  if (in_file_name.endswith('.gz')):\n",
    "    in_file = gzip.open(in_file_name)\n",
    "  else:\n",
    "    in_file = open(in_file_name)\n",
    "  header_line = in_file.readline()\n",
    "  #header_list = header_line.strip().split(',') #Remove todos os espaços em branco e divide a partir das vírgulas\n",
    "  \n",
    "  header_list = header_line.strip().split(';') #Remove todos os espaços em branco e divide a partir das vírgulas\n",
    "\n",
    "  weights_name_list = []\n",
    "  for w in weights_to_use_list:\n",
    "    weights_name_list.append(header_list[w])\n",
    "  print '  Weights to use:', weights_name_list\n",
    "\n",
    "  for line in in_file: #Para cada linha no arquivo\n",
    "    line_list = line.strip().split(';') #Lista com todas as linhas\n",
    "    #print line_list\n",
    "    #rec_id =    line_list[0].strip()+'-'+line_list[1].strip() #Armazena os identificadores dos pares!\n",
    "    rec_id =    line_list[0].strip()+'~'+line_list[1].strip() #Armazena os identificadores dos pares!\n",
    "\n",
    "    # Check if a certain entity-ID pair occurs more than once\n",
    "    #\n",
    "    if (rec_id in weight_vector_dict):\n",
    "      num_dup_rec_ids += 1\n",
    "    assert line_list[TRUE_MATCH_COL] in ['0.0', '1.0']\n",
    "\n",
    "    if (line_list[TRUE_MATCH_COL] == '1.0'):\n",
    "      match_status = True\n",
    "    else:\n",
    "      match_status = False\n",
    "\n",
    "    this_weight_vec = []\n",
    "    for w in weights_to_use_list:\n",
    "      this_weight_vec.append(float(line_list[w].strip()))\n",
    "\n",
    "    weight_vector_dict[rec_id] = (match_status, tuple(this_weight_vec))\n",
    "\n",
    "  print '  Number of weight vectors:', len(weight_vector_dict)\n",
    "  print '    Number of entity ID pairs that occurred more than once:', \\\n",
    "        num_dup_rec_ids \n",
    "  print\n",
    "  \n",
    "  #Adicionado por mim\n",
    "  \n",
    "  #lst = weight_vector_dict.items() \n",
    "  \n",
    "  #for i in [1,4,10]:\n",
    "\t#print lst[i] \n",
    "\t\n",
    "  sub = 0.1889763779527559\n",
    "  sub2 = 0.5487804878048781\n",
    "     \n",
    "    \n",
    "  for k,v in weight_vector_dict.iteritems(): #v[0] corresponde à classe; \n",
    "\t\t\t\t\t\t\t\t\t\t\t #v[1] corresponde a uma tupla com o vetor de pesos\n",
    "    if (sub in v[1]) & (sub2 in v[1]):\n",
    "        print k\n",
    "  #Isso é retornado:\n",
    "\t#REC_ID-422-REC_ID-230\n",
    "\t#REC_ID-425-REC_ID-217\n",
    "\t#REC_ID-425-REC_ID-215\n",
    "\t#REC_ID-423-REC_ID-230\n",
    "\t#REC_ID-424-REC_ID-230\n",
    "\t  \n",
    "  #print weight_vector_dict['REC_ID-1~REC_ID-0']\n",
    "  \n",
    "  \n",
    "\n",
    "  return weights_name_list, weight_vector_dict\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Function to analyse the quality of a dictionary of weight vectors, and then\n",
    "# remove weight vectors of low quality (i.e. those that correspond to both\n",
    "# matches and non-matches).\n",
    "#\n",
    "def analyse_filter_weight_vectors(weight_vector_dict):\n",
    "  \"\"\"Find number of unique weight vectors, their frequencies, and their\n",
    "     pureness (i.e. how many are matches/non-matches).\n",
    "\n",
    "     Returns the modified weight vector dictionary with non-pure weight vectors\n",
    "     removed, a dictionary with all unique weight vectors, as well as the\n",
    "     number of true matches and non-matches in the original weight vector\n",
    "     dictionary.\n",
    "  \"\"\"\n",
    "\n",
    "  num_weight_vectors = len(weight_vector_dict)\n",
    "\n",
    "  print 'Analyse set of %d weight vectors' % (num_weight_vectors)\n",
    "\n",
    "  unique_weight_vec_dict = {}  # Keys are weight vectors, values counts of how\n",
    "                               # of these are matches and non-matches\n",
    "  num_true_matches =     0  # Count the number of true matches and non-matches\n",
    "  num_true_non_matches = 0\n",
    "\n",
    "  for rec_id in weight_vector_dict:\n",
    "    (match_status, weight_vector_tuple) = weight_vector_dict[rec_id]\n",
    "\n",
    "    match_count_list = unique_weight_vec_dict.get(weight_vector_tuple, [0,0])\n",
    "    if (match_status == True):\n",
    "      num_true_matches += 1\n",
    "      match_count_list[0] += 1\n",
    "    else:\n",
    "      num_true_non_matches += 1\n",
    "      match_count_list[1] += 1\n",
    "\n",
    "    unique_weight_vec_dict[weight_vector_tuple] = match_count_list\n",
    "\n",
    "  num_unique_weight_vectors = len(unique_weight_vec_dict)\n",
    "\n",
    "  print '  Containing %d true matches and %d true non-matches' % \\\n",
    "        (num_true_matches, num_true_non_matches)\n",
    "  print '    (%.2f%% true matches)' % \\\n",
    "        (100.0*num_true_matches / len(weight_vector_dict))\n",
    "\n",
    "  print '  Identified %d unique weight vectors' % (num_unique_weight_vectors)\n",
    "\n",
    "  count_dict = {}  # Counts of how often weight vectors occur\n",
    "  for match_count_list in unique_weight_vec_dict.itervalues():\n",
    "    weight_vec_count = sum(match_count_list)\n",
    "    sum_count = count_dict.get(weight_vec_count, 0) + 1\n",
    "    count_dict[weight_vec_count] = sum_count\n",
    "\n",
    "  count_list = count_dict.items()\n",
    "  count_list.sort()\n",
    "\n",
    "  print '  Frequency distribution of occurences of weight vectors:'\n",
    "  print '    Occurence : Number of weight vectors that occur that often'\n",
    "  for (freq, count) in count_list:\n",
    "    print '      %5d : %5d  (%.2f%%)' % \\\n",
    "          (freq, count, 100.0*count/num_unique_weight_vectors)\n",
    "  print\n",
    "\n",
    "  # Identify all non-pure weight vectors and remove from original dictionary of\n",
    "  # weight vectors\n",
    "  #\n",
    "  non_pure_weight_vec_dict = {}  # Keys will be weight vector tuples, values\n",
    "                                 # their pureness\n",
    "  pureness_dict = {}  # Also collect statistics\n",
    "\n",
    "  for (weight_vector_tuple, match_count_list) in \\\n",
    "                                           unique_weight_vec_dict.iteritems():\n",
    "    pureness = float(match_count_list[0]) / sum(match_count_list)\n",
    "    pureness_count = pureness_dict.get(pureness, 0) + 1\n",
    "    pureness_dict[pureness] = pureness_count\n",
    "\n",
    "    if (pureness not in [0.0, 1.0]):\n",
    "      non_pure_weight_vec_dict[weight_vector_tuple] = pureness\n",
    "\n",
    "  pureness_list = pureness_dict.items()\n",
    "  pureness_list.sort(reverse=True)\n",
    "\n",
    "  print 'Identified %d non-pure unique weight vectors' % \\\n",
    "        (len(non_pure_weight_vec_dict)), '(from %d unique weight vectors)' % \\\n",
    "        (len(unique_weight_vec_dict))\n",
    "\n",
    "  print 'Pureness (as percentage of matches) for a certain unique weight ' + \\\n",
    "        'vector:'\n",
    "  print '  Pureness : Count'\n",
    "  for (pureness, count) in pureness_list:\n",
    "    print '     %5.3f : %2d' % (pureness, count),\n",
    "    if (pureness not in [0.0, 1.0]):\n",
    "      if ((pureness < MIN_REMOVE_PURENESS) or \\\n",
    "          (pureness > (1.0- MIN_REMOVE_PURENESS))):\n",
    "        print '  (minority class weight vectors with this pureness to be ' + \\\n",
    "              'removed)'\n",
    "      else:\n",
    "        print '  (all weight vectors with this pureness to be removed)'\n",
    "    else:\n",
    "      print\n",
    "  print\n",
    "  \n",
    "  # Remove non-pure weight vectors from the original dictionary of weight\n",
    "  # vectors (if pureness worse than MIN_REMOVE_PURENESS then remove any\n",
    "  # weight vector, otherwise only remove minority 'class' weight vectors\n",
    "  #\n",
    "  for rec_id in weight_vector_dict.keys():\n",
    "    match_status, weight_vector_tuple = weight_vector_dict[rec_id]\n",
    "\n",
    "    if weight_vector_tuple in non_pure_weight_vec_dict:\n",
    "      weight_vector_pureness = non_pure_weight_vec_dict[weight_vector_tuple]\n",
    "\n",
    "      # Check if the weight vector has to be removed in any case\n",
    "      #\n",
    "      if ((weight_vector_pureness > MIN_REMOVE_PURENESS) and \\\n",
    "          (weight_vector_pureness < (1.0-MIN_REMOVE_PURENESS))):\n",
    "        del weight_vector_dict[rec_id]\n",
    "\n",
    "      else:  # Only remove if the weight vector is in the minority 'class'\n",
    "\n",
    "        # Mostly non-matches, so only remove weight vectors which are true\n",
    "        # matches\n",
    "        #\n",
    "        if (weight_vector_pureness <= MIN_REMOVE_PURENESS):\n",
    "          if (match_status == True):\n",
    "            del weight_vector_dict[rec_id]\n",
    "\n",
    "        else:  # Mostly matches, so only remove true non-matches\n",
    "          if (match_status == False):\n",
    "            del weight_vector_dict[rec_id]\n",
    "\n",
    "  print 'Removed %d non-pure weight vectors' % \\\n",
    "        (num_weight_vectors - len(weight_vector_dict))\n",
    "  print\n",
    "\n",
    "  # Generate a weighted dictionary of unique weight vectors, their counts, and\n",
    "  # their match status\n",
    "  #\n",
    "  weighted_unique_weight_vec_dict = {}  # Keys are weight vectors, values are\n",
    "                                        # counts of how often they occur and\n",
    "                                        # their match status\n",
    "\n",
    "  for rec_id in weight_vector_dict:\n",
    "    (match_status, weight_vector_tuple) = weight_vector_dict[rec_id]\n",
    "\n",
    "    if (weight_vector_tuple not in weighted_unique_weight_vec_dict):\n",
    "      weight_vector_count_match_list = [1, match_status]\n",
    "    else:\n",
    "      weight_vector_count_match_list = \\\n",
    "                          weighted_unique_weight_vec_dict[weight_vector_tuple]\n",
    "      weight_vector_count_match_list[0] += 1\n",
    "      assert weight_vector_count_match_list[1] == match_status\n",
    "    weighted_unique_weight_vec_dict[weight_vector_tuple] = \\\n",
    "                                                weight_vector_count_match_list\n",
    "\n",
    "  print 'Final number of weight vectors to use:', len(weight_vector_dict)\n",
    "  print '  Number of unique weight vectors:', \\\n",
    "        len(weighted_unique_weight_vec_dict)\n",
    "  print\n",
    "\n",
    "  return weight_vector_dict, weighted_unique_weight_vec_dict, \\\n",
    "         num_true_matches, num_true_non_matches\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Function to calculate Euclidean distance between two vectors\n",
    "#\n",
    "def euclidean_dist(vec1, vec2, vec_len):\n",
    "\n",
    "  dist = 0.0\n",
    "\n",
    "  for i in range(vec_len):\n",
    "    x = vec1[i] - vec2[i]\n",
    "    dist += x*x\n",
    "\n",
    "  return math.sqrt(dist)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Function to convert a weight vector into a string with a specific number of\n",
    "# digits (with 3 as default)\n",
    "#\n",
    "def weight_vector_to_str(weight_vector, num_digit=3):\n",
    "  weight_vector_str = '['\n",
    "  for w in weight_vector:\n",
    "    digit_str = str(round(w, num_digit))\n",
    "    if (len(digit_str.split('.')[-1]) < num_digit):\n",
    "      num_miss_zero = num_digit - len(digit_str.split('.')[-1])\n",
    "      digit_str += '0'*num_miss_zero\n",
    "\n",
    "    weight_vector_str = weight_vector_str+digit_str+', '\n",
    "  weight_vector_str = weight_vector_str[:-2]+']'\n",
    "\n",
    "  return weight_vector_str\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Function to select weight vectors closest to [0,..,0] and [1,..,1] corners\n",
    "#\n",
    "def select_01(weight_vec_dict, k, num_weights):\n",
    "  \"\"\"Return k selected weight vectors from the given weight vector dictionary\n",
    "     that are closest to the  [0,..,0] and [1,..,1] corners, with k/2 selected\n",
    "     in each corner (if k is odd one more weight vector is selected in the\n",
    "     [1,...,1] (match) corner.\n",
    "\n",
    "     Returns a new dictionary that contains k weight vectors.\n",
    "  \"\"\"\n",
    "\n",
    "  print 'Initial selection of weight vectors closest to [0,..,0] and [1,..,1]'\n",
    "\n",
    "  zero_vector = [0.0]*num_weights\n",
    "  one_vector =  [1.0]*num_weights\n",
    "\n",
    "  # Two lists with tuples (distance to 0/1 and record identifier)\n",
    "  #\n",
    "  zero_close_list = []  # List of weight vectors closest to [0,..,0]\n",
    "  one_close_list =  []  # List of weight vectors closest to [1,..,1]\n",
    "\n",
    "  # Calculate length of the two lists\n",
    "  #\n",
    "  zero_list_len = k/2\n",
    "  if (k % 2 == 0):\n",
    "    one_list_len =  k/2\n",
    "  else:\n",
    "    one_list_len =  k/2+1\n",
    "\n",
    "  print '  Select %d weight vectors closest to zero and %d closest to one.' % \\\n",
    "        (zero_list_len, one_list_len)\n",
    "\n",
    "  max_0_dist = -1.0  # Current maximum distance to 0\n",
    "  max_1_dist = -1.0  # Current maximum distance to 1\n",
    "\n",
    "  for weight_vector_tuple in weight_vec_dict:\n",
    "    dist_to_0 = euclidean_dist(weight_vector_tuple, zero_vector, num_weights)\n",
    "    dist_to_1 = euclidean_dist(weight_vector_tuple, one_vector, num_weights)\n",
    "\n",
    "    #zero_close_list.append([dist_to_0, weight_vector_tuple])\n",
    "    #one_close_list.append([dist_to_1, weight_vector_tuple])\n",
    "\n",
    "    if (len(zero_close_list) < zero_list_len):  # List can grow\n",
    "      zero_close_list.append([dist_to_0, weight_vector_tuple])\n",
    "      if (dist_to_0 > max_0_dist):\n",
    "        max_0_dist = dist_to_0\n",
    "      zero_close_list.sort()\n",
    "    elif (dist_to_0 < max_0_dist):  # We have to replace a list element\n",
    "      zero_close_list = zero_close_list[:-1]  # Remove furthest away w. vector\n",
    "      zero_close_list.append([dist_to_0, weight_vector_tuple])\n",
    "      zero_close_list.sort()\n",
    "      max_0_dist = zero_close_list[-1][0]\n",
    "\n",
    "    if (len(one_close_list) < one_list_len):  # List can grow\n",
    "      one_close_list.append([dist_to_1, weight_vector_tuple])\n",
    "      if (dist_to_1 > max_1_dist):\n",
    "        max_1_dist = dist_to_1\n",
    "      one_close_list.sort()\n",
    "    elif (dist_to_1 < max_1_dist):  # We have to replace a list element\n",
    "      one_close_list = one_close_list[:-1]  # Remove furthest away w. vector\n",
    "      one_close_list.append([dist_to_1, weight_vector_tuple])\n",
    "      one_close_list.sort()\n",
    "      max_1_dist = one_close_list[-1][0]\n",
    "\n",
    "  print '  Closest to zero:'\n",
    "  for (dist, weight_vector_tuple) in zero_close_list:\n",
    "    print '    With distance to zero of %.4f: %s (%s)' % \\\n",
    "          (dist, weight_vector_to_str(weight_vector_tuple), \\\n",
    "           weight_vec_dict[weight_vector_tuple][1])\n",
    "  print '  Closest to one:'\n",
    "  for (dist, weight_vector_tuple) in one_close_list:\n",
    "    print '    With distance to one of %.4f:  %s (%s)' % \\\n",
    "          (dist, weight_vector_to_str(weight_vector_tuple), \\\n",
    "           weight_vec_dict[weight_vector_tuple][1])\n",
    "  print\n",
    "\n",
    "  # Build a dictionary of selected weight vectors\n",
    "  #\n",
    "  selected_weight_vec_dict = {}\n",
    "  for (dist, weight_vector_tuple) in zero_close_list+one_close_list:\n",
    "    weight_vector_count_match_list = weight_vec_dict[weight_vector_tuple]\n",
    "\n",
    "    assert weight_vector_tuple not in selected_weight_vec_dict\n",
    "    selected_weight_vec_dict[weight_vector_tuple] = \\\n",
    "                                                weight_vector_count_match_list\n",
    "\n",
    "  return selected_weight_vec_dict\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Function to select weight vectors closest to each of the 2^num_weights\n",
    "# corners (one weight vector per corner)\n",
    "#\n",
    "def select_corners(weight_vec_dict, num_weights):\n",
    "  \"\"\"Return selected weight vectors from the given weight vector dictionary\n",
    "     that are closest to each of the 2^num_weights corners, i.e. one weight\n",
    "     vector each from [0,..,0], [0,..,1], ... [1,..,1].\n",
    "\n",
    "     Returns a new dictionary that contains 2^num_weights weight vectors.\n",
    "  \"\"\"\n",
    "\n",
    "  print 'Initial selection of weight vectors closest to corners (one per ' + \\\n",
    "        'corner)'\n",
    "\n",
    "  # Build a dictionary of selected weight vectors\n",
    "  #\n",
    "  selected_weight_vec_dict = {}\n",
    "\n",
    "  # Get all permutations of 0 / 1 weights\n",
    "  #\n",
    "  all_corners_weight_vector_list = [seq for seq in itertools.product([0.0,1.0],\n",
    "                                    repeat=num_weights)]\n",
    "  assert len(all_corners_weight_vector_list) == 2**num_weights, \\\n",
    "         (len(all_corners_weight_vector_list), 2**num_weights)\n",
    "\n",
    "  for corner_weight_vector in all_corners_weight_vector_list:\n",
    "    min_corner_dist = 99999.0\n",
    "\n",
    "    for weight_vector_tuple in weight_vec_dict:\n",
    "\n",
    "      # Only select a weight vector once\n",
    "      #\n",
    "      if (weight_vector_tuple not in selected_weight_vec_dict):\n",
    "        corner_dist = euclidean_dist(weight_vector_tuple, corner_weight_vector,\n",
    "                                     num_weights)\n",
    "        if (corner_dist < min_corner_dist):\n",
    "          min_corner_dist = corner_dist\n",
    "          min_corner_weight_vector = weight_vector_tuple\n",
    "\n",
    "    corner_weight_vector_count_match_list = \\\n",
    "              weight_vec_dict[min_corner_weight_vector]\n",
    "    assert min_corner_weight_vector not in selected_weight_vec_dict\n",
    "    selected_weight_vec_dict[min_corner_weight_vector] = \\\n",
    "                                          corner_weight_vector_count_match_list\n",
    "    print '  Corner %s has closest weight vector with distance %.4f: %s (%s)' \\\n",
    "          % (weight_vector_to_str(corner_weight_vector), min_corner_dist,\n",
    "             weight_vector_to_str(min_corner_weight_vector),\n",
    "             weight_vec_dict[min_corner_weight_vector][1]) \n",
    "  print\n",
    "\n",
    "  assert len(selected_weight_vec_dict) == 2**num_weights\n",
    "\n",
    "  return selected_weight_vec_dict\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Function to select k weight vectors using the farthest first method\n",
    "#\n",
    "def select_farthest(weight_vec_dict, k, num_weights):\n",
    "  \"\"\"Return k selected weight vectors from the given weight vector dictionary\n",
    "     based on farthest first approach.\n",
    "\n",
    "     Returns a new dictionary that contains k weight vectors.\n",
    "  \"\"\"\n",
    "\n",
    "  print 'Farthest first selection of %d weight vectors from %d vectors' % \\\n",
    "        (k, len(weight_vec_dict))\n",
    "\n",
    "  if (k == len(weight_vec_dict)):  # Return all weight vectors\n",
    "    selected_weight_vector_dict = weight_vec_dict.copy()\n",
    "    return selected_weight_vector_dict\n",
    "\n",
    "  # Keep a dictionary of the so far selected weight vectors\n",
    "  #\n",
    "  selected_weight_vector_dict = {}\n",
    "\n",
    "  # Randomly select the first weight vector\n",
    "  #\n",
    "  first_weight_vector = weight_vec_dict.iterkeys().next()\n",
    "\n",
    "  selected_weight_vector_dict[first_weight_vector] = \\\n",
    "                                          weight_vec_dict[first_weight_vector]\n",
    "\n",
    "  # Loop until we have selected k+1 weight vectors (then remove the first one)\n",
    "  #\n",
    "  while (len(selected_weight_vector_dict) <= k):\n",
    "\n",
    "    loop_max_dist = -1.0    # The maximum distance of any unselected weight\n",
    "                            # vector to a selected weight vector\n",
    "    loop_max_weight_vec = None  # The corresponding record identifier of the\n",
    "                                # weight vector with this maximum distance\n",
    "\n",
    "    # Find the weight vector farthest away from all so far selected weight\n",
    "    # vectors\n",
    "    #\n",
    "    for this_weight_vector_tuple in weight_vec_dict:\n",
    "\n",
    "      # Only consider those not selected so far\n",
    "      #\n",
    "      if this_weight_vector_tuple not in selected_weight_vector_dict:\n",
    "\n",
    "        # Calculate minimum distance of the current weight vector to any so far\n",
    "        # selected weight vectors\n",
    "        #\n",
    "        this_min_dist = 999.0\n",
    "\n",
    "        for sel_weight_vector_tuple in selected_weight_vector_dict:\n",
    "          assert sel_weight_vector_tuple != this_weight_vector_tuple\n",
    "\n",
    "          dist = euclidean_dist(this_weight_vector_tuple, \\\n",
    "                                sel_weight_vector_tuple, num_weights)\n",
    "          if (dist < this_min_dist):\n",
    "            this_min_dist = dist\n",
    "\n",
    "        # Check if this minimum distance is the largest distance in loop so far\n",
    "        #\n",
    "        if (this_min_dist > loop_max_dist):\n",
    "          loop_max_dist =       this_min_dist\n",
    "          loop_max_weight_vec = this_weight_vector_tuple\n",
    "\n",
    "    # Add this new farthest weight vector to the selected weight vectors\n",
    "    #\n",
    "    selected_weight_vector_dict[loop_max_weight_vec] = \\\n",
    "                                           weight_vec_dict[loop_max_weight_vec]\n",
    "\n",
    "  del selected_weight_vector_dict[first_weight_vector]\n",
    "\n",
    "  assert len(selected_weight_vector_dict) == k\n",
    "\n",
    "  print '  The selected farthest weight vectors are:'\n",
    "  for sel_weight_vector in selected_weight_vector_dict:\n",
    "    print '    %s (%s)' % (weight_vector_to_str(sel_weight_vector),\n",
    "          selected_weight_vector_dict[sel_weight_vector][1])\n",
    "  print\n",
    "\n",
    "  return selected_weight_vector_dict\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Function to select k weight vectors that are densest to other weight vectors\n",
    "#\n",
    "def select_densest(weight_vec_dict, k, num_weights):\n",
    "  \"\"\"Return k selected weight vectors from the given weight vector dictionary\n",
    "     using a density based approach.\n",
    "\n",
    "     Returns a new dictionary that contains k weight vectors.\n",
    "  \"\"\"\n",
    "\n",
    "# TOO slow - n^2 approach - how to make faster? numpy\n",
    "# Density based clsutering?\n",
    "\n",
    "  print 'Density-based selection of %d weight vectors from %d vectors' % \\\n",
    "        (k, len(weight_vec_dict))\n",
    "\n",
    "  # Keep a list with average distance to all other weight vectors for each\n",
    "  # weight vector\n",
    "  #\n",
    "  dist_list = []  # To have tuples (average distance, weight_vector)\n",
    "\n",
    "  weight_vector_list = weight_vec_dict.keys()\n",
    "\n",
    "  for this_weight_vector_tuple_1 in weight_vector_list:\n",
    "\n",
    "    dist_sum = 0.0\n",
    "    for this_weight_vector_tuple_2 in weight_vector_list:\n",
    "\n",
    "      if (this_weight_vector_tuple_1 != this_weight_vector_tuple_2):\n",
    "        dist = euclidean_dist(this_weight_vector_tuple_1, \\\n",
    "                              this_weight_vector_tuple_2, num_weights)\n",
    "        dist_sum += dist\n",
    "    # No need to get average as all sums over same number of number of vectors\n",
    "    #\n",
    "    dist_list.append((dist_sum, this_weight_vector_tuple_1))\n",
    "\n",
    "  dist_list.sort()  # Smallest first\n",
    "\n",
    "  # A dictionary of the so far selected weight vectors\n",
    "  #\n",
    "  selected_weight_vector_dict = {}\n",
    "\n",
    "  # k first elements with smallest distance sums\n",
    "  #\n",
    "  for (dist_sum, weight_vector_tuple) in dist_list[:k]:\n",
    "    selected_weight_vector_dict[weight_vector_tuple] = \\\n",
    "                                         weight_vec_dict[weight_vector_tuple]\n",
    "\n",
    "  print '  The selected \"densest\" weight vectors are:'\n",
    "  for sel_weight_vector in selected_weight_vector_dict:\n",
    "    print '    %s (%s)' % (weight_vector_to_str(sel_weight_vector),\n",
    "          selected_weight_vector_dict[sel_weight_vector][1])\n",
    "  print\n",
    "\n",
    "  return selected_weight_vector_dict\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Function to select k weight vectors based on agglomerative clustering (get\n",
    "# k clusters, then select most central weight vector in each cluster)\n",
    "#\n",
    "def select_agglomerative(weight_vec_dict, k, num_weights):\n",
    "  \"\"\"Return k selected weight vectors from the given weight vector dictionary\n",
    "     using an agglomerative clustering approach.\n",
    "\n",
    "     If the weight vector dictionary is too large then sampling of weight\n",
    "     vectors is applied.\n",
    "\n",
    "     Returns a new dictionary that contains k weight vectors.\n",
    "  \"\"\"\n",
    "\n",
    "  num_weight_vec = len(weight_vec_dict)\n",
    "\n",
    "  print 'Agglomerative clustering based selection of' + \\\n",
    "        ' %d weight vectors from %d vectors' % (k, num_weight_vec)\n",
    "\n",
    "  weight_vector_tuple_list = weight_vec_dict.keys()\n",
    "\n",
    "  if (num_weight_vec > k*NUM_SAMPLE):\n",
    "    num_weight_vec = k*NUM_SAMPLE\n",
    "    print '  Randomly select %d weight vectors for clustering' % \\\n",
    "          (num_weight_vec)\n",
    "    random.shuffle(weight_vector_tuple_list)\n",
    "    weight_vector_tuple_list = weight_vector_tuple_list[:num_weight_vec]\n",
    "\n",
    "  # Prepare the data for clustering\n",
    "  #\n",
    "  cluster_data = numpy.zeros([num_weight_vec, num_weights])\n",
    "\n",
    "  i = 0\n",
    "  for weight_vector_tuple in weight_vector_tuple_list:\n",
    "    cluster_data[:][i] = weight_vector_tuple\n",
    "    i += 1\n",
    "\n",
    "  aggl_cluster =   sklearn.cluster.Ward(n_clusters=k, compute_full_tree=False)\n",
    "  cluster_labels = aggl_cluster.fit_predict(cluster_data)\n",
    "\n",
    "  assert max(cluster_labels) == (k-1), (max(cluster_labels), k)\n",
    "\n",
    "  cluster_centroid_list = []\n",
    "\n",
    "  for i in range(k):\n",
    "    cluster_centroid_list.append(numpy.zeros(num_weights))\n",
    "\n",
    "  cluster_size_list = [0]*k\n",
    "\n",
    "  for i in range(num_weight_vec):\n",
    "    cluster_num = cluster_labels[i]\n",
    "    cluster_size_list[cluster_num] += 1\n",
    "#    this_weight_vector = cluster_data[:][i]\n",
    "\n",
    "    for j in range(num_weights):\n",
    "      cluster_centroid_list[cluster_num][j] += cluster_data[i][j]\n",
    "\n",
    "  assert sum(cluster_size_list) == num_weight_vec\n",
    "  print '  Cluster sizes:', cluster_size_list\n",
    "\n",
    "  # Normalise cluster centroids\n",
    "  #\n",
    "  for i in range(k):\n",
    "    for j in range(num_weights):\n",
    "      cluster_centroid_list[i][j] /= cluster_size_list[i]\n",
    "\n",
    "  for i in range(k):  # Check all cluster centroids contain normalised values\n",
    "    assert max(cluster_centroid_list[i]) <= 1.0\n",
    "    assert min(cluster_centroid_list[i]) >= 0.0\n",
    "\n",
    "  # Find weight vector closest to each cluster centroid\n",
    "  #\n",
    "  centroid_closest_weight_vector_dict = {}  # One per cluster\n",
    "\n",
    "  for weight_vector_tuple in weight_vec_dict:  # Loop over all weight vectors\n",
    "\n",
    "    for i in range(k):\n",
    "      cluster_centroid = cluster_centroid_list[i]\n",
    "      cluster_min_dist, closest_weight_vector = \\\n",
    "                        centroid_closest_weight_vector_dict.get(i, [99999, []])\n",
    "      this_dist = euclidean_dist(weight_vector_tuple, cluster_centroid, \\\n",
    "                                 num_weights)\n",
    "      if (this_dist < cluster_min_dist):\n",
    "        centroid_closest_weight_vector_dict[i] = (this_dist, \\\n",
    "                                                  weight_vector_tuple)\n",
    "\n",
    "  # A dictionary of the selected weight vectors\n",
    "  #\n",
    "  selected_weight_vector_dict = {}\n",
    "\n",
    "  for (cluster_min_dist, closest_weight_vector) in \\\n",
    "                             centroid_closest_weight_vector_dict.itervalues():\n",
    "    selected_weight_vector_dict[closest_weight_vector] = \\\n",
    "                                         weight_vec_dict[closest_weight_vector]\n",
    "\n",
    "  print '  The selected weight vectors are:'\n",
    "  for sel_weight_vector in selected_weight_vector_dict:\n",
    "    print '    %s (%s)' % (weight_vector_to_str(sel_weight_vector),\n",
    "          selected_weight_vector_dict[sel_weight_vector][1])\n",
    "  print\n",
    "\n",
    "  return selected_weight_vector_dict\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Function to select k weight vectors randomly\n",
    "#\n",
    "def select_random(weight_vec_dict, k):\n",
    "  \"\"\"Return k randomly selected weight vectors from the given weight vector\n",
    "     dictionary.\n",
    "\n",
    "     Returns a new dictionary that contains k weight vectors.\n",
    "  \"\"\"\n",
    "\n",
    "  print 'Random selection of %d weight vectors from %d vectors' % \\\n",
    "        (k, len(weight_vec_dict))\n",
    "\n",
    "  sel_weight_vec_list = random.sample(weight_vec_dict.keys(), k)\n",
    "\n",
    "  # A dictionary of the so far selected weight vectors\n",
    "  #\n",
    "  selected_weight_vector_dict = {}\n",
    "\n",
    "  for weight_vec_tuple in sel_weight_vec_list:\n",
    "    selected_weight_vector_dict[weight_vec_tuple] = \\\n",
    "                                             weight_vec_dict[weight_vec_tuple]\n",
    "\n",
    "  assert len(selected_weight_vector_dict) == k\n",
    "\n",
    "  print '  The randomly selected weight vectors are:'\n",
    "  for sel_weight_vector in selected_weight_vector_dict:\n",
    "    print '    %s (%s)' % (weight_vector_to_str(sel_weight_vector),\n",
    "          selected_weight_vector_dict[sel_weight_vector][1])\n",
    "  print\n",
    "\n",
    "  return selected_weight_vector_dict\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Function to find and return the most central weight vectors\n",
    "#\n",
    "def select_medoid(weight_vec_dict, k, num_weights):\n",
    "  \"\"\"Return the k most central weight vectors as a dictionary, calculated as\n",
    "     those closest to the centroid.\n",
    "  \"\"\"\n",
    "\n",
    "  print 'Select %d medoids from %d weight vectors' % (k, len(weight_vec_dict))\n",
    "\n",
    "  if (k == len(weight_vec_dict)):  # Return all weight vectors\n",
    "    selected_weight_vector_dict = weight_vec_dict.copy()\n",
    "    return selected_weight_vector_dict\n",
    "\n",
    "  num_weight_vec = len(weight_vec_dict)\n",
    "\n",
    "  # First calculate the centroid of the given set of weight vectors\n",
    "  #\n",
    "  centroid = [0.0]*num_weights\n",
    "\n",
    "  for weight_vector_tuple in weight_vec_dict:\n",
    "    for w in range(num_weights):\n",
    "      centroid[w] += weight_vector_tuple[w]\n",
    "\n",
    "  for w in range(num_weights):  # Calculate averages\n",
    "    centroid[w] /= num_weight_vec\n",
    "\n",
    "  print '  Centroid weight vector:', weight_vector_to_str(centroid)\n",
    "\n",
    "  medoid_list = []\n",
    "  max_medoid_dist = -1.0  # Maximum distance of any medoid to the centroid\n",
    "\n",
    "  for weight_vector_tuple in weight_vec_dict:\n",
    "    centroid_dist = euclidean_dist(centroid, weight_vector_tuple, num_weights)\n",
    "\n",
    "    if (len(medoid_list) < k):  # List can grow\n",
    "      medoid_list.append([centroid_dist, weight_vector_tuple])\n",
    "      if (centroid_dist > max_medoid_dist):\n",
    "        max_medoid_dist = centroid_dist\n",
    "      medoid_list.sort()\n",
    "    elif (centroid_dist < max_medoid_dist): # We have to replace a list element\n",
    "      medoid_list = medoid_list[:-1]  # Remove furthest away weight vector\n",
    "      medoid_list.append([centroid_dist, weight_vector_tuple])\n",
    "      medoid_list.sort()\n",
    "      max_medoid_dist = medoid_list[-1][0]\n",
    "\n",
    "  medoid_dict = {}\n",
    "\n",
    "  print '  %d closest weight vectors:' % (k)\n",
    "  for (dist, weight_vector_tuple) in medoid_list:\n",
    "    medoid_dict[weight_vector_tuple] = weight_vec_dict[weight_vector_tuple]\n",
    "    print '    With distance of %.4f: %s (%s)' % \\\n",
    "          (dist, weight_vector_to_str(weight_vector_tuple), \\\n",
    "           weight_vec_dict[weight_vector_tuple][1])\n",
    "  print\n",
    "\n",
    "  assert len(medoid_dict) == k\n",
    "\n",
    "  return medoid_dict\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Function to perform the oracle\n",
    "# \n",
    "def oracle(weight_vec_dict, acc):\n",
    "  \"\"\"Assume a classification (manually) of the given accuracy on the given\n",
    "     weight vectors.\n",
    "\n",
    "     The function returns two dictionaries, one of the classified matches and\n",
    "     one of the classified non matches, the `purity' of classification as the\n",
    "     maximum of:\n",
    "      - percentage of weight vectors that are classified as matches,\n",
    "      - percentage of weight vectors that are classified as non-matches,\n",
    "     and the entropy of the classification calculated as:\n",
    "\n",
    "     entropy = - {(M/(M+NM)) * log2(M/(M+NM)) + (NM/(M+NM)) * log2(NM/(M+NM))}\n",
    "\n",
    "     where M is the number of matches and NM the number of non-matches.\n",
    "\n",
    "     Purity is a value between 0.5 and 1.0.\n",
    "\n",
    "     'acc' of these will be correct (i.e. according to their true match status,\n",
    "     while 1.0 - 'acc' will be wrong.\n",
    "  \"\"\"\n",
    "\n",
    "  print 'Perform oracle with %.2f accuracy on %d weight vectors' % \\\n",
    "        (100.0*acc, len(weight_vec_dict))\n",
    "\n",
    "  match_dict =     {}\n",
    "  non_match_dict = {}\n",
    "\n",
    "  num_weight_vec = len(weight_vec_dict)\n",
    "\n",
    "  num_correct = int(round(acc*num_weight_vec))\n",
    "  print '  The oracle will correctly classify %d weight vectors and ' % \\\n",
    "        (num_correct) + 'wrongly classify %d' % (num_weight_vec-num_correct)\n",
    "\n",
    "  weight_vector_list = weight_vec_dict.keys()\n",
    "  random.shuffle(weight_vector_list)\n",
    "\n",
    "  # Make sure the weight vectors are unique\n",
    "  #\n",
    "  assert len(set(weight_vector_list)) == len(weight_vector_list)\n",
    "\n",
    "  # Get the list of weight vectors to be classified correctly and wrongly\n",
    "  #\n",
    "  corr_list = random.sample(weight_vector_list, num_correct)\n",
    "  wrong_list = list(set(weight_vector_list) - set(corr_list))\n",
    "  assert len(set(corr_list).intersection(set(wrong_list))) == 0\n",
    "\n",
    "  num_tp = 0\n",
    "  num_fp = 0\n",
    "  num_tn = 0\n",
    "  num_fn = 0\n",
    "\n",
    "  # Perform the oracle classification\n",
    "  #\n",
    "  for weight_vector_tuple in corr_list:  # The correct classification\n",
    "    weight_vector_count_match_list = weight_vec_dict[weight_vector_tuple]\n",
    "    if (weight_vector_count_match_list[1] == True):  # A true match\n",
    "      match_dict[weight_vector_tuple] = weight_vector_count_match_list\n",
    "      num_tp += 1\n",
    "    else:\n",
    "      non_match_dict[weight_vector_tuple] = weight_vector_count_match_list\n",
    "      num_tn += 1\n",
    "\n",
    "  for weight_vector_tuple in wrong_list:  # The wrong classification\n",
    "    weight_vector_count_match_list = weight_vec_dict[weight_vector_tuple]\n",
    "    if (weight_vector_count_match_list[1] == True):  # A true match\n",
    "      non_match_dict[weight_vector_tuple] = weight_vector_count_match_list\n",
    "      num_fn += 1\n",
    "    else:\n",
    "      match_dict[weight_vector_tuple] = weight_vector_count_match_list\n",
    "      num_fp += 1\n",
    "\n",
    "  m =  float(len(match_dict))       # Number of matches\n",
    "  nm = float(len(non_match_dict))   # Number of non-matches\n",
    "  a =  float(len(weight_vec_dict))  # Number of all\n",
    "\n",
    "  purity = max(m/a, nm/a)\n",
    "  assert purity >= 0.5 and purity <= 1.0, purity\n",
    "\n",
    "  if (m != 0.0) and (nm != 0.0):\n",
    "    entropy = - m/a * math.log(m/a, 2) - nm/a * math.log(nm/a, 2)\n",
    "  else:\n",
    "    entropy = 0.0\n",
    "\n",
    "  assert entropy >= 0.0 and entropy <= 1.0, entropy\n",
    "\n",
    "  print '  Classified %d matches and %d non-matches' % \\\n",
    "        (len(match_dict), len(non_match_dict))\n",
    "  print '    Purity of oracle classification:  %.3f' % (purity)\n",
    "  print '    Entropy of oracle classification: %.3f' % (entropy)\n",
    "  print '    Number of true matches:     ', num_tp\n",
    "  print '    Number of false matches:    ', num_fp\n",
    "  print '    Number of true non-matches: ', num_tn\n",
    "  print '    Number of false non-matches:', num_fn\n",
    "  print\n",
    "  assert num_tp+num_fp == len(match_dict)\n",
    "  assert num_tn+num_fn == len(non_match_dict)\n",
    "\n",
    "  if (len(match_dict) == 0):\n",
    "    print '*** Warning: Oracle returns an empty match dictionary ***'\n",
    "  if (len(non_match_dict) == 0):\n",
    "    print '*** Warning: Oracle returns an empty non-match dictionary ***'\n",
    "\n",
    "  return match_dict, non_match_dict, purity, entropy\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Function to perform k nearest neighbour classification and split the cluster\n",
    "#\n",
    "def knn_split_classifier(weight_vector_dict, k, match_dict, non_match_dict):\n",
    "  \"\"\"Split the given weight vector dictionary into matches and non-matches\n",
    "     according to the match and non-match training dictionaries given using\n",
    "     a k nearest neighbour classifier.\n",
    "\n",
    "     Returns two new weight vector dictionaries, one for the classified matches\n",
    "     and the other for the classified non-matches.\n",
    "  \"\"\"\n",
    "\n",
    "  assert k%2 == 1, k  # k must be odd\n",
    "\n",
    "  print '%d-NN classification of %d weight vectors' % \\\n",
    "        (k, len(weight_vector_dict))\n",
    "  print '  Based on %d matches and %d non-matches' % \\\n",
    "        (len(match_dict), len(non_match_dict))\n",
    "\n",
    "  # The two dictionaries of classified weight vectors to be generated\n",
    "  #\n",
    "  match_weight_vector_dict =     {}\n",
    "  non_match_weight_vector_dict = {}\n",
    "\n",
    "  for (weight_vector_tuple, weight_vector_count_match_list) in \\\n",
    "                                              weight_vector_dict.iteritems():\n",
    "\n",
    "    this_dist_list = []  # List of this weight vector's distances to all\n",
    "                         # training vectors\n",
    "\n",
    "    for train_weight_vector in match_dict.iterkeys():\n",
    "      dist = euclidean_dist(weight_vector_tuple, train_weight_vector,\n",
    "                            num_weights)\n",
    "      this_dist_list.append((dist, 'm'))  # Distance to a match\n",
    "\n",
    "    for train_weight_vector in non_match_dict.iterkeys():\n",
    "      dist = euclidean_dist(weight_vector_tuple, train_weight_vector,\n",
    "                            num_weights)\n",
    "      this_dist_list.append((dist, 'nm'))  # Distance to a non-match\n",
    "\n",
    "    this_dist_list.sort()\n",
    "    this_dist_list_k = this_dist_list[:k]  # k closest training vectors\n",
    "\n",
    "    num_matches, num_non_matches = 0, 0\n",
    "\n",
    "    for (dist, match_status) in this_dist_list_k:\n",
    "      if (match_status == 'm'):\n",
    "        num_matches += 1\n",
    "      else:\n",
    "        num_non_matches += 1\n",
    "\n",
    "    if (num_matches > num_non_matches):\n",
    "      match_weight_vector_dict[weight_vector_tuple] = \\\n",
    "                                                 weight_vector_count_match_list\n",
    "    else:\n",
    "      non_match_weight_vector_dict[weight_vector_tuple] = \\\n",
    "                                                 weight_vector_count_match_list\n",
    "\n",
    "  assert len(match_weight_vector_dict) + len(non_match_weight_vector_dict) == \\\n",
    "         len(weight_vector_dict)\n",
    "\n",
    "  print '  Classified %d matches and %d non-matches' % \\\n",
    "        (len(match_weight_vector_dict), len(non_match_weight_vector_dict))\n",
    "  print\n",
    "\n",
    "  return match_weight_vector_dict, non_match_weight_vector_dict\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Function to perform SVM classification and split the cluster\n",
    "#\n",
    "def svm_split_classifier(weight_vector_dict, match_dict, non_match_dict,\n",
    "                         num_weights):\n",
    "  \"\"\"Split the given weight vector dictionary into matches and non-matches\n",
    "     according to the match and non-match training dictionaries given using an\n",
    "     SVM classifier.\n",
    "\n",
    "     Returns two new weight vector dictionaries, one for the classified matches\n",
    "     and the other for the classified non-matches.\n",
    "  \"\"\"\n",
    "\n",
    "  print 'SVM classification of %d weight vectors' % (len(weight_vector_dict))\n",
    "  print '  Based on %d matches and %d non-matches' % \\\n",
    "        (len(match_dict), len(non_match_dict))\n",
    "\n",
    "  # Prepare the training and test data sets for the classifier\n",
    "  #\n",
    "  num_train_vec = len(match_dict) + len(non_match_dict)\n",
    "  num_test_vec =  len(weight_vector_dict)\n",
    "\n",
    "  train_data =    numpy.zeros([num_train_vec, num_weights])\n",
    "  train_class =   numpy.zeros(num_train_vec)\n",
    "  train_weights = numpy.zeros(num_train_vec)\n",
    "\n",
    "  test_data =  numpy.zeros([num_test_vec, num_weights])\n",
    "  test_class = numpy.zeros(num_test_vec)\n",
    "\n",
    "  j = 0\n",
    "  for (weight_vector_tuple, weight_vector_count_match_list) in \\\n",
    "                                   match_dict.iteritems():\n",
    "    train_data[:][j] = weight_vector_tuple\n",
    "    train_class[j] =   1.0\n",
    "    train_weights[j] = weight_vector_count_match_list[0]\n",
    "    assert train_weights[j] >= 1\n",
    "    j += 1\n",
    "  for (weight_vector_tuple, weight_vector_count_match_list) in \\\n",
    "                                non_match_dict.iteritems():\n",
    "    train_data[:][j] = weight_vector_tuple\n",
    "    train_class[j] =   0.0\n",
    "    train_weights[j] = weight_vector_count_match_list[0]\n",
    "    assert train_weights[j] >= 1\n",
    "    j += 1\n",
    "\n",
    "  weight_vectors_to_classify_list = weight_vector_dict.items()\n",
    "\n",
    "  j = 0\n",
    "  for (weight_vector_tuple, weight_vector_count_match_list) in \\\n",
    "                                   weight_vectors_to_classify_list:\n",
    "    test_data[:][j] = weight_vector_tuple\n",
    "    match_status = weighted_unique_weight_vec_dict[weight_vector_tuple][1]\n",
    "    assert match_status in [True, False]\n",
    "    if (match_status == True):\n",
    "      test_class[j] = 1.0\n",
    "    else:\n",
    "      test_class[j] = 0.0\n",
    "    j += 1\n",
    "\n",
    "  classifier = sklearn.svm.SVC(kernel='linear', C=0.1)\n",
    "  classifier.fit(train_data, train_class, sample_weight=train_weights)\n",
    "  class_predict = classifier.predict(test_data)\n",
    "\n",
    "  # The two dictionaries of classified weight vectors to be generated\n",
    "  #\n",
    "  match_weight_vector_dict =     {}\n",
    "  non_match_weight_vector_dict = {}\n",
    "\n",
    "  num_matches, num_non_matches = 0, 0\n",
    "\n",
    "  for i in range(num_test_vec):\n",
    "    weight_vector_tuple, weight_vector_count_match_list = \\\n",
    "                                            weight_vectors_to_classify_list[i]\n",
    "    if (class_predict[i] == 1.0):\n",
    "      num_matches += 1\n",
    "      match_weight_vector_dict[weight_vector_tuple] = \\\n",
    "                                                weight_vector_count_match_list\n",
    "    else:\n",
    "      num_non_matches += 1\n",
    "      non_match_weight_vector_dict[weight_vector_tuple] = \\\n",
    "                                                weight_vector_count_match_list\n",
    "\n",
    "  assert len(match_weight_vector_dict) + len(non_match_weight_vector_dict) == \\\n",
    "         len(weight_vector_dict)\n",
    "\n",
    "  print '  Classified %d matches and %d non-matches' % \\\n",
    "        (len(match_weight_vector_dict), len(non_match_weight_vector_dict))\n",
    "  print\n",
    "\n",
    "  return match_weight_vector_dict, non_match_weight_vector_dict\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Function to perform decision tree classification and split the cluster\n",
    "#\n",
    "def dtree_split_classifier(weight_vector_dict, match_dict, non_match_dict,\n",
    "                           num_weights):\n",
    "  \"\"\"Split the given weight vector dictionary into matches and non-matches\n",
    "     according to the match and non-match training dictionaries given using a\n",
    "     decision tree classifier.\n",
    "\n",
    "     Returns two new weight vector dictionaries, one for the classified matches\n",
    "     and the other for the classified non-matches.\n",
    "  \"\"\"\n",
    "\n",
    "  print 'Decision tree classification of %d weight vectors' % \\\n",
    "        (len(weight_vector_dict))\n",
    "  print '  Based on %d matches and %d non-matches' % \\\n",
    "        (len(match_dict), len(non_match_dict))\n",
    "\n",
    "  # Prepare the training and test data sets for the classifier\n",
    "  #\n",
    "  num_train_vec = len(match_dict) + len(non_match_dict)\n",
    "  num_test_vec =  len(weight_vector_dict)\n",
    "\n",
    "  train_data =    numpy.zeros([num_train_vec, num_weights])\n",
    "  train_class =   numpy.zeros(num_train_vec)\n",
    "  train_weights = numpy.zeros(num_train_vec)\n",
    "\n",
    "  test_data =  numpy.zeros([num_test_vec, num_weights])\n",
    "  test_class = numpy.zeros(num_test_vec)\n",
    "\n",
    "  j = 0\n",
    "  for (weight_vector_tuple, weight_vector_count_match_list) in \\\n",
    "                                   match_dict.iteritems():\n",
    "    train_data[:][j] = weight_vector_tuple\n",
    "    train_class[j] =   1.0\n",
    "    train_weights[j] = weight_vector_count_match_list[0]\n",
    "    assert train_weights[j] >= 1\n",
    "    j += 1\n",
    "  for (weight_vector_tuple, weight_vector_count_match_list) in \\\n",
    "                                non_match_dict.iteritems():\n",
    "    train_data[:][j] = weight_vector_tuple\n",
    "    train_class[j] =   0.0\n",
    "    train_weights[j] = weight_vector_count_match_list[0]\n",
    "    assert train_weights[j] >= 1\n",
    "    j += 1\n",
    "\n",
    "  weight_vectors_to_classify_list = weight_vector_dict.items()\n",
    "\n",
    "  j = 0\n",
    "  for (weight_vector_tuple, weight_vector_count_match_list) in \\\n",
    "                                   weight_vectors_to_classify_list:\n",
    "    test_data[:][j] = weight_vector_tuple\n",
    "    match_status = weighted_unique_weight_vec_dict[weight_vector_tuple][1]\n",
    "    assert match_status in [True, False]\n",
    "    if (match_status == True):\n",
    "      test_class[j] = 1.0\n",
    "    else:\n",
    "      test_class[j] = 0.0\n",
    "    j += 1\n",
    "\n",
    "  classifier = sklearn.tree.DecisionTreeClassifier(criterion='gini')\n",
    "  classifier.fit(train_data, train_class, sample_weight=train_weights)\n",
    "  class_predict = classifier.predict(test_data)\n",
    "\n",
    "  # The two dictionaries of classified weight vectors to be generated\n",
    "  #\n",
    "  match_weight_vector_dict =     {}\n",
    "  non_match_weight_vector_dict = {}\n",
    "\n",
    "  num_matches, num_non_matches = 0, 0\n",
    "\n",
    "  for i in range(num_test_vec):\n",
    "    weight_vector_tuple, weight_vector_count_match_list = \\\n",
    "                                            weight_vectors_to_classify_list[i]\n",
    "    if (class_predict[i] == 1.0):\n",
    "      num_matches += 1\n",
    "      match_weight_vector_dict[weight_vector_tuple] = \\\n",
    "                                                weight_vector_count_match_list\n",
    "    else:\n",
    "      num_non_matches += 1\n",
    "      non_match_weight_vector_dict[weight_vector_tuple] = \\\n",
    "                                                weight_vector_count_match_list\n",
    "\n",
    "  assert len(match_weight_vector_dict) + len(non_match_weight_vector_dict) == \\\n",
    "         len(weight_vector_dict)\n",
    "\n",
    "  print '  Classified %d matches and %d non-matches' % \\\n",
    "        (len(match_weight_vector_dict), len(non_match_weight_vector_dict))\n",
    "  print\n",
    "\n",
    "  return match_weight_vector_dict, non_match_weight_vector_dict\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Function to calculate distance of a cluster from 0 corner\n",
    "#\n",
    "def cluster_0_dist(weight_vector_dict, num_weights, dist_mode):\n",
    "  \"\"\"Calculate distance of a cluster from the [0] corner using one of minimum,\n",
    "     average or maximum distance (corresponding to single, average or complete\n",
    "     link), based on Euclidean distance.\n",
    "\n",
    "     num_weights gives the dimensionality of the weight vectors.\n",
    "\n",
    "     Returns a numerical distance value.\n",
    "  \"\"\"\n",
    "\n",
    "  assert dist_mode in ['single', 'average', 'complete'], dist_mode\n",
    "\n",
    "  zero_vec = [0]*num_weights\n",
    "\n",
    "  min_dist = 999999.99\n",
    "\n",
    "  if (dist_mode == 'single'):\n",
    "    for weight_vector_tuple in weight_vector_dict:\n",
    "      print weight_vector_tuple\n",
    "      this_dist = euclidean_dist(zero_vec, weight_vector_tuple, num_weights)\n",
    "      if (this_dist < min_dist):\n",
    "        min_dist = this_dist\n",
    "\n",
    "  elif (dist_mode == 'complete'):\n",
    "    for weight_vector_tuple in weight_vector_dict:\n",
    "      print weight_vector_tuple\n",
    "      this_dist = euclidean_dist(zero_vec, weight_vector_tuple, num_weights)\n",
    "      if (this_dist > min_dist):\n",
    "        min_dist = this_dist\n",
    "\n",
    "  else:  # Calculate average distance\n",
    "    num_weight_vec = len(weight_vector_dict)\n",
    "    avrg_dist_vec = [0.0]*num_weights\n",
    "    for weight_vector_tuple in weight_vector_dict:\n",
    "      for i in range(num_weights):\n",
    "        avrg_dist_vec[i] += weight_vector_tuple[i]\n",
    "    for i in range(num_weights):\n",
    "      avrg_dist_vec[i] /= num_weight_vec\n",
    "    min_dist = euclidean_dist(zero_vec, avrg_dist_vec, num_weights)\n",
    "\n",
    "  assert min_dist >= 0, min_dist\n",
    "\n",
    "  return min_dist\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Function to calculate number of samples needed from a cluster to obtain\n",
    "# enough examples for a given confidence and margin of error\n",
    "#\n",
    "def get_sample_size(cluster_size, est_proportion, sample_error):\n",
    "  \"\"\"Follows equation on page 505 of An Introduction to Statistical Methods\n",
    "     and Data analysis, Ott and Longnecker, 6th edition.\n",
    "\n",
    "     We assume a confidence level of 95%.\n",
    "  \"\"\"\n",
    "\n",
    "  z_alpha2 = 1.95  # For 95% confidence interval\n",
    "\n",
    "  sample_size = z_alpha2**2 * est_proportion * (1.0 - est_proportion) \\\n",
    "                / (sample_error**2)\n",
    "  #print 'sample size:', sample_size,cluster_size,est_proportion,sample_error\n",
    "\n",
    "  # For small cluster sizes we adjust:\n",
    "  #\n",
    "  sample_size_adj = cluster_size*sample_size / (cluster_size + sample_size)\n",
    "\n",
    "  #print 'Sample size:', int(math.ceil(sample_size_adj))\n",
    "\n",
    "  #if (sample_size != sample_size_adj):\n",
    "  #  print '  Original sample size: %.2f and adjusted sample size: %.2f' % \\\n",
    "  #        (sample_size, sample_size_adj)\n",
    "  #print\n",
    "\n",
    "  return int(math.ceil(sample_size_adj))\n",
    "\n",
    "def atualizaDM_NDM(dic, file1, file2):\n",
    "\n",
    "    dmfile  = open(file1, 'ab')\n",
    "    ndmfile = open(file2, 'ab')\n",
    "\n",
    "    writer_dm = csv.writer(dmfile, delimiter=';', quotechar = ' ')\n",
    "    writer_ndm = csv.writer(ndmfile, delimiter=';', quotechar = ' ')\n",
    "\n",
    "\n",
    "    for pesos in oracle_class_cache_set:\n",
    "\n",
    "        for k,v in dic.iteritems(): \n",
    "            if pesos == v[1]:\n",
    "\n",
    "                ids = k.split('~')\n",
    "\n",
    "                if v[0]:\n",
    "                    print 'Verdadeiro!'\n",
    "                    writer_dm.writerow(ids)\n",
    "                else:\n",
    "                    print 'Falso!'\n",
    "                    writer_ndm.writerow(ids)\n",
    "\n",
    "                break\n",
    "\n",
    "    dmfile.close()\n",
    "    ndmfile.close()\n",
    "    \n",
    "\n",
    "\n",
    "#Geração do conjunto de treinamento\n",
    "def geraTrainSet(dic, dir, file1):\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(dir)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "    #cont = 0\n",
    "    \n",
    "#     trainSet  = open(dir+file1, 'ab')\n",
    "    trainSet  = open(dir+file1, 'wb')\n",
    "\n",
    "    writer_ts = csv.writer(trainSet, delimiter=';', quotechar = ' ')\n",
    " \n",
    "    for pesos in oracle_class_cache_set:\n",
    "\n",
    "        for k,v in dic.iteritems(): \n",
    "            if pesos == v[1]:\n",
    "                ids = k.split('~')\n",
    "\n",
    "                if v[0]:\n",
    "                    #print 'Verdadeiro!'\n",
    "                    #lista = [cont] + ids + list(v[1]) + [1.0]\n",
    "                    lista = list(v[1]) + [1.0]\n",
    "                    writer_ts.writerow(lista)\n",
    "                else:\n",
    "                    #print 'Falso!'\n",
    "                    #lista = [cont] + ids + list(v[1]) + [0.0]\n",
    "                    lista = list(v[1]) + [0.0]\n",
    "                    writer_ts.writerow(lista)\n",
    "                #cont = cont + 1\n",
    "                break\n",
    "        \n",
    "    trainSet.close()\n",
    "\t\n",
    "#Geração do conjunto de treinamento\n",
    "def geraTestSet(dic, dir, file1):\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(dir)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "    #cont = 0\n",
    "    cont1 = 0\n",
    "    \n",
    "#     testSet  = open(dir+file1, 'ab')\n",
    "    testSet  = open(dir+file1, 'wb')\n",
    "\n",
    "    writer_ts = csv.writer(testSet, delimiter=';', quotechar = ' ')\n",
    " \n",
    "\t#Remoção dos vetores selecionados para o conjunto de treinamento\n",
    "    for pesos in oracle_class_cache_set:\n",
    "\n",
    "        for k,v in dic.iteritems(): \n",
    "            if pesos == v[1]:\n",
    "                \n",
    "                del dic[k]\n",
    "                cont1 = cont1 + 1\n",
    "                break\n",
    "\t\n",
    "\t#print 'Número de deleções: %d' %(cont1)\n",
    "\t\n",
    "\t#Geração do conjunto de teste\n",
    "    for k,v in dic.iteritems(): \n",
    "            \n",
    "        ids = k.split('~')\n",
    "\n",
    "        if v[0]:\n",
    "            #lista = [cont] + ids + list(v[1]) + [1.0]\n",
    "            lista = list(v[1]) + [1.0]\n",
    "            writer_ts.writerow(lista)\n",
    "        else:\n",
    "            #lista = [cont] + ids + list(v[1]) + [0.0]\n",
    "            lista = list(v[1]) + [0.0]\n",
    "            writer_ts.writerow(lista)\n",
    "        #cont = cont + 1\n",
    "        \n",
    "        \n",
    "    testSet.close()\n",
    "    \n",
    "# def atualizaEstatAA(permutacao, dir):\n",
    "    \n",
    "#     With open(dir, rb) as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     for row in reader:\n",
    "#         if row[2] = permutacao #Coluna da permutação\n",
    "#             print\n",
    "#             print row\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "#Acho que não vai ser uma função.\n",
    "#Melhor criar o dataframe antes da primeira iteração ea atualizá-lo ao término de cada uma\n",
    "#Depois disso fecha o arquivo e salva\n",
    "# def atualizaEstatAA(permutacao, arq):\n",
    "    \n",
    "#     estatisticas = pd.read_csv(arq)\n",
    "    \n",
    "#     estatisticas.set_index('permutacao')\n",
    "        \n",
    "#         linha = estatisticas.loc[permutacao, : ] #Armazena a linha correspondente à permutação\n",
    "        \n",
    "#         tp = linha[: tp] + tp \n",
    "        #Idem para fp, tn e fn\n",
    "        #calcula as métricas além de armazenar os dados de inspecoesManuais, dm, ndm e permutacao e a etapa \"AA\"\n",
    "        \n",
    "        #Armazena no final do arquivo\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "# def setVetorSim(nome):\n",
    "#     print 'Entrei em setVetorSim() com o arquivo %s' %(nome)\n",
    "#     weight_vec_file_name = nome\n",
    "#     print 'weight_vec_file_name = %s' %(weight_vec_file_name)\n",
    "    \n",
    "# def getNomeVetorSim():\n",
    "#     return weight_vec_file_name\n",
    "\n",
    "# =============================================================================\n",
    "# Main program\n",
    "\n",
    "# Step 1: Load, analyse and clean the weight vectors file\n",
    "#\n",
    "\n",
    "#Repetições para os experimentos\n",
    "\n",
    "bases = [\"DBLP2-ACM\"] #Só funciona para uma base, por que tem-se que alterar os dados iniciais referentes às colunas\n",
    "# qps = [\"QP1\", \"QP2\"] #Todos menos QP6\n",
    "# qps = [\"qp1\", \"qp2b\", \"qp2m\", \"qp2r\", \"qp3all\", \"qp3lot\"] #Todos menos QP6 e QP7\n",
    "qps = [\"qp1\"]\n",
    "\n",
    "for base in bases:\n",
    "    \n",
    "    print(\"Base atual: {0}\".format(base))\n",
    "    \n",
    "    for qp in qps:\n",
    "        \n",
    "        print(\"QP atual: {0}\".format(qp))\n",
    "        \n",
    "        dirOrig = \"../csv/conjuntosDS/conjuntosDivergAA/\"+base+\"/\"+qp+\"/\"\n",
    "        dirEstat = \"../csv/estatisticas/\"+base+\"/\"+qp+\"/\"\n",
    "        estat = dirEstat+\"estatisticaDS.csv\"\n",
    "\n",
    "#         dirOrig = \"../csv/conjuntosDS/conjuntosDiverg/\"\n",
    "#         estat = \"../csv/estatisticaDS.csv\"\n",
    "\n",
    "        # Diretórios para Windows\n",
    "        # dirOrig = \"C:\\Users\\Diego\\Documents\\NetBeansProjects\\Master-SKYAM\\AS\\src\\csv\\conjuntosDS\\conjuntosDiverg\\\\\"\n",
    "        # estat = \"C:\\Users\\Diego\\Documents\\NetBeansProjects\\Master-SKYAM\\AS\\src\\csv\\estatisticaDS.csv\"\n",
    "\n",
    "\n",
    "        # dirOrig = \"..\\..\\Documents\\NetBeansProjects\\Master-SKYAM\\AS\\src\\csv\\conjuntosDS\\conjuntosDiverg\"\n",
    "        # estat = \"..\\..\\Documents\\NetBeansProjects\\Master-SKYAM\\AS\\src\\csv\\estatisticaDS.csv\"\n",
    "\n",
    "        # Diretórios para Linux\n",
    "        # dirOrig = \"./arqResult/csv/conjuntosDS/conjuntosDiverg/\"\n",
    "        # estat = \"./arqResult/csv/estatisticaDS.csv\"\n",
    "\n",
    "        # /home/diego/anaconda3/rerequestingofalgorithmsforconductingresearch/arqResult/csv\n",
    "\n",
    "        # /home/diego/anaconda3/rerequestingofalgorithmsforconductingresearch/arqResult/csv\n",
    "\n",
    "        etapa = '2 - AA[pet-chr]'\n",
    "\n",
    "        import pandas as pd\n",
    "        import re\n",
    "\n",
    "        estatisticas = pd.read_csv(estat, index_col=['algoritmosUtilizados', 'etapa', 'permutacao'], sep=';')\n",
    "        # estatisticas.set_index('permutacao')\n",
    "        estatisticas.head()\n",
    "        print(estatisticas.columns)\n",
    "\n",
    "\n",
    "        print 'estatisticas.shape'\n",
    "        print estatisticas.shape\n",
    "\n",
    "        arquivos = [] #Adicionado depois\n",
    "\n",
    "        for _, _, arquivo in os.walk(dirOrig):\n",
    "             #print(arquivo)\n",
    "             arquivos.extend(arquivo)   \n",
    "\n",
    "        #print 'quantidade de arquivos: %d' %(len(arquivos))        \n",
    "\n",
    "        #linhaAtual\n",
    "        #cont = 0\n",
    "\n",
    "        #for arq in arquivo:\n",
    "        for arq in arquivos:\n",
    "            #print 'Diego %d' %(cont)\n",
    "\n",
    "            if '_NEW' in arq:\n",
    "#             if ('diverg(10)1_NEW' in arq): #Aqui, Diego!\n",
    "                print 'Analisando o arquivo: %s' %(arq)\n",
    "                #cont+=1\n",
    "        #         num = arq.replace('diverg','')\n",
    "        #         num = num.replace('_NEW.csv','')\n",
    "        #'''\n",
    "\n",
    "                num = re.sub('diverg.*\\)', r'', arq) #Alterar para fazer a substituição de tudo em uma linha só\n",
    "                num = num.replace('_NEW.csv','')\n",
    "        #         print(num)\n",
    "\n",
    "                algUtl = re.sub('diverg.*\\(', r'', arq) #Alterar para fazer a substituição de tudo em uma linha só\n",
    "                algUtl = re.sub('\\).*', r'', algUtl) #Alterar para fazer a substituição de tudo em uma linha só\n",
    "        #         alg = alg.replace('\\).*','')\n",
    "        #         print 'alg: %s' %(algUtl)\n",
    "                algUtl = int(algUtl)\n",
    "                #Passando o csv para selecionar os vetores para conjunto de treinamento\n",
    "        #         setVetorSim(arq)\n",
    "\n",
    "        #         print 'Está sendo analisado o arquivo %s' %(getNomeVetorSim())\n",
    "\n",
    "        #       \n",
    "                permutacao = int(num)\n",
    "\n",
    "\n",
    "        #         linhaAtual = estatisticas.xs((algUtl, '1 - acm diverg', permutacao))\n",
    "                linhaAtual = estatisticas.xs((algUtl, '1 - acm diverg', permutacao))\n",
    "                #linhaAtual = estatisticas.loc[algUtl, '1 - acm diverg', permutacao, : ] #Armazena a linha correspondente à permutação\n",
    "        #         linhaAtual = estatisticas.loc[['1 - acm diverg', permutacao], : ] #Armazena a linha correspondente à permutação\n",
    "\n",
    "        #         linhaAtual.tolist()\n",
    "#                 print type(linhaAtual)\n",
    "#                 print 'Linha atual aqui, jovem!'\n",
    "#                 print linhaAtual.shape\n",
    "#                 print linhaAtual\n",
    "        #         print 'colunas'\n",
    "        #         print linhaAtual.columns\n",
    "        #         print linhaAtual\n",
    "                #print type(linhaAtual['precision'])#[:] #.get_value\n",
    "                #print linhaAtual['precision'].item()#[:] #.get_value\n",
    "\n",
    "        #         linha\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                start_time = time.time()\n",
    "\n",
    "        #         weights_name_list, weight_vector_dict = \\\n",
    "        #                         load_weight_vec_file(weight_vec_file_name, weights_to_use_list)\n",
    "                weights_name_list, weight_vector_dict = \\\n",
    "                                load_weight_vec_file(dirOrig+arq, weights_to_use_list)\n",
    "                file_num_weight_vectors = len(weight_vector_dict)\n",
    "\n",
    "                  #Adicionado por Diego\n",
    "\n",
    "                #weight_vector_dict_orig = copy.deepcopy(weight_vector_dict)\n",
    "                weight_vector_dict_orig = dict(weight_vector_dict)\n",
    "\n",
    "                weight_vector_dict, weighted_unique_weight_vec_dict, num_true_matches, \\\n",
    "                       num_true_non_matches = analyse_filter_weight_vectors(weight_vector_dict)\n",
    "\n",
    "                unique_num_weight_vectors = len(weighted_unique_weight_vec_dict)\n",
    "\n",
    "                data_prep_time = time.time() - start_time\n",
    "                print 'Time to load and analyse the weight vector file: %.2f sec' % \\\n",
    "                     (data_prep_time)\n",
    "                print\n",
    "\n",
    "                # A flag, set to True once the first (initial) selection has been done (as the\n",
    "                # initial selection function is different from all following ones)\n",
    "                #\n",
    "                init_selection_done = False\n",
    "\n",
    "                cluster_queue = []  # Clusters of weight vectors that need to be split further\n",
    "\n",
    "                cluster_size_list =     []  # Collect statistics about cluster sizes, their\n",
    "                cluster_pureness_list = []  # pureness, entropy and time required\n",
    "                cluster_entropy_list =  []\n",
    "                cluster_use_pure_list = []  # Purness of only clusters used for training\n",
    "                cluster_sample_size =   []  # Number of samples required per cluster\n",
    "                loop_sel_time_list =    []\n",
    "                loop_oracle_time_list = []\n",
    "                loop_class_time_list =  []\n",
    "                num_clusters_used =      0  # How many clusters were used for training\n",
    "\n",
    "                # Calculate initial estimated proportion of matches as minimum data set size\n",
    "                # divided by number of weight vectors\n",
    "                #\n",
    "                cluster_est_proportion = float(min_data_set_size) / unique_num_weight_vectors\n",
    "\n",
    "                cluster_est_proportion = 0.5  # Gives largest possible sample\n",
    "\n",
    "                print 'Initial estimated match proportion: %.3f' % \\\n",
    "                      (cluster_est_proportion)\n",
    "                print\n",
    "\n",
    "                # The queue contains tuples with:\n",
    "                # (weight vector dictionary, cluster purity, cluster_entropy, cluster size,\n",
    "                # cluster_est_proportion)\n",
    "\n",
    "                # Start with all weight vectors in one cluster\n",
    "\n",
    "                # For the initial cluster we set purity to 0.5 (minimum possible value) as it\n",
    "                # is unknown; similar maximum entropy is 1.0\n",
    "                #\n",
    "                cluster_queue.append((weighted_unique_weight_vec_dict, 0.5, 1.0,\n",
    "                                      len(weighted_unique_weight_vec_dict),\n",
    "                                      cluster_est_proportion))\n",
    "\n",
    "                # Keep a set of all weight vectors 'manually' classified by the oracle (so we\n",
    "                # can check the budget and stop once maximum budget is reached)\n",
    "                #\n",
    "                oracle_class_cache_set = set()\n",
    "\n",
    "                # Two dictionaries with the weight vectors selected as training data\n",
    "                #\n",
    "                final_match_weight_vector_dict =     {}\n",
    "                final_non_match_weight_vector_dict = {}\n",
    "\n",
    "                # Step 2: Recursively split clusters until stopping criteria achieved\n",
    "                #\n",
    "                loop_count = 0\n",
    "\n",
    "                while (cluster_queue != []):  # As long as we have clusters to be split further\n",
    "                  loop_count += 1\n",
    "\n",
    "                  print '- '*40\n",
    "                  print 'Loop %d: Queue length: %d' % (loop_count, len(cluster_queue))\n",
    "                  print '  Number of manual oracle classifications performed:', \\\n",
    "                        len(oracle_class_cache_set)\n",
    "                  print '  Size, purity, entropy, and estimated match proportion of ' + \\\n",
    "                        'clusters in queue:'\n",
    "                  for cluster_tuple in cluster_queue:\n",
    "                    print '   ', (cluster_tuple[3], cluster_tuple[1], cluster_tuple[2], \\\n",
    "                                  cluster_tuple[4])\n",
    "                  print\n",
    "                  print 'Current size of match and non-match training data sets: %d / %d' % \\\n",
    "                        (len(final_match_weight_vector_dict),\n",
    "                         len(final_non_match_weight_vector_dict))\n",
    "                  print\n",
    "\n",
    "                  # Step 2a: Select a cluster from the queue\n",
    "\n",
    "                  # Extension January 2015: Different orderings of the blocks in the queue\n",
    "                  # 'fifo' First in first out - our current approach\n",
    "                  # 'random' Random order - use random.choice()\n",
    "                  # 'max_puri'  Clusters with highest purity first (purity from parent cluster)\n",
    "                  # 'min_puri'  Clusters with lowest purity first (purity from parent cluster)\n",
    "                  # 'max_size'  Largest clusters first\n",
    "                  # 'min_size'  Smallest clusters first\n",
    "                  # 'min_entr'  Clusters with lowest entropy first (based on entropy of parent\n",
    "                  #             cluster, as child cluster entropy can only be smaller)\n",
    "                  # 'max_entr'  Clusters with highest entropy first (again based on parent\n",
    "                  #             cluster entropy)\n",
    "                  # 'max_size'  Largest clusters first\n",
    "                  # 'min_size'  Smallest clusters first\n",
    "                  # 'close_01'  Closest to 0 or 1 corner\n",
    "                  # 'close_mid' Closest to middle (half between 0 and 1 corners)\n",
    "                  # 'balance'   Select so that training set sizes are balanced\n",
    "                  # 'sample'    Select cluster which has largest ratio of cluster size divided\n",
    "                  #             by number of number of samples required\n",
    "\n",
    "                  # Need to calculate distances of all clusters to 0 corner first\n",
    "                  #\n",
    "                  if (queue_order in ['close_01','close_mid','balance']):  \n",
    "\n",
    "                    cluster_dist_list = []  # Pairs of distances and corresponding clusters\n",
    "\n",
    "                    for cluster_tuple in cluster_queue:\n",
    "                      cluster_dist = cluster_0_dist(cluster_tuple[0], num_weights, \\\n",
    "                                                    CLUSTER_MIN_DIST)\n",
    "                      if (queue_order == 'close_01'):\n",
    "                        abs_dist = min(cluster_dist, 1.0-cluster_dist)  # Distance from corner\n",
    "                        cluster_dist_list.append((abs_dist, cluster_tuple))\n",
    "                      elif (queue_order == 'close_mid'):\n",
    "                        abs_dist = abs(0.5-cluster_dist)  # Distance from middle\n",
    "                        cluster_dist_list.append((abs_dist, cluster_tuple))\n",
    "                      elif (queue_order == 'balance'):\n",
    "                        cluster_dist_list.append((cluster_dist, cluster_tuple))\n",
    "                      else:\n",
    "                        raise Exception, queue_order\n",
    "\n",
    "                    cluster_dist_list.sort()  # Smallest distance first\n",
    "\n",
    "                    if (queue_order in ['close_01', 'close_mid']):  # Take first\n",
    "                      next_cluster_tuple = cluster_dist_list[0][1]\n",
    "                      cluster_queue.remove(next_cluster_tuple)\n",
    "                    else:  # Select closest depending on size of current training data sets\n",
    "                      num_matches =     len(final_match_weight_vector_dict)\n",
    "                      num_non_matches = len(final_non_match_weight_vector_dict)\n",
    "\n",
    "                      # Use <= to favour selecting likely matches over non-matches\n",
    "                      #\n",
    "                      if (num_matches <= num_non_matches): # Select cluster closest to 1 corner\n",
    "                        print '  Balanced cluster selection, select cluster closest to' + \\\n",
    "                              ' [1,..,1]'\n",
    "                        print\n",
    "                        next_cluster_tuple = cluster_dist_list[-1][1]\n",
    "                        cluster_queue.remove(next_cluster_tuple)\n",
    "                      else:\n",
    "                        print '  Balanced cluster selection, select cluster closest to' + \\\n",
    "                              '[0,...,0]'\n",
    "                        print\n",
    "                        next_cluster_tuple = cluster_dist_list[0][1]\n",
    "                        cluster_queue.remove(next_cluster_tuple)\n",
    "\n",
    "                  elif (queue_order == 'fifo'):\n",
    "                    next_cluster_tuple = cluster_queue.pop(0)\n",
    "\n",
    "                  elif (queue_order == 'random'):\n",
    "                    next_cluster_tuple = random.choice(cluster_queue)\n",
    "                    cluster_queue.remove(next_cluster_tuple)\n",
    "\n",
    "                  elif (queue_order == 'max_puri'):\n",
    "                    check_max_purity = 0.0\n",
    "                #    next_cluster_tuple = None\n",
    "                    for cluster_tuple in cluster_queue:\n",
    "                      if (cluster_tuple[1] > check_max_purity):\n",
    "                        check_max_purity = cluster_tuple[1]\n",
    "                        next_cluster_tuple = cluster_tuple\n",
    "                    cluster_queue.remove(next_cluster_tuple)\n",
    "\n",
    "                  elif (queue_order == 'min_puri'):\n",
    "                    check_min_purity = 2.0\n",
    "                #    next_cluster_tuple = None\n",
    "                    for cluster_tuple in cluster_queue:\n",
    "                      if (cluster_tuple[1] < check_min_purity):\n",
    "                        check_min_purity = cluster_tuple[1]\n",
    "                        next_cluster_tuple = cluster_tuple\n",
    "                    cluster_queue.remove(next_cluster_tuple)\n",
    "\n",
    "                  elif (queue_order == 'max_entr'):\n",
    "                    check_max_entropy = -1.0\n",
    "                #    next_cluster_tuple = None\n",
    "                    for cluster_tuple in cluster_queue:\n",
    "                      if (cluster_tuple[2] > check_max_entropy):\n",
    "                        check_max_entropy = cluster_tuple[2]\n",
    "                        next_cluster_tuple = cluster_tuple\n",
    "                    cluster_queue.remove(next_cluster_tuple)\n",
    "\n",
    "                  elif (queue_order == 'min_entr'):\n",
    "                    check_min_entropy = 2.0\n",
    "                #    next_cluster_tuple = None\n",
    "                    for cluster_tuple in cluster_queue:\n",
    "                      if (cluster_tuple[2] < check_min_entropy):\n",
    "                        check_min_entropy = cluster_tuple[2]\n",
    "                        next_cluster_tuple = cluster_tuple\n",
    "                    cluster_queue.remove(next_cluster_tuple)\n",
    "\n",
    "                  elif (queue_order == 'max_size'):\n",
    "                    check_max_size = -1.0\n",
    "                #    next_cluster_tuple = None\n",
    "                    for cluster_tuple in cluster_queue:\n",
    "                      if (cluster_tuple[3] > check_max_size):\n",
    "                        check_max_size = cluster_tuple[2]\n",
    "                        next_cluster_tuple = cluster_tuple\n",
    "                    cluster_queue.remove(next_cluster_tuple)\n",
    "\n",
    "                  elif (queue_order == 'min_size'):\n",
    "                    check_min_size = 99999999.0\n",
    "                #    next_cluster_tuple = None\n",
    "                    for cluster_tuple in cluster_queue:\n",
    "                      if (cluster_tuple[3] < check_min_size):\n",
    "                        check_min_size = cluster_tuple[2]\n",
    "                        next_cluster_tuple = cluster_tuple\n",
    "                    cluster_queue.remove(next_cluster_tuple)\n",
    "\n",
    "                  elif (queue_order == 'sample'):\n",
    "                    check_max_ratio = -1.0\n",
    "\n",
    "                    for cluster_tuple in cluster_queue:\n",
    "                      this_cluster_size = cluster_tuple[3]\n",
    "                      this_cluster_prop = cluster_tuple[4]\n",
    "                      this_select_num = get_sample_size(this_cluster_size,\n",
    "                                                        this_cluster_prop, sample_error)\n",
    "                      this_cluster_ratio = float(this_cluster_size) / this_select_num\n",
    "                      if (this_cluster_ratio > check_max_ratio):\n",
    "                        check_max_ratio = this_cluster_ratio\n",
    "                        next_cluster_tuple = cluster_tuple\n",
    "                    cluster_queue.remove(next_cluster_tuple)\n",
    "\n",
    "                ## TODO: Add optimal ordering - combine balancing, sample size, purity etc.\n",
    "\n",
    "                  else:\n",
    "                    raise Exception, queue_order\n",
    "\n",
    "                  (next_weight_vec_dict, cluster_purity, cluster_entropy, cluster_size, \\\n",
    "                                                  cluster_est_proportion) = next_cluster_tuple\n",
    "\n",
    "                  cluster_size_list.append(cluster_size)\n",
    "\n",
    "                  print 'Selected cluster with (queue ordering: %s):' % (queue_order)\n",
    "                  print '- Purity %.2f and entropy %.2f' % (cluster_purity, cluster_entropy)\n",
    "                  print '- Size %d weight vectors' % (cluster_size)\n",
    "                  print '- Estimated match proportion %.3f' % (cluster_est_proportion)\n",
    "                  print\n",
    "\n",
    "                  # Calculate sample size (number of weight vectors to select) for this cluster\n",
    "                  #\n",
    "                  select_num = get_sample_size(cluster_size, cluster_est_proportion, \\\n",
    "                                               sample_error)\n",
    "                  cluster_sample_size.append(select_num)\n",
    "\n",
    "                  print 'Sample size for this cluster:', select_num\n",
    "                  print\n",
    "\n",
    "                  # Step 2b: Get representative weight vectors from this cluster\n",
    "                  #\n",
    "                  start_time = time.time()\n",
    "\n",
    "                  if (init_selection_done == False):  # Do initial selection\n",
    "                    init_selection_done = True\n",
    "                    print 'Perform initial selection using \"%s\" method' % (init_method)\n",
    "                    print\n",
    "\n",
    "                    if (init_method == 'far'):\n",
    "                      rep_weight_vec_dict = select_farthest(next_weight_vec_dict, select_num, \\\n",
    "                                                            num_weights)\n",
    "                    elif (init_method == '01'):\n",
    "                      rep_weight_vec_dict = select_01(next_weight_vec_dict, select_num, \\\n",
    "                                                      num_weights)\n",
    "                    elif (init_method == 'corner'):\n",
    "                      rep_weight_vec_dict = select_corners(next_weight_vec_dict, num_weights)\n",
    "\n",
    "                    else:  # Random\n",
    "                     rep_weight_vec_dict = select_random(next_weight_vec_dict, select_num)\n",
    "\n",
    "                  else:  # Any following selection\n",
    "                    if (select_method == 'far'):\n",
    "                      rep_weight_vec_dict = select_farthest(next_weight_vec_dict, select_num, \\\n",
    "                                                            num_weights)\n",
    "                    elif (select_method == 'far_med'):\n",
    "                      rep_weight_vec_dict = select_farthest(next_weight_vec_dict, select_num, \\\n",
    "                                                            num_weights)\n",
    "                      centroid_weight_vec_dict = select_medoid(next_weight_vec_dict, 1, \\\n",
    "                                                               num_weights)\n",
    "                      assert len(centroid_weight_vec_dict) == 1\n",
    "\n",
    "                      # Add centroids to representative weight vectors\n",
    "                      #\n",
    "                      for weight_vector_tuple in centroid_weight_vec_dict:\n",
    "                        rep_weight_vec_dict[weight_vector_tuple] = \\\n",
    "                                                  centroid_weight_vec_dict[weight_vector_tuple]\n",
    "\n",
    "                    elif (select_method == 'dense'):\n",
    "                      rep_weight_vec_dict = select_densest(next_weight_vec_dict, select_num, \\\n",
    "                                                           num_weights)\n",
    "\n",
    "                    elif (select_method == 'aggl'):\n",
    "                      rep_weight_vec_dict = select_agglomerative(next_weight_vec_dict,\n",
    "                                                                 select_num, \\\n",
    "                                                                 num_weights)\n",
    "\n",
    "                    else:  # Random\n",
    "                      rep_weight_vec_dict = select_random(next_weight_vec_dict, select_num)\n",
    "\n",
    "                  sel_time = time.time() - start_time\n",
    "                  loop_sel_time_list.append(sel_time)\n",
    "\n",
    "                  # Step 2c: Give selected weight vectors to oracle for 'manual' classification\n",
    "                  #\n",
    "                  start_time = time.time()\n",
    "\n",
    "                  match_dict, non_match_dict, purity, entropy = \\\n",
    "                                                       oracle(rep_weight_vec_dict, oracle_acc)\n",
    "                  cluster_pureness_list.append(purity)\n",
    "                  cluster_entropy_list.append(entropy)\n",
    "\n",
    "                  assert len(match_dict) + len(non_match_dict) == len(rep_weight_vec_dict)\n",
    "\n",
    "                  # Calculate a new estimate for match proportion based on size of manually\n",
    "                  # classified weight vectors\n",
    "                  #\n",
    "                  cluster_est_proportion = float(len(match_dict)) / len(rep_weight_vec_dict)\n",
    "\n",
    "                  # Update the cache with the 'manually' classified weight vectors\n",
    "                  #\n",
    "                  for weight_vector_tuple in rep_weight_vec_dict:\n",
    "                    oracle_class_cache_set.add(weight_vector_tuple)\n",
    "\n",
    "                  # Add the manually classified weight vectors into the final training sets\n",
    "                  # and remove from current cluster as well as from the original weight vector\n",
    "                  # dictionary\n",
    "                  #\n",
    "                  for weight_vector_tuple in match_dict:\n",
    "                    final_match_weight_vector_dict[weight_vector_tuple] = \\\n",
    "                                                                match_dict[weight_vector_tuple]\n",
    "                    del next_weight_vec_dict[weight_vector_tuple]\n",
    "                    if (next_weight_vec_dict != weighted_unique_weight_vec_dict):\n",
    "                      del weighted_unique_weight_vec_dict[weight_vector_tuple]\n",
    "\n",
    "                  for weight_vector_tuple in non_match_dict:\n",
    "                    final_non_match_weight_vector_dict[weight_vector_tuple] = \\\n",
    "                                                            non_match_dict[weight_vector_tuple]\n",
    "                    del next_weight_vec_dict[weight_vector_tuple]\n",
    "                    if (next_weight_vec_dict != weighted_unique_weight_vec_dict):\n",
    "                      del weighted_unique_weight_vec_dict[weight_vector_tuple]\n",
    "\n",
    "                  print 'Deleted %d weight vectors (classified by oracle) from cluster' % \\\n",
    "                        (len(match_dict)+len(non_match_dict))\n",
    "                  print\n",
    "\n",
    "                  oracle_time = time.time() - start_time\n",
    "                  loop_oracle_time_list.append(oracle_time)\n",
    "\n",
    "                  # Step 2f: Decide if cluster needs/can be split further or not\n",
    "                  # Stopping criteria:\n",
    "                  # 1) Cluster is pure enough and not too large (<= max_cluster_size)\n",
    "                  #    -> Add to final training data\n",
    "                  # 2) Cluster is too small for further splitting -> Do not use for training\n",
    "                  # 3) No more budget left for future 'manual' oracle classification\n",
    "\n",
    "                  # If the cluster is pure enough and not too large then add all its weight\n",
    "                  # vectors to the final dictionaries of training data, and remove them from\n",
    "                  # the original weight vector dictionary\n",
    "                  #\n",
    "                  if ((cluster_size <= max_cluster_size) and (purity >= min_purity)):\n",
    "                    print 'Cluster is pure enough and not too large, add its ' + \\\n",
    "                          '%d weight vectors to:' % (cluster_size)\n",
    "\n",
    "                    num_clusters_used += 1\n",
    "                    cluster_use_pure_list.append(purity)\n",
    "\n",
    "                    if (len(match_dict) > len(non_match_dict)):  # The cluster contains matches\n",
    "                      print '  Match training set'\n",
    "                      for weight_vector_tuple in next_weight_vec_dict.keys():\n",
    "                        final_match_weight_vector_dict[weight_vector_tuple] = \\\n",
    "                                                     next_weight_vec_dict[weight_vector_tuple]\n",
    "                        del weighted_unique_weight_vec_dict[weight_vector_tuple]\n",
    "                    else:  # The cluster contains non-matches\n",
    "                      print '  Non-match training set'\n",
    "                      for weight_vector_tuple in next_weight_vec_dict.keys():\n",
    "                        final_non_match_weight_vector_dict[weight_vector_tuple] = \\\n",
    "                                                     next_weight_vec_dict[weight_vector_tuple]\n",
    "                        del weighted_unique_weight_vec_dict[weight_vector_tuple]\n",
    "                    print\n",
    "\n",
    "                  # Check if the cluster can be split further\n",
    "                  #\n",
    "                  elif (cluster_size <= min_cluster_size):\n",
    "                    print 'Cluster is too small for further splitting, but not pure enough ' \\\n",
    "                          + 'for further splitting, so do not add to training data'\n",
    "                    print\n",
    "\n",
    "                  else:  # The cluster is too large or not pure enough and it can be split\n",
    "                    print 'Cluster not pure enough or too large, and can be split further'\n",
    "                    print\n",
    "\n",
    "                    # Check if we still have manual classifications left\n",
    "                    #\n",
    "                    if (len(oracle_class_cache_set) >= budget_num_class):\n",
    "                      print 'Reached end of manual classification budget'\n",
    "                      print\n",
    "                      break  # Leave while loop, no more budget to do manual classification\n",
    "\n",
    "                    # Step 2d: Split the cluster using a binary classifier\n",
    "                    #\n",
    "                    if ((len(match_dict) > 0) and (len(non_match_dict) > 0)):\n",
    "                      # We need training weight vectors in both classes\n",
    "\n",
    "                      start_time = time.time()\n",
    "\n",
    "                      if (split_classifier == 'knn'):\n",
    "                        class_match_dict, class_non_match_dict = \\\n",
    "                                          knn_split_classifier(next_weight_vec_dict, KNN_K, \\\n",
    "                                                               match_dict, non_match_dict)\n",
    "                      elif (split_classifier == 'dtree'):\n",
    "                        class_match_dict, class_non_match_dict = \\\n",
    "                                   dtree_split_classifier(next_weight_vec_dict, match_dict, \\\n",
    "                                                          non_match_dict, num_weights)\n",
    "                      else:\n",
    "                        class_match_dict, class_non_match_dict = \\\n",
    "                                   svm_split_classifier(next_weight_vec_dict, match_dict, \\\n",
    "                                                        non_match_dict, num_weights)\n",
    "\n",
    "                      class_time = time.time() - start_time\n",
    "                      loop_class_time_list.append(class_time)\n",
    "\n",
    "                      # First check that both sub-clusters contain weight vectors, if not (i.e.\n",
    "                      # if all weight vectors are in one sub-cluster) then we cannot add as we\n",
    "                      # would add the same cluster as we had before -> endless loop\n",
    "                      #\n",
    "                      if ((len(class_match_dict) > 0) and (len(class_non_match_dict) > 0)):\n",
    "\n",
    "                        # Only add to queue if a sub-cluster contains enough weight vectors\n",
    "                        # (i.e. more than needed in the selection process)\n",
    "                        #\n",
    "                        if (len(class_match_dict) > 0):\n",
    "                          select_num = get_sample_size(len(class_match_dict), \\\n",
    "                                                       cluster_est_proportion, \\\n",
    "                                                       sample_error)\n",
    "                          if (len(class_match_dict) >= select_num):\n",
    "                            cluster_queue.append((class_match_dict, purity, entropy,\n",
    "                                                  len(class_match_dict),\n",
    "                                                  cluster_est_proportion))\n",
    "                          else:\n",
    "                            print '  Match cluster not large enough for required sample size'\n",
    "\n",
    "                        if (len(class_non_match_dict) > 0):\n",
    "                          select_num = get_sample_size(len(class_non_match_dict), \\\n",
    "                                                       cluster_est_proportion, \\\n",
    "                                                       sample_error)\n",
    "                          if (len(class_non_match_dict) > select_num):\n",
    "                            cluster_queue.append((class_non_match_dict, purity, entropy,\n",
    "                                                  len(class_non_match_dict),\n",
    "                                                  cluster_est_proportion))\n",
    "                          else:\n",
    "                            print '  Non-match cluster not large enough for required ' + \\\n",
    "                                  'sample size'\n",
    "\n",
    "                      else:\n",
    "                        # cluster not used further on in training\n",
    "\n",
    "                        # This case happens if we have a pure cluster that is too large\n",
    "                        # It will likely not be split further - so what can we do here?\n",
    "                        # Split randomly into 2? Do fathest first with k=2, then split into 2\n",
    "                        # acording to nearest?\n",
    "                        pass  # TODO **********************************************************\n",
    "\n",
    "                    else:  # We have training weight vectors in one class only\n",
    "                      pass # TODO - what here?\n",
    "\n",
    "                dirDest = \"../csv/conjuntosDS/treinoTeste/\"+base+\"/\"+qp+\"/PtChr/\"\n",
    "        #         dirDest = \"C:/Users/Diego/Documents/NetBeansProjects/Master-SKYAM/AS/src/csv/conjuntosDS/treinoTeste/\"\n",
    "        #         dirDest = \"../../Documents/NetBeansProjects/Master-SKYAM/AS/src/csv/conjuntosDS/treinoTeste/\"\n",
    "\n",
    "\n",
    "        #         geraTrainSet(weight_vector_dict_orig, dirDest, 'train' + '(' + int(algUtl) + ')' + num + '.csv')    \n",
    "\n",
    "        #         geraTestSet(weight_vector_dict_orig, dirDest, 'test' + '(' + int(algUtl) + ')' + num + '.csv')\n",
    "\n",
    "        #         print 'Number of manual oracle classifications done: %d (out of total ' % \\\n",
    "        #       (len(oracle_class_cache_set)) + 'budget of %d)' % (budget_num_class)\n",
    "        #         print ''\n",
    "\n",
    "                abordagem = 'DS'\n",
    "                #print 'abordagem é %s' %(abordagem)\n",
    "\n",
    "                #algUtl = linhaAtual['algoritmosUtilizados'].item()\n",
    "                iteracao = 1\n",
    "                inspecoesManuais = len(oracle_class_cache_set)\n",
    "                print linhaAtual['da'].item()\n",
    "\n",
    "                da = linhaAtual['da'].item()\n",
    "                dm = len(final_match_weight_vector_dict)\n",
    "                ndm = len(final_non_match_weight_vector_dict)\n",
    "\n",
    "                tp = float(linhaAtual['tp'].item() + dm)\n",
    "                fp = float(linhaAtual['fp'].item())\n",
    "                tn = float(linhaAtual['tn'].item())# + ndm) #Retirado\n",
    "                fn = float(linhaAtual['fn'].item() - dm) #Adicionado\n",
    "\n",
    "        #         print 'tp'\n",
    "        #         print type(tp)\n",
    "        #         print tp\n",
    "        #         print 'fp'\n",
    "        #         print type(fp)\n",
    "        #         print fp\n",
    "        #         print 'tn'\n",
    "        #         print type(tn)\n",
    "        #         print tn\n",
    "        #         print 'fn'\n",
    "        #         print type(fn)\n",
    "        #         print fn\n",
    "\n",
    "                precision = tp/(tp+fp)\n",
    "        #         print type(precisao)\n",
    "        #         print precisao\n",
    "        #         print 'Precisão:'\n",
    "        #         print type(precision)\n",
    "        #         print precision\n",
    "                recall = tp/(tp+fn)\n",
    "                fmeasure = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "\n",
    "\n",
    "                #Adicionando valor à última linha\n",
    "                estatisticas.loc[(algUtl, etapa, permutacao), ['abordagem', 'iteracao', 'inspecoesManuais',\n",
    "                   'precision', 'recall', 'f-measure', 'da', 'dm', 'ndm', 'tp',\n",
    "                   'fp', 'tn', 'fn'] ] = ([abordagem, iteracao, inspecoesManuais,\n",
    "                   precision, recall, fmeasure, da, dm, ndm, tp, fp, tn, fn])\n",
    "\n",
    "#                 dirDest = \"../csv/conjuntosDS/treinoTeste/\"\n",
    "#                 dirDest = \"../csv/conjuntosDS/treinoTeste/\"+base+\"/\"+qp+\"/\"+\"treinoTeste/\"\n",
    "        #         dirDest = \"../../Documents/NetBeansProjects/Master-SKYAM/AS/src/csv/conjuntosDS/treinoTeste/\"\n",
    "        #         dirDest = \"./arqResult/csv/conjuntosDS/conjuntosDiverg/treinoTeste/\"\n",
    "\n",
    "                #algUtl = str(algUtl).replace('.0','')\n",
    "                algUtl = str(algUtl)\n",
    "\n",
    "                geraTrainSet(weight_vector_dict_orig, dirDest, 'train' + '(' + algUtl + ')' + num + '.csv')    \n",
    "\n",
    "                geraTestSet(weight_vector_dict_orig, dirDest, 'test' + '(' + algUtl + ')' + num + '.csv')\n",
    "\n",
    "                #Para voltar o dataframe ao normal (Depois organizar as colunas)\n",
    "\n",
    "        estatisticas = estatisticas.reset_index(level=['algoritmosUtilizados', 'etapa', 'permutacao'])\n",
    "\n",
    "        estatisticas = estatisticas[['abordagem', 'etapa', 'algoritmosUtilizados', 'permutacao', 'iteracao', 'inspecoesManuais', 'precision', 'recall', 'f-measure', 'da', 'dm', 'ndm', 'tp', 'fp', 'tn', 'fn']]\n",
    "\n",
    "        estatisticas[['algoritmosUtilizados', 'iteracao', 'inspecoesManuais', 'da', 'dm', 'ndm', 'tp', 'fp', 'tn', 'fn']] = \\\n",
    "        estatisticas[['algoritmosUtilizados', 'iteracao', 'inspecoesManuais', 'da', 'dm', 'ndm', 'tp', 'fp', 'tn', 'fn']].astype(int)\n",
    "\n",
    "        # Diretório para Windows\n",
    "#         dirEst = \"../csv/\"\n",
    "        # dirEst = \"C:\\Users\\Diego\\Documents\\NetBeansProjects\\Master-SKYAM\\AS\\src\\csv\\\\\"\n",
    "        # dirEst = \"../../Documents/NetBeansProjects/Master-SKYAM/AS/src/csv/\"\n",
    "\n",
    "\n",
    "        # Diretório para Linux\n",
    "        # dirEst = \"./arqResult/csv/\"\n",
    "\n",
    "        estatisticas.to_csv(dirEstat+'estatisticaDS2-PtChr.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
